[
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Sep 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "blog",
    "section": "",
    "text": "Sep 10, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 26, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2022-08-19_ConcreteIntro/2022-08-19_ConcreteIntro.html",
    "href": "posts/2022-08-19_ConcreteIntro/2022-08-19_ConcreteIntro.html",
    "title": "Introduction to the Concrete Dataset",
    "section": "",
    "text": "One such dataset is the Concrete Compressive Strength Dataset found on the UCI Machine Learning Repository. Many thanks to the original owner, Prof. I-Cheng Yeh for making this dataset available to the public!\nConventional concrete contains cement, fine and coarse aggregates and water. High performance concrete incorporates additional ingredients such as fly ash, blast furnace slag and chemical additives like superplasticizer. The compressive strength of concrete has been empirically found to have an inverse relationship to the water-to-cement ratio also known as the Abrams’ rule. High performance concrete is a more complex material and experimental data does not always support this general rule. This dataset contains over 1000 high performance concrete formulations containing the ingredients described above along with the compressive strength of each formulation. In addition, the age of the concrete before testing is also recorded.\n\nBasic Analysis of the Concrete Dataset\nThe concrete dataset was downloaded from the UCI repository as an Excel file and imported into R using the readxl package without further modification. Once the dataset was loaded into R, the column names were cleaned up into a format more amenable to data analysis. The complete dataset contains 1030 rows of concrete formulations of which the first 5 are shown below.\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(knitr)\n\n\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))\n\n\nknitr::kable(head(concrete_tbl, 5), caption = \"First 5 Rows of Concrete Dataset\")\n\n\nFirst 5 Rows of Concrete Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\ncement\nblast_furnace_slag\nfly_ash\nwater\nsuperplasticizer\ncoarse_aggregate\nfine_aggregate\nage\ncompressive_strength\n\n\n\n\n540.0\n0.0\n0\n162\n2.5\n1040.0\n676.0\n28\n79.98611\n\n\n540.0\n0.0\n0\n162\n2.5\n1055.0\n676.0\n28\n61.88737\n\n\n332.5\n142.5\n0\n228\n0.0\n932.0\n594.0\n270\n40.26954\n\n\n332.5\n142.5\n0\n228\n0.0\n932.0\n594.0\n365\n41.05278\n\n\n198.6\n132.4\n0\n192\n0.0\n978.4\n825.5\n360\n44.29608"
  },
  {
    "objectID": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html",
    "href": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html",
    "title": "Exploratory Analysis of the Concrete Dataset",
    "section": "",
    "text": "Several exploratory data analysis (EDA) packages are used to evaluate the concrete dataset.\nIn the previous post, the concrete dataset was introduced. In this post, we further explore topics such as data completeness, distributions and correlations both with the target variable (compressive strength) and between predictor variables (ingredients). I use Several R packages which I have found to make this analysis quite simple and efficient: skimr, GGally and correlationFunnel."
  },
  {
    "objectID": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#load-libraries",
    "href": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#load-libraries",
    "title": "Exploratory Analysis of the Concrete Dataset",
    "section": "Load libraries",
    "text": "Load libraries\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n#EDA\nlibrary(skimr)\nlibrary(GGally)\nlibrary(correlationfunnel)\n\nA good first analysis once the dataset is loaded is to use the skimr package to provide an overview of the data columns.\n\nskimr::skim(concrete_tbl)\n\n\nData summary\n\n\nName\nconcrete_tbl\n\n\nNumber of rows\n1030\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncement\n0\n1\n281.17\n104.51\n102.00\n192.38\n272.90\n350.00\n540.0\n▆▇▇▃▂\n\n\nblast_furnace_slag\n0\n1\n73.90\n86.28\n0.00\n0.00\n22.00\n142.95\n359.4\n▇▂▃▁▁\n\n\nfly_ash\n0\n1\n54.19\n64.00\n0.00\n0.00\n0.00\n118.27\n200.1\n▇▁▂▂▁\n\n\nwater\n0\n1\n181.57\n21.36\n121.75\n164.90\n185.00\n192.00\n247.0\n▁▅▇▂▁\n\n\nsuperplasticizer\n0\n1\n6.20\n5.97\n0.00\n0.00\n6.35\n10.16\n32.2\n▇▆▁▁▁\n\n\ncoarse_aggregate\n0\n1\n972.92\n77.75\n801.00\n932.00\n968.00\n1029.40\n1145.0\n▃▅▇▅▂\n\n\nfine_aggregate\n0\n1\n773.58\n80.18\n594.00\n730.95\n779.51\n824.00\n992.6\n▂▃▇▃▁\n\n\nage\n0\n1\n45.66\n63.17\n1.00\n7.00\n28.00\n56.00\n365.0\n▇▁▁▁▁\n\n\ncompressive_strength\n0\n1\n35.82\n16.71\n2.33\n23.71\n34.44\n46.14\n82.6\n▅▇▇▃▁\n\n\n\n\n\nThe good news is that there were no missing data points in the concrete dataset. There are concrete compositions with no blast furnace slag, fly ash or superplasticizer. The age before testing is skewed to lower age before testing. These observations are further supported looking at the histograms shown below.\n\nconcrete_tbl %>%\n  pivot_longer(cement:age, names_to=\"ingredient\", values_to = \"amount\") %>%\n  ggplot(aes(x=amount)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(~ingredient, scales = \"free\")"
  },
  {
    "objectID": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#variable-correlations",
    "href": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#variable-correlations",
    "title": "Exploratory Analysis of the Concrete Dataset",
    "section": "Variable correlations",
    "text": "Variable correlations\nNext, we analyze the correlations between variables. When the amount of one ingredient is increased, we expect one or more of the other ingredients in the concrete mixture to decrease. So, some correlation between the concrete ingredients is expected.\n\nGGally::ggcorr(concrete_tbl)\n\n\n\n\nThe correlation analysis showed a strong, positive correlation with cement content and compressive strength and less strong correlations with age with compressive strength and superplasticizer with compressive strength. An inverse correlation between water and superplasticizer was detected perhaps due to the water content of the superplasticizer requiring less additional water in the formulation.\nAnother way of visualizing the correlation of variables with the property you wish to predict is the called a “correlation funnel”.\n\nconcrete_tbl %>%\n  binarize(n_bins = 3) %>%\n  correlate(`compressive_strength__41.36856_Inf`) %>%\n  plot_correlation_funnel(interactive = FALSE)\n\n\n\n\nThe correlation funnel shows some degree of correlation between the cement, water, superplasticizer and age with compressive strength. The fly ash, coarse and fine aggregate and blast furnace slag showed very little correlation with compressive strength."
  },
  {
    "objectID": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#summary",
    "href": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#summary",
    "title": "Exploratory Analysis of the Concrete Dataset",
    "section": "Summary",
    "text": "Summary\nThis post has shown several techniques for exploring the concrete dataset. The next post will use a generalized linear modeling approach to predict concrete compressive strength and compare the results with the conventional material modeling approach. Subsequent articles will use machine learning techniques such as artificial neural networks and extreme gradient boosting."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2022-09-03_ConcreteGLM/2022-09-03_ConcreteGLM.html",
    "href": "posts/2022-09-03_ConcreteGLM/2022-09-03_ConcreteGLM.html",
    "title": "GLM model for Concrete Strenght",
    "section": "",
    "text": "A generalized linear model (GLM) was built to predict compressive strength of high-performance concrete formulations.\nAn elastic net regularization has been employed to develop the generalized linear model using the glmnet engine within the tidymodels framework. These results will be compared with a conventional materials modeling approach in the next post.\n##Load libraries"
  },
  {
    "objectID": "posts/2022-09-03_ConcreteGLM/2022-09-03_ConcreteGLM.html#stage-machine-learning-approach",
    "href": "posts/2022-09-03_ConcreteGLM/2022-09-03_ConcreteGLM.html#stage-machine-learning-approach",
    "title": "GLM model for Concrete Strenght",
    "section": "3-Stage Machine Learning Approach",
    "text": "3-Stage Machine Learning Approach\nWe will utilize the 3-stage machine learning approach promoted by Matt Dancho at Business Science. He posted an excellent tutorial “Product Price Prediction: A Tidy Hyperparameter Tuning and Cross Validation Tutorial”. I haven’t found a better example of applying the tidymodels framework to develop a predictive model.\nThe 3-stage hyperparameter tuning process:\n1. Find Parameters: Use hyperparameter tuning on a “training dataset” that sections your training data into cross validation folds. The output of stage 1 is the parameter set.\n2. Compare and Select the Best Model: Evaluate the performance on a hidden “test dataset”. The output at Stage 2 is what we determined as the best model.\n3. Train Final Model: Once we have selected the best model, we train the full dataset. This model goes into production.\n\nStage 1: Find Parameters\nHere we want to make different machine learning models and try them out by performing the following steps: - Initial Splitting: Separate into random training and test datasets - Preprocessing: Make a pipeline to transform raw data into a dataset ready for machine learning - Cross Validation Specification: Sample the training data into splits - Model Specification: Select model algorithms and identify key tuning parameters - Grid Specification: Set up a grid using wise parameter choices - Hyperparameter Tuning: Implement the tuning process\nInitial splitting of the dataset into Training and Test Dataset Here we use the rsample package to create an 80/20 split. The concrete dataset contains 1030 formulations of which 825 are randomly assigned to training and 205 are randomly assigned to testing.\n\nset.seed(123)\nconcrete_split <- initial_split(concrete_tbl, prop = 0.80)\nconcrete_train <- training(concrete_split)\nconcrete_test <- testing(concrete_split)\n\nPreprocessing is accomplished by using the recipe package. The recipe provides the steps required to transform our raw data into a dataset suitable for machine learning. The Concrete dataset actually doesn’t require much reformatting. The major issue was the lengthy column names which was addressed immediately after the dataset was imported. The dataset contained all numerical values and no missing data. Initially we will just center and scale the predictors before sending to the glmnet model.\n\nconcrete_rec <- recipe(compressive_strength ~ ., data = concrete_train) %>%\n  step_center(all_predictors()) %>%\n  step_scale(all_predictors())\n\nconcrete_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          8\n\nOperations:\n\nCentering for all_predictors()\nScaling for all_predictors()\n\n\nCross validation folds are created in order to assess the performance of the model parameters. Here we use 5-fold cross validation to create splits from our training dataset and also using the preprocessing pipeline specified above.\n\nset.seed(234)\nconcrete_folds <- vfold_cv(concrete_train, v = 5)\n\nconcrete_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [659/165]> Fold1\n2 <split [659/165]> Fold2\n3 <split [659/165]> Fold3\n4 <split [659/165]> Fold4\n5 <split [660/164]> Fold5\n\n\nModel specifications are created using the parsnip package. Here we specify a linear regression model using the glmnet engine. glmnet uses an Elastic Net which combines LASSO and Ridge Regression techniques. This is a linear algorithm which may have difficulty with the skewed numeric data which is present in the Concrete dataset. Notice that the penalty and mixture parameters have been specified to be tuned.\n\nglmnet_spec <- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"regression\")\n\nglmnet_spec\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = tune()\n\nComputational engine: glmnet \n\n\nGrid specifications sets up a variety of parameter values used with our model to find which combination yields the lowest prediction error (or best accuracy). Here we specify the parameter ranges and grid function using the dials package.\nSpecify the grid function (max entropy, hypercube etc.). Here we make a grid of 20 values using the grid_max_entropy() function in the dials package. Since there are just 2 tuning parameters in this case, we can visualize the grid selections. Note the penalty parameter is on the log base 10 scale by default. The dials package helps us make smarter choices for the critical tuning parameters.\n\nset.seed(345)\nglmnet_grid <- grid_max_entropy(penalty(), mixture(), size = 20)\n\nglmnet_grid %>%\n  ggplot(aes(penalty, mixture)) +\n  geom_point(color = \"steelblue\", size = 3) +\n  scale_x_log10() +\n  theme_light() +\n  labs(title = \"Max Entropy Grid\", x = \"Penalty (log scale)\", y = \"Mixture\")\n\n\n\n\n\nconcrete_wf <- workflow() %>%\n  add_recipe(concrete_rec) %>%\n  add_model(glmnet_spec)\n\nHyperparameter tuning is now performed using the tune_grid() function from the tune package. Here we specific the formula, model, resamples, grid and metrics. The metrics come from the yardstick package. For regression problems, we can specify multiple metrics such as mae, mape, rmse and rsq into a metric_set().\n\ndoParallel::registerDoParallel()\n\nglmnet_res <- tune_grid(\n  concrete_wf,\n  resamples = concrete_folds,\n  grid = glmnet_grid,\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\nIdentify the best hyperparameter values using the show_best() function.\n\nglmnet_res %>% show_best(\"mae\", n = 5)\n\n# A tibble: 5 × 8\n   penalty mixture .metric .estimator  mean     n std_err .config              \n     <dbl>   <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1 1.09e- 9   0.155 mae     standard    8.10     5   0.216 Preprocessor1_Model06\n2 5.89e- 4   0.208 mae     standard    8.10     5   0.216 Preprocessor1_Model07\n3 4.05e-10   0.392 mae     standard    8.10     5   0.216 Preprocessor1_Model10\n4 1.16e- 6   0.340 mae     standard    8.10     5   0.216 Preprocessor1_Model08\n5 1.41e- 8   0.520 mae     standard    8.10     5   0.216 Preprocessor1_Model12\n\n\nVisualize the tuning results\n\n\n\n\n\n\n\nStage 2: Compare and Select the Best Model\nSelect the best parameters based on the lowest mean absolute error.\n\nparams_glmnet_best <- glmnet_res %>% select_best(\"mae\")\nparams_glmnet_best\n\n# A tibble: 1 × 3\n        penalty mixture .config              \n          <dbl>   <dbl> <chr>                \n1 0.00000000109   0.155 Preprocessor1_Model06\n\n\nFinalize the model with the best parameters.\n\nfinal_glmnet <- finalize_workflow(concrete_wf, params_glmnet_best)\n\nfinal_glmnet\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_center()\n• step_scale()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1.09262294094878e-09\n  mixture = 0.155459027038887\n\nComputational engine: glmnet \n\n\nWhich Features are most important?\n\nfinal_glmnet %>%\n  fit(data = concrete_train) %>%\n  pull_workflow_fit() %>%\n  vip(aesthetics = list(fill = \"steelblue\")) +\n  labs(title = \"GLMNET Model Importance - Compressive Strength (MPa) Prediction\")\n\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nPlease use `extract_fit_parsnip()` instead.\n\n\n\n\n\n\n\nStage 3: Train Final Model\nFit model on train and evaluate on test.\n\nfinal_res <- last_fit(final_glmnet, concrete_split, metrics = metric_set(rmse, rsq, mae))\n\nAssess final model performance metrics.\n\ncollect_metrics(final_res)\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      11.4   Preprocessor1_Model1\n2 rsq     standard       0.615 Preprocessor1_Model1\n3 mae     standard       9.09  Preprocessor1_Model1\n\n\nVisualize actual vs. predicted compressive strength for final model."
  },
  {
    "objectID": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html",
    "href": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html",
    "title": "Conventional Material Models for Concrete Dataset",
    "section": "",
    "text": "Fitting the concrete dataset to a pre-determined equation using a non-linear, least squares approximation.\nAbrams’ law states that the strength of a concrete mix is inversely related to the mass ratio of water to cement. In other words, as the water content increases, the strength of the concrete decreases. Experimental data however shows that this law does not provide the complete picture and concrete formulations with the same water:cement can have significantly different performance."
  },
  {
    "objectID": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#load-libraries-and-data",
    "href": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#load-libraries-and-data",
    "title": "Conventional Material Models for Concrete Dataset",
    "section": "Load libraries and data",
    "text": "Load libraries and data\n\nlibrary(readxl)\nlibrary(tidyverse)\n\ntheme_set(theme_light())\n\n\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"../data/\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))"
  },
  {
    "objectID": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#plot-compressive-strength-as-a-function-of-watercement",
    "href": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#plot-compressive-strength-as-a-function-of-watercement",
    "title": "Conventional Material Models for Concrete Dataset",
    "section": "Plot compressive strength as a function of water:cement",
    "text": "Plot compressive strength as a function of water:cement\n\nconcrete_tbl <- concrete_tbl %>%\n  mutate(water_cement = water / cement,\n         water_binder = water / (cement + blast_furnace_slag + fly_ash))\n\nconcrete_tbl %>%\n  ggplot(aes(water_cement, compressive_strength)) +\n  geom_point(alpha = 0.15) +\n  geom_smooth(formula = y ~ x, method = \"lm\") +\n  theme_light() +\n  labs(title = \"Concrete Compressive Strength vs. Water:Cement\",\n       x = \"Water:Cement\", y = \"Compressive Strength (MPa)\")\n\n\n\n\nIt is apparent from the plot above that water:cement is not the only factor important for determining the compressive strength of concrete. For example, there are multiple formulations with a water:cement of ~1 with a range of compressive strengths from less than 10 MPa to greater than 50 MPa. The age of the concrete at the time of testing is also recognized as an important factor in determining the concrete strength for a sample.\n\\[ f^\\prime_c(t) = a X^b \\cdot [c \\ln(t)+(d)] \\]\nwhere t = age at test, X = w/c or water-to-binder ratio and a, b, c, d are regression coefficients\nThe above equation also includes the age at test variable (t) to predict the compressive strength. This equation uses four parameters reminding me of the famous quote by mathematician John von Neumann, “with four parameters I can fit an elephant, with five I can make him wiggle his trunk.”\nThis equation is fit to the experimental dataset using non-linear least squares approximation. The nls function in base R has been used as shown below."
  },
  {
    "objectID": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#nls-fit-using-watercement",
    "href": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#nls-fit-using-watercement",
    "title": "Conventional Material Models for Concrete Dataset",
    "section": "NLS Fit using water:cement",
    "text": "NLS Fit using water:cement\n\nwc <- concrete_tbl$water_cement\nwb <- concrete_tbl$water_binder\nage <- concrete_tbl$age\ncs <- concrete_tbl$compressive_strength\n\ncsFunc <- function(wc, age, a, b, c, d) { (a * wc^b) + (c * log(age) + d)}\n\nFit with water:cement\n\ncsFit <- nls(cs ~ csFunc(wc, age, a, b, c, d), start=list(a=30, b=-0.6, c=0.3, d=0.1))\n\nsummary(csFit)\n\n\nFormula: cs ~ csFunc(wc, age, a, b, c, d)\n\nParameters:\n  Estimate Std. Error t value Pr(>|t|)    \na  14.0112     2.9035   4.826 1.61e-06 ***\nb  -1.0536     0.1357  -7.763 2.00e-14 ***\nc   8.1770     0.2587  31.608  < 2e-16 ***\nd -12.8289     3.1703  -4.047 5.59e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.869 on 1026 degrees of freedom\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 3.529e-06\n\n\nVisualize actual vs. predicted compressive strength for water:cement model.\n\n\n\n\n\n\nNLS Fit using Water:Binder\nFit with water:binder\n\ncsFit_wb <- nls(cs ~ csFunc(wb, age, a, b, c, d), start=list(a=10, b=-0.5, c=10, d=10))\n\nsummary(csFit_wb)\n\n\nFormula: cs ~ csFunc(wb, age, a, b, c, d)\n\nParameters:\n  Estimate Std. Error t value Pr(>|t|)    \na  23.5011     6.9342   3.389 0.000728 ***\nb  -0.8614     0.1440  -5.980 3.07e-09 ***\nc   8.5739     0.2043  41.971  < 2e-16 ***\nd -39.2343     8.2396  -4.762 2.20e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.774 on 1026 degrees of freedom\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 2.226e-06\n\n\nVisualize actual vs. predicted compressive strength for water:binder model.\n\n\n\n\n\nThe NLS model using water:binder was a better fit to the experimental data than the NLS model using water:cement. The R2 for the water:binder model was 0.78 compared to an R2 of 0.65 for the water:cement model."
  }
]