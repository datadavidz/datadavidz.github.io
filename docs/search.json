[
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Dec 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "blog",
    "section": "",
    "text": "Advent of Code Day 21: Named List\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 20: Doubly-Linked List\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2025\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 19: Depth-First Search\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2024\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 18: Surface Area of Lava Droplet\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 17: Coordinates as Complex Numbers\n\n\n\n\n\n\n\n\n\n\n\nFeb 29, 2024\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 16: BitOps and Hashtables\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 15: Distress Beacon\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 14: Falling Sand\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2023\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 13: Recursion\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 12: Breadth-First Search\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2023\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 11: Monkey Business\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2023\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 10: Hidden Message\n\n\n\n\n\n\n\n\n\n\n\nJul 18, 2023\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 9: Tracking\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2023\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 8: Matrices\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 7: Tracking file paths\n\n\n\n\n\n\n\n\n\n\n\nMay 5, 2023\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 6: Decoding Signals\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 5: Stacking Crates\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 4: Separate\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2023\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 3: Manipulating Strings\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 2: Using Lookup Tables\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code Day 1: Working with Lists\n\n\n\n\n\n\n\n\n\n\n\nJan 5, 2023\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nA Rudimentary Shiny App for the Concrete API\n\n\n\n\n\n\nshiny\n\n\nMLOps\n\n\n\n\n\n\n\n\n\nDec 2, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a Dockerized API Running on an AWS EC2 instance\n\n\n\n\n\n\ntidymodels\n\n\nMLOps\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nPin a Vetiver Model to an AWS S3 Container\n\n\n\n\n\n\ntidymodels\n\n\nMLOps\n\n\n\n\n\n\n\n\n\nOct 14, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Process Model for the Concrete Dataset\n\n\n\n\n\n\nsci-kit\n\n\nreticulate\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of Concrete Models\n\n\n\n\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest Model for Concrete Dataset\n\n\n\n\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nPrediction of Concrete Strength using XGBoost\n\n\n\n\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network for Concrete Dataset\n\n\n\n\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\nSep 17, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nConventional Material Models for Concrete Dataset\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nGLM model for Concrete Strength\n\n\n\n\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\nSep 3, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nPython Setup in RStudio using reticulate\n\n\n\n\n\n\nreticulate\n\n\n\n\n\n\n\n\n\nSep 2, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Analysis of the Concrete Dataset\n\n\n\n\n\n\n\n\n\n\n\nAug 26, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to the Concrete Dataset\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: CEO Departures\n\n\n\n\n\n\nDataViz\n\n\n\n\n\n\n\n\n\nApr 27, 2021\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: U.S. Post Offices\n\n\n\n\n\n\nDataViz\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\ndatadavidz\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "My blog to share my data science projects for fun. I am an R enthusiast and enjoy exploring all kinds of topics in the tidyverse. Thanks for stopping by and be sure to check out the posts!"
  },
  {
    "objectID": "posts/2022-08-19_ConcreteIntro/2022-08-19_ConcreteIntro.html",
    "href": "posts/2022-08-19_ConcreteIntro/2022-08-19_ConcreteIntro.html",
    "title": "Introduction to the Concrete Dataset",
    "section": "",
    "text": "I have a particular interest in the ability of machine learning algorithms to predict formulations and I am always searching for these types of datasets.\nOne such dataset is the Concrete Compressive Strength Dataset found on the UCI Machine Learning Repository. Many thanks to the original owner, Prof. I-Cheng Yeh for making this dataset available to the public!\nConventional concrete contains cement, fine and coarse aggregates and water. High performance concrete incorporates additional ingredients such as fly ash, blast furnace slag and chemical additives like superplasticizer. The compressive strength of concrete has been empirically found to have an inverse relationship to the water-to-cement ratio also known as the Abrams’ rule. High performance concrete is a more complex material and experimental data does not always support this general rule. This dataset contains over 1000 high performance concrete formulations containing the ingredients described above along with the compressive strength of each formulation. In addition, the age of the concrete before testing is also recorded.\n\nBasic Analysis of the Concrete Dataset\nThe concrete dataset was downloaded from the UCI repository as an Excel file and imported into R using the readxl package without further modification. Once the dataset was loaded into R, the column names were cleaned up into a format more amenable to data analysis. The complete dataset contains 1030 rows of concrete formulations of which the first 5 are shown below.\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(knitr)\n\n\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))\n\n\nknitr::kable(head(concrete_tbl, 5), caption = \"First 5 Rows of Concrete Dataset\")\n\n\nFirst 5 Rows of Concrete Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\ncement\nblast_furnace_slag\nfly_ash\nwater\nsuperplasticizer\ncoarse_aggregate\nfine_aggregate\nage\ncompressive_strength\n\n\n\n\n540.0\n0.0\n0\n162\n2.5\n1040.0\n676.0\n28\n79.98611\n\n\n540.0\n0.0\n0\n162\n2.5\n1055.0\n676.0\n28\n61.88737\n\n\n332.5\n142.5\n0\n228\n0.0\n932.0\n594.0\n270\n40.26954\n\n\n332.5\n142.5\n0\n228\n0.0\n932.0\n594.0\n365\n41.05278\n\n\n198.6\n132.4\n0\n192\n0.0\n978.4\n825.5\n360\n44.29608"
  },
  {
    "objectID": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html",
    "href": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html",
    "title": "Exploratory Analysis of the Concrete Dataset",
    "section": "",
    "text": "Several exploratory data analysis (EDA) packages are used to evaluate the concrete dataset.\nIn the previous post, the concrete dataset was introduced. In this post, we further explore topics such as data completeness, distributions and correlations both with the target variable (compressive strength) and between predictor variables (ingredients). I use Several R packages which I have found to make this analysis quite simple and efficient: skimr, GGally and correlationFunnel."
  },
  {
    "objectID": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#load-libraries",
    "href": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#load-libraries",
    "title": "Exploratory Analysis of the Concrete Dataset",
    "section": "Load libraries",
    "text": "Load libraries\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n#EDA\nlibrary(skimr)\nlibrary(GGally)\nlibrary(correlationfunnel)\n\nA good first analysis once the dataset is loaded is to use the skimr package to provide an overview of the data columns.\n\nskimr::skim(concrete_tbl)\n\n\nData summary\n\n\nName\nconcrete_tbl\n\n\nNumber of rows\n1030\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncement\n0\n1\n281.17\n104.51\n102.00\n192.38\n272.90\n350.00\n540.0\n▆▇▇▃▂\n\n\nblast_furnace_slag\n0\n1\n73.90\n86.28\n0.00\n0.00\n22.00\n142.95\n359.4\n▇▂▃▁▁\n\n\nfly_ash\n0\n1\n54.19\n64.00\n0.00\n0.00\n0.00\n118.27\n200.1\n▇▁▂▂▁\n\n\nwater\n0\n1\n181.57\n21.36\n121.75\n164.90\n185.00\n192.00\n247.0\n▁▅▇▂▁\n\n\nsuperplasticizer\n0\n1\n6.20\n5.97\n0.00\n0.00\n6.35\n10.16\n32.2\n▇▆▁▁▁\n\n\ncoarse_aggregate\n0\n1\n972.92\n77.75\n801.00\n932.00\n968.00\n1029.40\n1145.0\n▃▅▇▅▂\n\n\nfine_aggregate\n0\n1\n773.58\n80.18\n594.00\n730.95\n779.51\n824.00\n992.6\n▂▃▇▃▁\n\n\nage\n0\n1\n45.66\n63.17\n1.00\n7.00\n28.00\n56.00\n365.0\n▇▁▁▁▁\n\n\ncompressive_strength\n0\n1\n35.82\n16.71\n2.33\n23.71\n34.44\n46.14\n82.6\n▅▇▇▃▁\n\n\n\n\n\nThe good news is that there were no missing data points in the concrete dataset. There are concrete compositions with no blast furnace slag, fly ash or superplasticizer. The age before testing is skewed to lower age before testing. These observations are further supported looking at the histograms shown below.\n\nconcrete_tbl %>%\n  pivot_longer(cement:age, names_to=\"ingredient\", values_to = \"amount\") %>%\n  ggplot(aes(x=amount)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(~ingredient, scales = \"free\")"
  },
  {
    "objectID": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#variable-correlations",
    "href": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#variable-correlations",
    "title": "Exploratory Analysis of the Concrete Dataset",
    "section": "Variable correlations",
    "text": "Variable correlations\nNext, we analyze the correlations between variables. When the amount of one ingredient is increased, we expect one or more of the other ingredients in the concrete mixture to decrease. So, some correlation between the concrete ingredients is expected.\n\nGGally::ggcorr(concrete_tbl)\n\n\n\n\nThe correlation analysis showed a strong, positive correlation with cement content and compressive strength and less strong correlations with age with compressive strength and superplasticizer with compressive strength. An inverse correlation between water and superplasticizer was detected perhaps due to the water content of the superplasticizer requiring less additional water in the formulation.\nAnother way of visualizing the correlation of variables with the property you wish to predict is the called a “correlation funnel”.\n\nconcrete_tbl %>%\n  binarize(n_bins = 3) %>%\n  correlate(`compressive_strength__41.36856_Inf`) %>%\n  plot_correlation_funnel(interactive = FALSE)\n\n\n\n\nThe correlation funnel shows some degree of correlation between the cement, water, superplasticizer and age with compressive strength. The fly ash, coarse and fine aggregate and blast furnace slag showed very little correlation with compressive strength."
  },
  {
    "objectID": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#summary",
    "href": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#summary",
    "title": "Exploratory Analysis of the Concrete Dataset",
    "section": "Summary",
    "text": "Summary\nThis post has shown several techniques for exploring the concrete dataset. The next post will use a generalized linear modeling approach to predict concrete compressive strength and compare the results with the conventional material modeling approach. Subsequent articles will use machine learning techniques such as artificial neural networks and extreme gradient boosting.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\nWarning: package 'sessioninfo' was built under R version 4.2.1\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package           * version date (UTC) lib source\n P correlationfunnel * 0.2.0   2020-06-09 [?] CRAN (R 4.2.1)\n P dplyr             * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats           * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P GGally            * 2.1.2   2021-06-21 [?] CRAN (R 4.2.1)\n P ggplot2           * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P purrr             * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P readr             * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl            * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P sessioninfo       * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P skimr             * 2.1.4   2022-04-15 [?] CRAN (R 4.2.1)\n P stringr           * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble            * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidyr             * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse         * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpolCg0M/renv-library-cb838f17a38\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2022-09-03_ConcreteGLM/2022-09-03_ConcreteGLM.html",
    "href": "posts/2022-09-03_ConcreteGLM/2022-09-03_ConcreteGLM.html",
    "title": "GLM model for Concrete Strength",
    "section": "",
    "text": "A generalized linear model (GLM) was built to predict the compressive strength of high-performance concrete formulations.\nAn elastic net regularization has been employed to develop the generalized linear model using the glmnet engine within the tidymodels framework. These results will be compared with a conventional materials modeling approach in the next post.\n##Load libraries and data"
  },
  {
    "objectID": "posts/2022-09-03_ConcreteGLM/2022-09-03_ConcreteGLM.html#stage-machine-learning-approach",
    "href": "posts/2022-09-03_ConcreteGLM/2022-09-03_ConcreteGLM.html#stage-machine-learning-approach",
    "title": "GLM model for Concrete Strength",
    "section": "3-Stage Machine Learning Approach",
    "text": "3-Stage Machine Learning Approach\nWe will utilize the 3-stage machine learning approach promoted by Matt Dancho at Business Science. He posted an excellent tutorial “Product Price Prediction: A Tidy Hyperparameter Tuning and Cross Validation Tutorial”. I haven’t found a better example of applying the tidymodels framework to develop a predictive model.\nThe 3-stage hyperparameter tuning process:\n1. Find Parameters: Use hyperparameter tuning on a “training dataset” that sections your training data into cross validation folds. The output of stage 1 is the parameter set.\n2. Compare and Select the Best Model: Evaluate the performance on a hidden “test dataset”. The output at Stage 2 is what we determined as the best model.\n3. Train Final Model: Once we have selected the best model, we train the full dataset. This model goes into production.\n\nStage 1: Find Parameters\nHere we want to make different machine learning models and try them out by performing the following steps: - Initial Splitting: Separate into random training and test datasets - Preprocessing: Make a pipeline to transform raw data into a dataset ready for machine learning - Cross Validation Specification: Sample the training data into splits - Model Specification: Select model algorithms and identify key tuning parameters - Grid Specification: Set up a grid using wise parameter choices - Hyperparameter Tuning: Implement the tuning process\nInitial splitting of the dataset into Training and Test Dataset Here we use the rsample package to create an 80/20 split. The concrete dataset contains 1030 formulations of which 825 are randomly assigned to training and 205 are randomly assigned to testing.\n\nset.seed(123)\nconcrete_split <- initial_split(concrete_tbl, prop = 0.80)\nconcrete_train <- training(concrete_split)\nconcrete_test <- testing(concrete_split)\n\nPreprocessing is accomplished by using the recipe package. The recipe provides the steps required to transform our raw data into a dataset suitable for machine learning. The Concrete dataset actually doesn’t require much reformatting. The major issue was the lengthy column names which was addressed immediately after the dataset was imported. The dataset contained all numerical values and no missing data. Initially we will just center and scale the predictors before sending to the glmnet model.\n\nconcrete_rec <- recipe(compressive_strength ~ ., data = concrete_train) %>%\n  step_center(all_predictors()) %>%\n  step_scale(all_predictors())\n\nconcrete_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          8\n\nOperations:\n\nCentering for all_predictors()\nScaling for all_predictors()\n\n\nCross validation folds are created in order to assess the performance of the model parameters. Here we use 5-fold cross validation to create splits from our training dataset and also using the preprocessing pipeline specified above.\n\nset.seed(234)\nconcrete_folds <- vfold_cv(concrete_train, v = 5)\n\nconcrete_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [659/165]> Fold1\n2 <split [659/165]> Fold2\n3 <split [659/165]> Fold3\n4 <split [659/165]> Fold4\n5 <split [660/164]> Fold5\n\n\nModel specifications are created using the parsnip package. Here we specify a linear regression model using the glmnet engine. glmnet uses an Elastic Net which combines LASSO and Ridge Regression techniques. This is a linear algorithm which may have difficulty with the skewed numeric data which is present in the Concrete dataset. Notice that the penalty and mixture parameters have been specified to be tuned.\n\nglmnet_spec <- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"regression\")\n\nglmnet_spec\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = tune()\n\nComputational engine: glmnet \n\n\nGrid specifications sets up a variety of parameter values used with our model to find which combination yields the lowest prediction error (or best accuracy). Here we specify the parameter ranges and grid function using the dials package.\nSpecify the grid function (max entropy, hypercube etc.). Here we make a grid of 20 values using the grid_max_entropy() function in the dials package. Since there are just 2 tuning parameters in this case, we can visualize the grid selections. Note the penalty parameter is on the log base 10 scale by default. The dials package helps us make smarter choices for the critical tuning parameters.\n\nset.seed(345)\nglmnet_grid <- grid_max_entropy(penalty(), mixture(), size = 20)\n\nglmnet_grid %>%\n  ggplot(aes(penalty, mixture)) +\n  geom_point(color = \"steelblue\", size = 3) +\n  scale_x_log10() +\n  theme_light() +\n  labs(title = \"Max Entropy Grid\", x = \"Penalty (log scale)\", y = \"Mixture\")\n\n\n\n\n\nconcrete_wf <- workflow() %>%\n  add_recipe(concrete_rec) %>%\n  add_model(glmnet_spec)\n\nHyperparameter tuning is now performed using the tune_grid() function from the tune package. Here we specific the formula, model, resamples, grid and metrics. The metrics come from the yardstick package. For regression problems, we can specify multiple metrics such as mae, mape, rmse and rsq into a metric_set().\n\ndoParallel::registerDoParallel()\n\nglmnet_res <- tune_grid(\n  concrete_wf,\n  resamples = concrete_folds,\n  grid = glmnet_grid,\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\nIdentify the best hyperparameter values using the show_best() function.\n\nglmnet_res %>% show_best(\"mae\", n = 5)\n\n# A tibble: 5 × 8\n   penalty mixture .metric .estimator  mean     n std_err .config              \n     <dbl>   <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1 1.09e- 9   0.155 mae     standard    8.10     5   0.216 Preprocessor1_Model06\n2 5.89e- 4   0.208 mae     standard    8.10     5   0.216 Preprocessor1_Model07\n3 4.05e-10   0.392 mae     standard    8.10     5   0.216 Preprocessor1_Model10\n4 1.16e- 6   0.340 mae     standard    8.10     5   0.216 Preprocessor1_Model08\n5 1.41e- 8   0.520 mae     standard    8.10     5   0.216 Preprocessor1_Model12\n\n\nVisualize the tuning results\n\n\n\n\n\n\n\nStage 2: Compare and Select the Best Model\nSelect the best parameters based on the lowest mean absolute error.\n\nparams_glmnet_best <- glmnet_res %>% select_best(\"mae\")\nparams_glmnet_best\n\n# A tibble: 1 × 3\n        penalty mixture .config              \n          <dbl>   <dbl> <chr>                \n1 0.00000000109   0.155 Preprocessor1_Model06\n\n\nFinalize the model with the best parameters.\n\nfinal_glmnet <- finalize_workflow(concrete_wf, params_glmnet_best)\n\nfinal_glmnet\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_center()\n• step_scale()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1.09262294094878e-09\n  mixture = 0.155459027038887\n\nComputational engine: glmnet \n\n\nWhich Features are most important?\n\nfinal_glmnet %>%\n  fit(data = concrete_train) %>%\n  pull_workflow_fit() %>%\n  vip(aesthetics = list(fill = \"steelblue\")) +\n  labs(title = \"GLMNET Model Importance - Compressive Strength (MPa) Prediction\")\n\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nPlease use `extract_fit_parsnip()` instead.\n\n\n\n\n\n\n\nStage 3: Train Final Model\nFit model on train and evaluate on test.\n\nfinal_res <- last_fit(final_glmnet, concrete_split, metrics = metric_set(rmse, rsq, mae))\n\nAssess final model performance metrics.\n\ncollect_metrics(final_res)\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      11.4   Preprocessor1_Model1\n2 rsq     standard       0.615 Preprocessor1_Model1\n3 mae     standard       9.09  Preprocessor1_Model1\n\n\nVisualize actual vs. predicted compressive strength for final model."
  },
  {
    "objectID": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html",
    "href": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html",
    "title": "Conventional Material Models for Concrete Dataset",
    "section": "",
    "text": "Fitting the concrete dataset to a pre-determined equation using a non-linear, least squares approximation.\nAbrams’ law states that the strength of a concrete mix is inversely related to the mass ratio of water to cement. In other words, as the water content increases, the strength of the concrete decreases. Experimental data however shows that this law does not provide the complete picture and concrete formulations with the same water:cement can have significantly different performance."
  },
  {
    "objectID": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#load-libraries-and-data",
    "href": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#load-libraries-and-data",
    "title": "Conventional Material Models for Concrete Dataset",
    "section": "Load libraries and data",
    "text": "Load libraries and data\n\nlibrary(readxl)\nlibrary(tidyverse)\n\ntheme_set(theme_light())\n\n\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"../data/\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))"
  },
  {
    "objectID": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#plot-compressive-strength-as-a-function-of-watercement",
    "href": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#plot-compressive-strength-as-a-function-of-watercement",
    "title": "Conventional Material Models for Concrete Dataset",
    "section": "Plot compressive strength as a function of water:cement",
    "text": "Plot compressive strength as a function of water:cement\n\nconcrete_tbl <- concrete_tbl %>%\n  mutate(water_cement = water / cement,\n         water_binder = water / (cement + blast_furnace_slag + fly_ash))\n\nconcrete_tbl %>%\n  ggplot(aes(water_cement, compressive_strength)) +\n  geom_point(alpha = 0.15) +\n  geom_smooth(formula = y ~ x, method = \"lm\") +\n  theme_light() +\n  labs(title = \"Concrete Compressive Strength vs. Water:Cement\",\n       x = \"Water:Cement\", y = \"Compressive Strength (MPa)\")\n\n\n\n\nIt is apparent from the plot above that water:cement is not the only factor important for determining the compressive strength of concrete. For example, there are multiple formulations with a water:cement of ~1 with a range of compressive strengths from less than 10 MPa to greater than 50 MPa. The age of the concrete at the time of testing is also recognized as an important factor in determining the concrete strength for a sample.\n\\[ f^\\prime_c(t) = a X^b \\cdot [c \\ln(t)+(d)] \\]\nwhere t = age at test, X = w/c or water-to-binder ratio and a, b, c, d are regression coefficients\nThe above equation also includes the age at test variable (t) to predict the compressive strength. This equation uses four parameters reminding me of the famous quote by mathematician John von Neumann, “with four parameters I can fit an elephant, with five I can make him wiggle his trunk.”\nThis equation is fit to the experimental dataset using non-linear least squares approximation. The nls function in base R has been used as shown below."
  },
  {
    "objectID": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#nls-fit-using-watercement",
    "href": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#nls-fit-using-watercement",
    "title": "Conventional Material Models for Concrete Dataset",
    "section": "NLS Fit using water:cement",
    "text": "NLS Fit using water:cement\n\nwc <- concrete_tbl$water_cement\nwb <- concrete_tbl$water_binder\nage <- concrete_tbl$age\ncs <- concrete_tbl$compressive_strength\n\ncsFunc <- function(wc, age, a, b, c, d) { (a * wc^b) + (c * log(age) + d)}\n\nFit with water:cement\n\ncsFit <- nls(cs ~ csFunc(wc, age, a, b, c, d), start=list(a=30, b=-0.6, c=0.3, d=0.1))\n\nsummary(csFit)\n\n\nFormula: cs ~ csFunc(wc, age, a, b, c, d)\n\nParameters:\n  Estimate Std. Error t value Pr(>|t|)    \na  14.0112     2.9035   4.826 1.61e-06 ***\nb  -1.0536     0.1357  -7.763 2.00e-14 ***\nc   8.1770     0.2587  31.608  < 2e-16 ***\nd -12.8289     3.1703  -4.047 5.59e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.869 on 1026 degrees of freedom\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 3.529e-06\n\n\nVisualize actual vs. predicted compressive strength for water:cement model.\n\n\n\n\n\n\nNLS Fit using Water:Binder\nFit with water:binder\n\ncsFit_wb <- nls(cs ~ csFunc(wb, age, a, b, c, d), start=list(a=10, b=-0.5, c=10, d=10))\n\nsummary(csFit_wb)\n\n\nFormula: cs ~ csFunc(wb, age, a, b, c, d)\n\nParameters:\n  Estimate Std. Error t value Pr(>|t|)    \na  23.5011     6.9342   3.389 0.000728 ***\nb  -0.8614     0.1440  -5.980 3.07e-09 ***\nc   8.5739     0.2043  41.971  < 2e-16 ***\nd -39.2343     8.2396  -4.762 2.20e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.774 on 1026 degrees of freedom\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 2.226e-06\n\n\nVisualize actual vs. predicted compressive strength for water:binder model."
  },
  {
    "objectID": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html",
    "href": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html",
    "title": "Neural Network for Concrete Dataset",
    "section": "",
    "text": "A single-layer neural network is fit to predict concrete compressive strength.\nIn this post, we will begin to use machine learning techniques for predicting compressive strength of formulations using the concrete dataset. In a previous post, we created a model using a conventional material modeling approach which resulted in an R2 of 0.78. Here we will use a single-layer neural network to predict compressive strength and compare the results with the conventional material model."
  },
  {
    "objectID": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#load-libraries-and-data",
    "href": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#load-libraries-and-data",
    "title": "Neural Network for Concrete Dataset",
    "section": "Load libraries and data",
    "text": "Load libraries and data\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n#Tidymodels\nlibrary(tidymodels)\nlibrary(vip)\n\n\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"../data/\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))"
  },
  {
    "objectID": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#stage-1-model-tuning",
    "href": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#stage-1-model-tuning",
    "title": "Neural Network for Concrete Dataset",
    "section": "Stage 1: Model Tuning",
    "text": "Stage 1: Model Tuning\nInitial splitting of the dataset into Training and Test Dataset Here we use the rsample package to create an 80/20 split. The concrete dataset contains 1030 formulations of which 825 are randomly assigned to training and 205 are randomly assigned to testing.\n\nset.seed(123)\nconcrete_split <- initial_split(concrete_tbl, prop = 0.80)\nconcrete_train <- training(concrete_split)\nconcrete_test <- testing(concrete_split)\n\nPreprocessing is accomplished by using the recipe package. The recipe provides the steps required to transform our raw data into a dataset suitable for machine learning. The Concrete dataset actually doesn’t require much reformatting. The major issue was the lengthy column names which was addressed immediately after the dataset was imported. The dataset contained all numerical values and no missing data. Initially we will just center and scale the predictors before sending to the nnet model.\n\nconcrete_rec <- recipe(compressive_strength ~ ., data = concrete_train) %>%\n  step_center(all_predictors()) %>%\n  step_scale(all_predictors())\n\nconcrete_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          8\n\nOperations:\n\nCentering for all_predictors()\nScaling for all_predictors()\n\n\nCross validation folds are created in order to assess the performance of the model parameters. Here we use 5-fold cross validation to create splits from our training dataset and also using the preprocessing pipeline specified above.\n\nset.seed(234)\nconcrete_folds <- vfold_cv(concrete_train, v = 5)\n\nconcrete_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [659/165]> Fold1\n2 <split [659/165]> Fold2\n3 <split [659/165]> Fold3\n4 <split [659/165]> Fold4\n5 <split [660/164]> Fold5\n\n\nModel specifications are created using the parsnip package. Here we specify a multi-layer perceptron (mlp) using the nnet engine. The multi-layer perceptron is a single-layer neural network. Notice that the penalty and number of hidden units parameters have been specified to be tuned.\n\nmlp_spec = mlp(\n  hidden_units = tune(),\n  penalty = tune(),\n  epochs = 3000,\n  #activation = \"linear\"\n) %>%\n  set_engine(\"nnet\") %>%\n  #set_engine(\"nnet\", objective = \"reg:squarederror\") %>%\n  set_mode(\"regression\")\n\nmlp_spec\n\nSingle Layer Neural Network Model Specification (regression)\n\nMain Arguments:\n  hidden_units = tune()\n  penalty = tune()\n  epochs = 3000\n\nComputational engine: nnet \n\n\nGrid specifications sets up a variety of parameter values used with our model to find which combination yields the lowest prediction error (or best accuracy). Here we specify the parameter ranges and grid function using the dials package.\nSpecify the grid function (max entropy, hypercube etc.). Here we make a grid of 20 values using the grid_max_entropy() function in the dials package. Since there are just 2 tuning parameters in this case, we can visualize the grid selections. Note the penalty parameter is on the log base 10 scale by default. The dials package helps us make smarter choices for the critical tuning parameters.\n\nset.seed(345)\nnnet_grid <- grid_max_entropy(penalty(), hidden_units(), size = 20)\n\nnnet_grid %>%\n  ggplot(aes(penalty, hidden_units)) +\n  geom_point(color = \"steelblue\", size = 3) +\n  scale_x_log10() +\n  theme_light() +\n  labs(title = \"Max Entropy Grid\", x = \"Penalty (log scale)\", y = \"Hidden Units\")\n\n\n\n\nDefine a workflow for the tuning process\n\nconcrete_wf <- workflow() %>%\n  add_recipe(concrete_rec) %>%\n  add_model(mlp_spec)\n\nHyperparameter tuning is now performed using the tune_grid() function from the tune package. Here we specific the formula, model, resamples, grid and metrics. The metrics come from the yardstick package. For regression problems, we can specify multiple metrics such as mae, mape, rmse and rsq into a metric_set().\n\ndoParallel::registerDoParallel()\n\nbegin <- Sys.time()\n\nset.seed(456)\n\nnnet_res <- tune_grid(\n  concrete_wf,\n  resamples = concrete_folds,\n  grid = nnet_grid,\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\nend1 <- Sys.time() - begin"
  },
  {
    "objectID": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#stage-2-compare-and-select-the-best-model",
    "href": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#stage-2-compare-and-select-the-best-model",
    "title": "Neural Network for Concrete Dataset",
    "section": "Stage 2: Compare and Select the Best Model",
    "text": "Stage 2: Compare and Select the Best Model\nIdentify the best hyperparameter values using the show_best() function.\n\nnnet_res %>% show_best(\"mae\", n = 5)\n\n# A tibble: 5 × 8\n  hidden_units   penalty .metric .estimator  mean     n std_err .config         \n         <int>     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>           \n1           10 0.0928    mae     standard    4.14     5  0.234  Preprocessor1_M…\n2           10 0.00165   mae     standard    4.32     5  0.189  Preprocessor1_M…\n3            7 0.0000358 mae     standard    4.36     5  0.322  Preprocessor1_M…\n4            7 0.316     mae     standard    4.40     5  0.0475 Preprocessor1_M…\n5            6 0.00285   mae     standard    4.61     5  0.315  Preprocessor1_M…\n\n\nVisualize the tuning results\n\n\n\n\n\nSelect the best parameters based on the lowest mean absolute error.\n\nparams_nnet_best <- nnet_res %>% select_best(\"mae\")\nparams_nnet_best\n\n# A tibble: 1 × 3\n  hidden_units penalty .config              \n         <int>   <dbl> <chr>                \n1           10  0.0928 Preprocessor1_Model10\n\n\n\nfinal_nnet <- finalize_workflow(concrete_wf, params_nnet_best)\n\nfinal_nnet\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: mlp()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_center()\n• step_scale()\n\n── Model ───────────────────────────────────────────────────────────────────────\nSingle Layer Neural Network Model Specification (regression)\n\nMain Arguments:\n  hidden_units = 10\n  penalty = 0.0927943905608392\n  epochs = 3000\n\nComputational engine: nnet \n\n\n\nset.seed(567)\n\nfinal_nnet %>%\n  fit(data = concrete_train) %>%\n  extract_fit_parsnip() %>%\n  vip(aesthetics = list(fill = \"steelblue\")) +\n  labs(title = \"NNET Model Importance - Compressive Strength (MPa) Prediction\")"
  },
  {
    "objectID": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#stage-3-train-final-model",
    "href": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#stage-3-train-final-model",
    "title": "Neural Network for Concrete Dataset",
    "section": "Stage 3: Train Final Model",
    "text": "Stage 3: Train Final Model\nFit model on train and evaluate on test.\n\nset.seed(678)\n\nfinal_res <- last_fit(final_nnet, concrete_split, metrics = metric_set(rmse, rsq, mae))\n\nAssess final model performance metrics.\n\ncollect_metrics(final_res)\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       6.53  Preprocessor1_Model1\n2 rsq     standard       0.875 Preprocessor1_Model1\n3 mae     standard       4.94  Preprocessor1_Model1\n\n\nVisualize actual vs. predicted compressive strength for final model."
  },
  {
    "objectID": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#summary",
    "href": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#summary",
    "title": "Neural Network for Concrete Dataset",
    "section": "Summary",
    "text": "Summary\nThe single-layer, feed-forward neural network had an R2 of 0.87 and RMSE of 6.5 MPa.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version date (UTC) lib source\n P broom        * 1.0.1   2022-08-29 [?] CRAN (R 4.2.1)\n P dials        * 1.0.0   2022-06-14 [?] CRAN (R 4.2.1)\n P dplyr        * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats      * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2      * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P infer        * 1.0.3   2022-08-22 [?] CRAN (R 4.2.1)\n P modeldata    * 1.0.0   2022-07-01 [?] CRAN (R 4.2.1)\n P parsnip      * 1.0.0   2022-06-16 [?] CRAN (R 4.2.1)\n P purrr        * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P readr        * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl       * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P recipes      * 1.0.1   2022-07-07 [?] CRAN (R 4.2.1)\n P rsample      * 1.0.0   2022-06-24 [?] CRAN (R 4.2.1)\n P scales       * 1.2.1   2022-08-20 [?] CRAN (R 4.2.1)\n P sessioninfo  * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr      * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble       * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidymodels   * 1.0.0   2022-07-13 [?] CRAN (R 4.2.1)\n P tidyr        * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse    * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n P tune         * 1.0.0   2022-07-07 [?] CRAN (R 4.2.1)\n P vip          * 0.3.2   2020-12-17 [?] CRAN (R 4.0.5)\n P workflows    * 1.0.0   2022-07-05 [?] CRAN (R 4.2.1)\n P workflowsets * 1.0.0   2022-07-12 [?] CRAN (R 4.2.1)\n P yardstick    * 1.0.0   2022-06-06 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpE9tHQu/renv-library-30081bca49\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html",
    "href": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html",
    "title": "Prediction of Concrete Strength using XGBoost",
    "section": "",
    "text": "A gradient boosting model to predict the compressive strength of concrete was built using a tidymodels approach.\nIn this post, we will begin to use machine learning techniques for predicting compressive strength of formulations using the concrete dataset. In a previous post, we created a model using a conventional material modeling approach which resulted in an R2 of 0.78. Here we will use an XGBoost model to predict compressive strength and compare the results with a conventional material model."
  },
  {
    "objectID": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html#load-libraries-and-data",
    "href": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html#load-libraries-and-data",
    "title": "Prediction of Concrete Strength using XGBoost",
    "section": "Load libraries and data",
    "text": "Load libraries and data\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(ragg)\n\n#Tidymodels\nlibrary(tidymodels)\nlibrary(xgboost)\nlibrary(vip)\n\n\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"../data/\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))"
  },
  {
    "objectID": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html#stage-1-model-tuning",
    "href": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html#stage-1-model-tuning",
    "title": "Prediction of Concrete Strength using XGBoost",
    "section": "Stage 1: Model Tuning",
    "text": "Stage 1: Model Tuning\nInitial splitting of the dataset into Training and Test Dataset Here we use the rsample package to create an 80/20 split. The concrete dataset contains 1030 formulations of which 825 are randomly assigned to training and 205 are randomly assigned to testing.\n\nset.seed(123)\nconcrete_split <- initial_split(concrete_tbl, prop = 0.80)\nconcrete_train <- training(concrete_split)\nconcrete_test <- testing(concrete_split)\n\nPreprocessing is accomplished by using the recipe package. The recipe provides the steps required to transform our raw data into a dataset suitable for machine learning. The Concrete dataset actually doesn’t require much reformatting. The major issue was the lengthy column names which was addressed immediately after the dataset was imported. The dataset contained all numerical values and no missing data. Initially we will just center and scale the predictors before sending to the nnet model.\n\nconcrete_rec <- recipe(compressive_strength ~ ., data = concrete_train) %>%\n  step_center(all_predictors()) %>%\n  step_scale(all_predictors())\n\nconcrete_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          8\n\nOperations:\n\nCentering for all_predictors()\nScaling for all_predictors()\n\n\nCross validation folds are created in order to assess the performance of the model parameters. Here we use 5-fold cross validation to create splits from our training dataset and also using the preprocessing pipeline specified above.\n\nset.seed(234)\nconcrete_folds <- vfold_cv(concrete_train, v = 5)\n\nconcrete_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [659/165]> Fold1\n2 <split [659/165]> Fold2\n3 <split [659/165]> Fold3\n4 <split [659/165]> Fold4\n5 <split [660/164]> Fold5\n\n\nModel specifications are created using the parsnip package. Here we specify a boosted tree model using the XGBoost engine. Notice that the min n, tree depth and learn rate parameters have been specified to be tuned.\n\nxgboost_spec = boost_tree(\n  mode = \"regression\",\n  trees = 1000,\n  min_n = tune(),\n  tree_depth = tune(),\n  learn_rate = tune()\n) %>%\n  set_engine(\"xgboost\", objective = \"reg:squarederror\") %>%\n  set_mode(\"regression\")\n\nxgboost_spec\n\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n\nEngine-Specific Arguments:\n  objective = reg:squarederror\n\nComputational engine: xgboost \n\n\nGrid specifications sets up a variety of parameter values used with our model to find which combination yields the lowest prediction error (or best accuracy). Here we specify the parameter ranges and grid function using the dials package.\n\nset.seed(345)\nxgboost_grid <- grid_max_entropy(min_n(), tree_depth(), learn_rate(), size = 30)\n\nxgboost_grid\n\n# A tibble: 30 × 3\n   min_n tree_depth learn_rate\n   <int>      <int>      <dbl>\n 1     6          1   8.78e- 7\n 2    38          3   9.89e- 8\n 3    30          1   1.18e- 2\n 4    23          5   3.77e- 7\n 5    37         15   1.07e-10\n 6    16          6   3.19e- 4\n 7    12          7   1.34e-10\n 8    40         10   9.93e- 8\n 9     2         15   2.08e- 8\n10    36         10   2.64e- 2\n# … with 20 more rows\n\n\nDefine a workflow for the tuning process\n\nconcrete_wf <- workflow() %>%\n  add_recipe(concrete_rec) %>%\n  add_model(xgboost_spec)\n\nHyperparameter tuning is now performed using the tune_grid() function from the tune package. Here we specific the formula, model, resamples, grid and metrics. The metrics come from the yardstick package. For regression problems, we can specify multiple metrics such as mae, mape, rmse and rsq into a metric_set().\n\ndoParallel::registerDoParallel()\n\nset.seed(456)\n\nbegin <- Sys.time()\n\nxgboost_res <- tune_grid(\n  concrete_wf,\n  resamples = concrete_folds,\n  grid = xgboost_grid,\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\nend1 <- Sys.time() - begin"
  },
  {
    "objectID": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html#stage-2-compare-and-select-the-best-model",
    "href": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html#stage-2-compare-and-select-the-best-model",
    "title": "Prediction of Concrete Strength using XGBoost",
    "section": "Stage 2: Compare and Select the Best Model",
    "text": "Stage 2: Compare and Select the Best Model\nIdentify the best hyperparameter values using the show_best() function.\n\nxgboost_res %>% show_best(\"mae\", n = 5)\n\n# A tibble: 5 × 9\n  min_n tree_depth learn_rate .metric .estimator  mean     n std_err .config    \n  <int>      <int>      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>      \n1    18         10    0.0265  mae     standard    2.98     5  0.103  Preprocess…\n2    28          7    0.0302  mae     standard    3.10     5  0.103  Preprocess…\n3    36         10    0.0264  mae     standard    3.14     5  0.0829 Preprocess…\n4    19          2    0.0904  mae     standard    3.37     5  0.110  Preprocess…\n5    24         14    0.00824 mae     standard    3.46     5  0.0668 Preprocess…\n\n\nVisualize the tuning results\n\n\n\n\n\nSelect the best parameters based on the lowest mean absolute error.\n\nparams_xgboost_best <- xgboost_res %>% select_best(\"mae\")\nparams_xgboost_best\n\n# A tibble: 1 × 4\n  min_n tree_depth learn_rate .config              \n  <int>      <int>      <dbl> <chr>                \n1    18         10     0.0265 Preprocessor1_Model14\n\n\nFinalize workflow with the best model parameters\n\nfinal_xgboost <- finalize_workflow(concrete_wf, params_xgboost_best)\n\nfinal_xgboost\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_center()\n• step_scale()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 18\n  tree_depth = 10\n  learn_rate = 0.0264752492619167\n\nEngine-Specific Arguments:\n  objective = reg:squarederror\n\nComputational engine: xgboost \n\n\nWhich Features are most important?\n\nfinal_xgboost %>%\n  fit(data = concrete_train) %>%\n  extract_fit_parsnip() %>%\n  vip(aesthetics = list(fill = \"steelblue\")) +\n  labs(title = \"XGBoost Model Importance - Compressive Strength (MPa) Prediction\")\n\n\n\n\n\nStage 3: Train Final Model\nFit model on train and evaluate on test.\n\nfinal_res <- last_fit(final_xgboost, concrete_split, metrics = metric_set(rmse, rsq, mae))\n\nAssess final model performance metrics.\n\ncollect_metrics(final_res)\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       4.33  Preprocessor1_Model1\n2 rsq     standard       0.945 Preprocessor1_Model1\n3 mae     standard       2.69  Preprocessor1_Model1\n\n\nVisualize actual vs. predicted compressive strength for final model."
  },
  {
    "objectID": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html#summary",
    "href": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html#summary",
    "title": "Prediction of Concrete Strength using XGBoost",
    "section": "Summary",
    "text": "Summary\nThe XGBoost model to predict the compressive strength of concrete performed better (RMSE = 4.3 MPa, R2 = 0.945) than a conventional materials model(RMSE = 7.9 MPa, R2 = 0.78).\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version date (UTC) lib source\n P broom        * 1.0.1   2022-08-29 [?] CRAN (R 4.2.1)\n P dials        * 1.0.0   2022-06-14 [?] CRAN (R 4.2.1)\n P dplyr        * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats      * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2      * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P infer        * 1.0.3   2022-08-22 [?] CRAN (R 4.2.1)\n P modeldata    * 1.0.0   2022-07-01 [?] CRAN (R 4.2.1)\n P parsnip      * 1.0.0   2022-06-16 [?] CRAN (R 4.2.1)\n P purrr        * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P ragg         * 1.2.2   2022-02-21 [?] CRAN (R 4.2.1)\n P readr        * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl       * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P recipes      * 1.0.1   2022-07-07 [?] CRAN (R 4.2.1)\n P rsample      * 1.0.0   2022-06-24 [?] CRAN (R 4.2.1)\n P scales       * 1.2.1   2022-08-20 [?] CRAN (R 4.2.1)\n P sessioninfo  * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr      * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble       * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidymodels   * 1.0.0   2022-07-13 [?] CRAN (R 4.2.1)\n P tidyr        * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse    * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n P tune         * 1.0.0   2022-07-07 [?] CRAN (R 4.2.1)\n P vip          * 0.3.2   2020-12-17 [?] CRAN (R 4.0.5)\n P workflows    * 1.0.0   2022-07-05 [?] CRAN (R 4.2.1)\n P workflowsets * 1.0.0   2022-07-12 [?] CRAN (R 4.2.1)\n P xgboost      * 1.6.0.1 2022-04-16 [?] CRAN (R 4.2.1)\n P yardstick    * 1.0.0   2022-06-06 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpquYv5l/renv-library-21f850a61f6a\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html",
    "href": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html",
    "title": "Random Forest Model for Concrete Dataset",
    "section": "",
    "text": "A predictive model for compressive strength of concrete is built using a random forest algorithm.\nIn this post, we will begin to use machine learning techniques for predicting compressive strength of formulations using the concrete dataset. In a previous post, we created a model using a conventional material modeling approach which resulted in an R2 of 0.78. Here we will use a random forest model to predict compressive strength and compare the results with the conventional material model."
  },
  {
    "objectID": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html#load-libraries-and-data",
    "href": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html#load-libraries-and-data",
    "title": "Random Forest Model for Concrete Dataset",
    "section": "Load libraries and data",
    "text": "Load libraries and data\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n#Tidymodels\nlibrary(tidymodels)\nlibrary(ranger)\nlibrary(vip)\n\n\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"../data/\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))"
  },
  {
    "objectID": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html#stage-1-model-tuning",
    "href": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html#stage-1-model-tuning",
    "title": "Random Forest Model for Concrete Dataset",
    "section": "Stage 1: Model Tuning",
    "text": "Stage 1: Model Tuning\nInitial splitting of the dataset into Training and Test Dataset Here we use the rsample package to create an 80/20 split. The concrete dataset contains 1030 formulations of which 825 are randomly assigned to training and 205 are randomly assigned to testing.\n\nset.seed(123)\nconcrete_split <- initial_split(concrete_tbl, prop = 0.80)\nconcrete_train <- training(concrete_split)\nconcrete_test <- testing(concrete_split)\n\nPreprocessing is accomplished by using the recipe package. The recipe provides the steps required to transform our raw data into a dataset suitable for machine learning. The Concrete dataset actually doesn’t require much reformatting. The major issue was the lengthy column names which was addressed immediately after the dataset was imported. The dataset contained all numerical values and no missing data. Initially we will just center and scale the predictors before sending to the nnet model.\n\nconcrete_rec <- recipe(compressive_strength ~ ., data = concrete_train) %>%\n  step_center(all_predictors()) %>%\n  step_scale(all_predictors())\n\nconcrete_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          8\n\nOperations:\n\nCentering for all_predictors()\nScaling for all_predictors()\n\n\nCross validation folds are created in order to assess the performance of the model parameters. Here we use 5-fold cross validation to create splits from our training dataset and also using the preprocessing pipeline specified above.\n\nset.seed(234)\nconcrete_folds <- vfold_cv(concrete_train, v = 5)\n\nconcrete_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [659/165]> Fold1\n2 <split [659/165]> Fold2\n3 <split [659/165]> Fold3\n4 <split [659/165]> Fold4\n5 <split [660/164]> Fold5\n\n\nModel specifications are created using the parsnip package. Here we specify a random forest model using the ranger engine. Notice that the min n and mtry parameters have been specified to be tuned.\n\nrf_spec = rand_forest(\n  trees = 1000,\n  min_n = tune(),\n  mtry = tune()\n) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\nrf_spec\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n\nComputational engine: ranger \n\n\nGrid specifications sets up a variety of parameter values used with our model to find which combination yields the lowest prediction error (or best accuracy). Here we specify the parameter ranges and grid function using the dials package.\nSpecify the grid function (max entropy, hypercube etc.). Here we make a grid of 20 values using the grid_max_entropy() function in the dials package. Since there are just 2 tuning parameters in this case, we can visualize the grid selections. Note the penalty parameter is on the log base 10 scale by default. The dials package helps us make smarter choices for the critical tuning parameters.\n\nset.seed(345)\nrf_grid <- grid_max_entropy(min_n(), mtry(c(1L, 10L)), size = 20)\n\nrf_grid %>%\n  ggplot(aes(min_n, mtry)) +\n  geom_point(color = \"steelblue\", size = 3) +\n  #scale_x_log10() +\n  theme_light() +\n  labs(title = \"Max Entropy Grid\", x = \"min n\", y = \"mtry\")\n\n\n\n\nDefine a workflow for the tuning process\n\nconcrete_wf <- workflow() %>%\n  add_recipe(concrete_rec) %>%\n  add_model(rf_spec)\n\nHyperparameter tuning is now performed using the tune_grid() function from the tune package. Here we specific the formula, model, resamples, grid and metrics. The metrics come from the yardstick package. For regression problems, we can specify multiple metrics such as mae, mape, rmse and rsq into a metric_set().\n\ndoParallel::registerDoParallel()\n\nset.seed(456)\n\nbegin <- Sys.time()\n\nrf_res <- tune_grid(\n  concrete_wf,\n  resamples = concrete_folds,\n  grid = rf_grid,\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\nend1 <- Sys.time() - begin"
  },
  {
    "objectID": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html#stage-2-compare-and-select-the-best-model",
    "href": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html#stage-2-compare-and-select-the-best-model",
    "title": "Random Forest Model for Concrete Dataset",
    "section": "Stage 2: Compare and Select the Best Model",
    "text": "Stage 2: Compare and Select the Best Model\nIdentify the best hyperparameter values using the show_best() function.\n\nrf_res %>% show_best(\"mae\", n = 5)\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     7     2 mae     standard    3.67     5  0.0647 Preprocessor1_Model01\n2     5     4 mae     standard    3.73     5  0.0648 Preprocessor1_Model14\n3     9     6 mae     standard    3.79     5  0.0695 Preprocessor1_Model15\n4     6    10 mae     standard    3.96     5  0.0734 Preprocessor1_Model09\n5     2     6 mae     standard    4.36     5  0.105  Preprocessor1_Model16\n\n\nVisualize the tuning results\n\nautoplot(rf_res)\n\n\n\n\nSelect the best parameters based on the lowest mean absolute error.\n\nparams_rf_best <- rf_res %>% select_best(\"mae\")\nparams_rf_best\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1     7     2 Preprocessor1_Model01\n\n\nFinalize workflow with the best model parameters\n\nfinal_rf <- finalize_workflow(concrete_wf, params_rf_best)\n\nfinal_rf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_center()\n• step_scale()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 7\n  trees = 1000\n  min_n = 2\n\nComputational engine: ranger \n\n\nWhich Features are most important? For random forest, we are defining the importance measure as permutation which requires a new specification since including this calculation in the initial specification would slow down the tuning process.\n\nimp_spec <- rf_spec %>%\n  finalize_model(params_rf_best) %>%\n  set_engine(\"ranger\", importance = \"permutation\")\n\nworkflow() %>%\n  add_recipe(concrete_rec) %>%\n  add_model(imp_spec) %>%\n  fit(data = concrete_train) %>%\n  extract_fit_parsnip() %>%\n  vip(aesthetics = list(fill = \"steelblue\")) +\n  labs(title = \"Random Forest Model Importance - Compressive Strength (MPa) Prediction\")\n\n\n\n\n\nStage 3: Train Final Model\nFit model on train and evaluate on test.\n\nfinal_res <- last_fit(final_rf, concrete_split, metrics = metric_set(rmse, rsq, mae))\n\nAssess final model performance metrics.\n\ncollect_metrics(final_res)\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       5.07  Preprocessor1_Model1\n2 rsq     standard       0.929 Preprocessor1_Model1\n3 mae     standard       3.31  Preprocessor1_Model1\n\n\nVisualize actual vs. predicted compressive strength for final model.\n\ncollect_predictions(final_res) %>%\n  ggplot(aes(compressive_strength, .pred)) +\n  geom_abline(slope = 1, lty = 2, color = \"gray50\", alpha = 0.5) +\n  geom_point(alpha = 0.6, color = \"midnightblue\") +\n  ylim(0, NA) +\n  labs(title = \"Random Forest Model Performance for Concrete Dataset\", \n       x = \"Actual Compressive Strength (MPa)\", \n       y = \"Predicted Compressive Strength (MPa)\")"
  },
  {
    "objectID": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html#summary",
    "href": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html#summary",
    "title": "Random Forest Model for Concrete Dataset",
    "section": "Summary",
    "text": "Summary\nThe random forest model to predict the compressive strength of concrete performed better (RMSE = 5.0 MPa, R2 = 0.93) than a conventional materials model(RMSE = 7.9 MPa, R2 = 0.78) but not quite as good as the XGBoost model (RMSE = 4.3 MPa, R2 = 0.945).\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version date (UTC) lib source\n P broom        * 1.0.1   2022-08-29 [?] CRAN (R 4.2.1)\n P dials        * 1.0.0   2022-06-14 [?] CRAN (R 4.2.1)\n P dplyr        * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats      * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2      * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P infer        * 1.0.3   2022-08-22 [?] CRAN (R 4.2.1)\n P modeldata    * 1.0.0   2022-07-01 [?] CRAN (R 4.2.1)\n P parsnip      * 1.0.0   2022-06-16 [?] CRAN (R 4.2.1)\n P purrr        * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P ranger       * 0.14.1  2022-06-18 [?] CRAN (R 4.2.1)\n P readr        * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl       * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P recipes      * 1.0.1   2022-07-07 [?] CRAN (R 4.2.1)\n P rsample      * 1.0.0   2022-06-24 [?] CRAN (R 4.2.1)\n P scales       * 1.2.1   2022-08-20 [?] CRAN (R 4.2.1)\n P sessioninfo  * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr      * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble       * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidymodels   * 1.0.0   2022-07-13 [?] CRAN (R 4.2.1)\n P tidyr        * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse    * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n P tune         * 1.0.0   2022-07-07 [?] CRAN (R 4.2.1)\n P vip          * 0.3.2   2020-12-17 [?] CRAN (R 4.0.5)\n P workflows    * 1.0.0   2022-07-05 [?] CRAN (R 4.2.1)\n P workflowsets * 1.0.0   2022-07-12 [?] CRAN (R 4.2.1)\n P yardstick    * 1.0.0   2022-06-06 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpIPfqiD/renv-library-11c44d35eb2\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html",
    "href": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html",
    "title": "Summary of Concrete Models",
    "section": "",
    "text": "A comparison of the predictive performance and speed for the different modeling approaches.\nSeveral models have been created to predict the compressive strength of high performance concrete based on the I-Cheng Yeh dataset. A conventional material model using a pre-determined transfer function which was fit to the data using a non-linear least squares approach. Four different models were created using machine learning algorithms, elastic net (glmnet), single-layer neural net (nnet), random forest (ranger) and boosted tree (xgboost), applied to the dataset."
  },
  {
    "objectID": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html#load-libraries-and-data",
    "href": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html#load-libraries-and-data",
    "title": "Summary of Concrete Models",
    "section": "Load libraries and data",
    "text": "Load libraries and data\n\n\nCode\nlibrary(knitr)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n\n\nCode\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"../data/\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))"
  },
  {
    "objectID": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html#predictive-accuracy",
    "href": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html#predictive-accuracy",
    "title": "Summary of Concrete Models",
    "section": "Predictive Accuracy",
    "text": "Predictive Accuracy\nEach model performance was assessed by several metrics: R-squared (R2), Root Mean Square Error (RMSE) and Mean Absolute Error (MAE).\n\n\n\n\n\n\n\n\n\n\nmetric\nnls\nglm\nmlp\nrf\nxgboost\n\n\n\n\nrmse\n7.76\n11.37\n6.53\n5.08\n4.33\n\n\nrsq\n0.78\n0.62\n0.88\n0.93\n0.95\n\n\nmae\n5.95\n9.09\n4.94\n3.32\n2.69\n\n\n\n\n\nAs shown in the figure and table above, the random forest (rf) and boosted tree (xgboost) models showed a significant improvement in predictive capability as compared with the conventional modeling approach (nls). The xgboost model had an R2 of 0.95 compared to 0.78 for the nls model with similar improvements in root mean square error and mean absolute error. The glmnet model gave worse performance than the non-linear models."
  },
  {
    "objectID": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html#benchmark-performance",
    "href": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html#benchmark-performance",
    "title": "Summary of Concrete Models",
    "section": "Benchmark performance",
    "text": "Benchmark performance\nHere we load the final models for the different approaches for comparison of prediction time. In this case, the time to make predictions for 10,300 (10 times the original dataset) was determined as a benchmark.\n\n\nCode\n#load all the models\nconcrete_nls <- readRDS(\"../results/concrete_nls_model.rds\")\nconcrete_glm <- readRDS(\"../results/concrete_glm_model.rds\")\nconcrete_mlp <- readRDS(\"../results/concrete_mlp_model.rds\")\nconcrete_rf <- readRDS(\"../results/concrete_rf_model.rds\")\nconcrete_xgb <- readRDS(\"../results/concrete_xgb_model.rds\")\n\n\nCreate the prediction dataset using 10 times the original dataset for purpose of comparing very fast prediction times.\n\n\nCode\ntemp <- concrete_tbl %>% slice(rep(row_number(), 10))\n\n\nBenchmarking was performed in the following manner using Sys.time to capture the time before and after each set of model predictions.\n\n\nCode\nbegin <- Sys.time()\na_temp <- predict(concrete_nls, new_data = temp)\nend1 <- Sys.time()\n\nb_temp <- predict(concrete_glm, new_data = temp)\nend2 <- Sys.time()\n\nc_temp <- predict(concrete_mlp, new_data = temp)\nend3 <- Sys.time()\n\nd_temp <- predict(concrete_rf, new_data = temp)\nend4 <- Sys.time()\n\ne_temp <- predict(concrete_xgb, new_data = temp)\nend5 <- Sys.time()\n\n# print(end1 - begin)[[1]]\n# print(end2 - end1)[[1]]\n# print(end3 - end2)[[1]]\n# print(end4 - end3)[[1]]\n\n#rm(temp)\n\n\nAs shown in the figure and table below, the xgboost model was the slowest taking about 1 second to perform 10,300 predictions. For the example of making a prediction of compressive strength of concrete for a particular formulation, however, this amount of time is trivial and the increased accuracy would be preferred over a faster and less accurate model.\n\n\n\n\n\n\n\n\n\n\nModel\nTime (ms)\n\n\n\n\nnls\n2\n\n\nglm\n132\n\n\nmlp\n60\n\n\nrf\n488\n\n\nxgboost\n555"
  },
  {
    "objectID": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html#summary",
    "href": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html#summary",
    "title": "Summary of Concrete Models",
    "section": "Summary",
    "text": "Summary\nPrediction with the conventional model (nls) is about two orders of magnitude faster than the boosted tree model (xgboost). It should be noted that the random forest model is about 40% faster than the xgboost model, in this case, with similar predictive accuracy.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version date (UTC) lib source\n P broom        * 1.0.1   2022-08-29 [?] CRAN (R 4.2.1)\n P dials        * 1.0.0   2022-06-14 [?] CRAN (R 4.2.1)\n P dplyr        * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats      * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2      * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P infer        * 1.0.3   2022-08-22 [?] CRAN (R 4.2.1)\n P knitr        * 1.40    2022-08-24 [?] CRAN (R 4.2.1)\n P modeldata    * 1.0.0   2022-07-01 [?] CRAN (R 4.2.1)\n P parsnip      * 1.0.0   2022-06-16 [?] CRAN (R 4.2.1)\n P purrr        * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P readr        * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl       * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P recipes      * 1.0.1   2022-07-07 [?] CRAN (R 4.2.1)\n P rsample      * 1.0.0   2022-06-24 [?] CRAN (R 4.2.1)\n P scales       * 1.2.1   2022-08-20 [?] CRAN (R 4.2.1)\n P sessioninfo  * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr      * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble       * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidymodels   * 1.0.0   2022-07-13 [?] CRAN (R 4.2.1)\n P tidyr        * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse    * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n P tune         * 1.0.0   2022-07-07 [?] CRAN (R 4.2.1)\n P workflows    * 1.0.0   2022-07-05 [?] CRAN (R 4.2.1)\n P workflowsets * 1.0.0   2022-07-12 [?] CRAN (R 4.2.1)\n P yardstick    * 1.0.0   2022-06-06 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpYhQbrp/renv-library-2ff468663bb8\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-09-03_ConcreteGLM/2022-09-03_ConcreteGLM.html#summary",
    "href": "posts/2022-09-03_ConcreteGLM/2022-09-03_ConcreteGLM.html#summary",
    "title": "GLM model for Concrete Strength",
    "section": "Summary",
    "text": "Summary\nThe regularized linear model had relatively poor predictive performance (RMSE = 11.4 MPa, R2 = 0.62). A non-linear, least squares model will be built with better model performance in the next post.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version date (UTC) lib source\n P broom        * 1.0.1   2022-08-29 [?] CRAN (R 4.2.1)\n P dials        * 1.0.0   2022-06-14 [?] CRAN (R 4.2.1)\n P dplyr        * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats      * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2      * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P glmnet       * 4.1-1   2021-02-21 [?] CRAN (R 4.0.5)\n P infer        * 1.0.3   2022-08-22 [?] CRAN (R 4.2.1)\n P Matrix       * 1.4-1   2022-03-23 [?] CRAN (R 4.2.0)\n P modeldata    * 1.0.0   2022-07-01 [?] CRAN (R 4.2.1)\n P parsnip      * 1.0.0   2022-06-16 [?] CRAN (R 4.2.1)\n P purrr        * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P readr        * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl       * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P recipes      * 1.0.1   2022-07-07 [?] CRAN (R 4.2.1)\n P rsample      * 1.0.0   2022-06-24 [?] CRAN (R 4.2.1)\n P scales       * 1.2.1   2022-08-20 [?] CRAN (R 4.2.1)\n P sessioninfo  * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr      * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble       * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidymodels   * 1.0.0   2022-07-13 [?] CRAN (R 4.2.1)\n P tidyr        * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse    * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n P tune         * 1.0.0   2022-07-07 [?] CRAN (R 4.2.1)\n P vip          * 0.3.2   2020-12-17 [?] CRAN (R 4.0.5)\n P workflows    * 1.0.0   2022-07-05 [?] CRAN (R 4.2.1)\n P workflowsets * 1.0.0   2022-07-12 [?] CRAN (R 4.2.1)\n P yardstick    * 1.0.0   2022-06-06 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/Rtmpuo7gRW/renv-library-30a47499110a\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#summary",
    "href": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#summary",
    "title": "Conventional Material Models for Concrete Dataset",
    "section": "Summary",
    "text": "Summary\nThe NLS model using water:binder was a better fit to the experimental data than the NLS model using water:cement. The R2 for the water:binder model was 0.78 compared to an R2 of 0.65 for the water:cement model.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P dplyr       * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats     * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2     * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P purrr       * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P readr       * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl      * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble      * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidyr       * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse   * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpIHRddC/renv-library-22e4303bbb\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-09-02_ReticulateSetup/2022-09-02_ReticulateSetup.html",
    "href": "posts/2022-09-02_ReticulateSetup/2022-09-02_ReticulateSetup.html",
    "title": "Python Setup in RStudio using reticulate",
    "section": "",
    "text": "My experience setting up Python using the reticulate package and the RStudio IDE.\nThis post is a summary of my initial exploration to set up Python to operate within RStudio. I was mostly interested in being able to apply machine learning algorithms from Sci-kit Learn but through the RStudio IDE. The possibility of switching between R and Python languages within an analysis was also intriguing to me. I was somewhat surprised that there doesn’t seem to be a consensus on how to set up Python with RStudio. I found many different recommendations as far as how to install Python and how to configure RStudio. I believe this lack of consensus is due to the feature being rather new and also there are many possible configurations depending on your usage and preferences.\nI started with a fresh installation of Python and I chose to use the Miniconda installer for Windows 64-bit. No problems here except the default installation directory contained my Windows user name which contained a space. The installer gave a warning and I instead chose to install in folder in root directory called miniconda. I then updated to the latest version of RStudio (2022.02.2) and installation of the reticulate package. The reticulate package is essential for using Python in the RStudio environment.\n\nLoad libraries\n\nlibrary(tidyverse)\nlibrary(reticulate)\n\nFrom here, I tried the setup recommended by Tiffany Timbers on her Github and further discussed in an R Ladies Baltimore video. This setup involved setting a system environment variable in the .Rprofile to specify which Python installation to use: Sys.setenv(RETICULATE_PYTHON = \"path_to_miniconda's_python\"). While this approach does in fact work, this system setting locks in the Python installation to use and you need to modify the .Rprofile in order to use a different conda python environment. The setup also mentioned to make configuration changes to Git Bash and RStudio terminal settings that I found were not necessary. The setup instructions were created in December 2020 so, perhaps, subsequent RStudio versions have made these terminal configurations obsolete.\nNext, I found the reticulate installation recommended by Matt Dancho on the Business Science website. This setup recommended setting up a conda environment using the following command: conda create -n py3.8 python=3.8 scikit-learn pandas numpy matplotlib. This command creates a new environment “py3.8”, installs Python 3.8 and installs the latest versions of scikit-learn, pandas, numpy and matplotlib.\n\n\nList the conda environments in RMarkdown\n\nconda_list()\n\n      name                                   python\n1     base                 C:\\\\miniconda/python.exe\n2 my-rdkit C:\\\\miniconda\\\\envs\\\\my-rdkit/python.exe\n3    py3.8    C:\\\\miniconda\\\\envs\\\\py3.8/python.exe\n\n\nYou can then set your conda environment using reticulate::use_condaenv.\n\nuse_condaenv(\"py3.8\", required = TRUE)\n\nThe conda environment used by reticulate can then be checked.\n\npy_config()\n\npython:         C:/miniconda/envs/py3.8/python.exe\nlibpython:      C:/miniconda/envs/py3.8/python38.dll\npythonhome:     C:/miniconda/envs/py3.8\nversion:        3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 05:59:45) [MSC v.1929 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          C:/miniconda/envs/py3.8/Lib/site-packages/numpy\nnumpy_version:  1.22.4\n\nNOTE: Python version was forced by RETICULATE_PYTHON\n\n\n\n\nTest 1: Is Python Working?\n\n1 + 1\n\n2\n\n\nNote that here we are using a Python code block in Rmarkdown.\n\n\nTest 2: Numpy and Pandas\n\nimport numpy as np\nimport pandas as pd\n\nUse numpy to create a sequence of numbers in an array\n\nnp.arange(1,10)\n\narray([1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\nUse pandas to create a dataframe\n\n# Make a sequence in a data frame using dict format\ndf = pd.DataFrame(data = {\"sequence\":np.arange(1,20,.01)})\n\n# Use assign (mutate) equivalent to calculate the np.sin() of the series\ndf = df.assign(value=np.sin(df[\"sequence\"]))\n\ndf\n\n      sequence     value\n0         1.00  0.841471\n1         1.01  0.846832\n2         1.02  0.852108\n3         1.03  0.857299\n4         1.04  0.862404\n...        ...       ...\n1895     19.95  0.891409\n1896     19.96  0.895896\n1897     19.97  0.900294\n1898     19.98  0.904602\n1899     19.99  0.908819\n\n[1900 rows x 2 columns]\n\n\n\n\nTest #3: Generate a plot using Matplotlib\n\nimport matplotlib as plt\n\ndf.plot(x=\"sequence\", y = \"value\", title = \"Matplotlib\")\n\n\n\n\n\n\nTest #4: Build a model using Sci-kit Learn\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state=0)\n\nX = [[ 1,  2,  3],  # 2 samples, 3 features\n     [11, 12, 13]]\n\ny = [0, 1]  # classes of each sample\n\nclf.fit(X, y)\n\nRandomForestClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=0)\n\n\n\nclf.predict(X)  # predict classes of the training data\n\narray([0, 1])\n\n\n\n\nTip from Business Science post - 4 Conda Terminal Commands\nAt some point you will need to create, modify, add more packages to your Conda Environment(s). Here are 4 useful commands:\n\nRun conda env list to list the available conda environments\nRun conda activate <env_name> to activate a conda environment\nRun conda update --all to update all python packages in a conda environment.\nRun conda install <package_name> to install a new package\n\n\n\nSummary\nI found the approach recommended in the post by Matt Dancho was more straightforward and I haven’t found any downside yet. I like the flexibilty to change the conda environment for each analysis (Rmarkdown file) rather than adjusting the .Rprofile setting and rebooting R each time for the change to take effect. I will continue to update this post as I learn more tips and tricks for mixing R and Python code in Rmarkdown using the RStudio IDE.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P dplyr       * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats     * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2     * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P purrr       * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P readr       * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P reticulate  * 1.26    2022-08-31 [?] CRAN (R 4.2.1)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble      * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidyr       * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse   * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/Rtmpm0ubzp/renv-library-3b9c661d2710\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n─ Python configuration ───────────────────────────────────────────────────────\n python:         C:/miniconda/envs/py3.8/python.exe\n libpython:      C:/miniconda/envs/py3.8/python38.dll\n pythonhome:     C:/miniconda/envs/py3.8\n version:        3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 05:59:45) [MSC v.1929 64 bit (AMD64)]\n Architecture:   64bit\n numpy:          C:/miniconda/envs/py3.8/Lib/site-packages/numpy\n numpy_version:  1.22.4\n \n NOTE: Python version was forced by RETICULATE_PYTHON\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-10-13_ConcreteGP/2022-10-13_ConcreteGP.html",
    "href": "posts/2022-10-13_ConcreteGP/2022-10-13_ConcreteGP.html",
    "title": "Gaussian Process Model for the Concrete Dataset",
    "section": "",
    "text": "A GP model to predict the compressive strength of concrete is built using R and Python.\nThis post shares my first analysis of the Concrete dataset using a Gaussian Process modeling approach. I was interested in Gaussian Process models due to the possibility of building a non-linear regression model which fits the dataset well and allows for predictions on new data along with the uncertainty in that prediction. I have previously analyzed this dataset using a variety of machine learning approaches which allows for a good comparison in prediction performance.\nThe analysis combines R and Python as I wanted to reuse some of the data cleaning from the previous analyses written in R while the Gaussian Process model was built using Python. The most relevant articles I could find on Gaussian Process modeling contained examples in Python so I decided to use a similar approach. In this post, I am using the GaussianProcessRegressor model in the Sci-Kit Learn package to build the model. An RStudio blog was written in 2019 in R using tfprobability package on the same dataset but, honestly, I found it difficult to follow and the modeling results (MSE) was higher than my sklearn model."
  },
  {
    "objectID": "posts/2022-10-13_ConcreteGP/2022-10-13_ConcreteGP.html#evaluating-the-gp-model-predictions-for-the-test-data",
    "href": "posts/2022-10-13_ConcreteGP/2022-10-13_ConcreteGP.html#evaluating-the-gp-model-predictions-for-the-test-data",
    "title": "Gaussian Process Model for the Concrete Dataset",
    "section": "Evaluating the GP model predictions for the test data",
    "text": "Evaluating the GP model predictions for the test data\nYou need to scale the test data before prediction using the same scaling used on the training dataset.\n\nX_test_scale = scaler.transform(X_test)\ny_pred_te_scale, y_pred_te_std_scale = gp_model.predict(X_test_scale, return_std=True)\ny_pred_te = target_scaler.inverse_transform(y_pred_te_scale.reshape(-1,1))\ny_pred_te_std = y_pred_te_std_scale * target_scaler.scale_\n\ntpred_gp = metrics.r2_score(y_test, y_pred_te)\n\ntest_metrics = [[\"Rsq\", metrics.r2_score(y_test, y_pred_te)], [\"Adjusted RSq\", 1 - (1-metrics.r2_score(y_test, y_pred_te))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)], [\"MAE\", metrics.mean_absolute_error(y_test, y_pred_te)], [\"MSE\", metrics.mean_squared_error(y_test, y_pred_te)], [\"RMSE\", np.sqrt(metrics.mean_squared_error(y_test, y_pred_te))]]\n\ntest_metrics_df = pd.DataFrame(test_metrics, columns = [\"metric\", \"value\"])\nprint(test_metrics_df)\n\n         metric      value\n0           Rsq   0.891769\n1  Adjusted RSq   0.889621\n2           MAE   4.019690\n3           MSE  29.627775\n4          RMSE   5.443140\n\n\nThe model performance was a bit worse for the testing data as compared to the training data. I believe one reason is that cross-validation was not used and the model is overfitting the training data. One of the advantages of the Gaussian Process model is the estimation of uncertainty in the prediction. In the figure below, the predicted vs. measured compressive strengths for the test dataset are displayed along with error bars for +/- 1 standard deviation.\n\npred_test <- tibble(y_test = py$y_test, y_pred_te = as.vector(py$y_pred_te), y_pred_te_std = py$y_pred_te_std)\n\nggplot(data = pred_test, aes(x = y_test, y = y_pred_te)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = y_pred_te - y_pred_te_std, ymax = y_pred_te + y_pred_te_std)) +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Gaussian Process Model: Predicted vs. Measured for Testing Data\",\n       x = \"Actual Compressive Strength (MPa)\",\n       y = \"Predicted Compressive Strength (MPa)\") +\n  theme_light()\n\n`geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "posts/2022-10-13_ConcreteGP/2022-10-13_ConcreteGP.html#summary",
    "href": "posts/2022-10-13_ConcreteGP/2022-10-13_ConcreteGP.html#summary",
    "title": "Gaussian Process Model for the Concrete Dataset",
    "section": "Summary",
    "text": "Summary\nA Gaussian process model has been built for the concrete dataset. The predictive performance of this model was lower than for random forest and xgboost models (GP R2 = 0.89 vs. RF R2 = 0.94). The main advantage of the Gaussian Process model is the calculation of prediction error which can be very helpful in assessing confidence in future predictions.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P dplyr       * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats     * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2     * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P purrr       * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P readr       * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl      * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P reticulate  * 1.26    2022-08-31 [?] CRAN (R 4.2.1)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble      * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidyr       * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse   * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpQ5SNYG/renv-library-35903dd013ae\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n─ Python configuration ───────────────────────────────────────────────────────\n python:         C:/miniconda/envs/py3.8/python.exe\n libpython:      C:/miniconda/envs/py3.8/python38.dll\n pythonhome:     C:/miniconda/envs/py3.8\n version:        3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 05:59:45) [MSC v.1929 64 bit (AMD64)]\n Architecture:   64bit\n numpy:          C:/miniconda/envs/py3.8/Lib/site-packages/numpy\n numpy_version:  1.22.4\n \n NOTE: Python version was forced by use_python function\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html",
    "href": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html",
    "title": "Pin a Vetiver Model to an AWS S3 Container",
    "section": "",
    "text": "An XGBoost model for predicting concrete strength is transformed into a deployable model object and uploaded to an AWS S3 container.\nIn this post, I will take the XGBoost model for predicting concrete compressive strength described in a previous post, convert the model into a deployable model object using vetiverand “pin” it to an S3 bucket. The purpose of this effort is to make the model accessible in the cloud to an API running in a different location. The development of the API will be discussed in the next post. S3 stands for the AWS Simple Storage Service which exists in the cloud. I chose AWS over other vetiver-compatible options simply because I already had an existing account."
  },
  {
    "objectID": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html#build-the-model-again",
    "href": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html#build-the-model-again",
    "title": "Pin a Vetiver Model to an AWS S3 Container",
    "section": "Build the model (again)",
    "text": "Build the model (again)\nThis section just performs the steps to build the XGBoost model described in detail in the previous post.\nExpand to see the code\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n#Tidymodels\nlibrary(tidymodels)\nlibrary(xgboost)\n\n#MLOps\nlibrary(vetiver)\nlibrary(pins)\n\n#Load the dataset\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"../data/\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))\n\n#Split the data into training and testing datasets\nset.seed(123)\nconcrete_split <- initial_split(concrete_tbl, prop = 0.80)\nconcrete_train <- training(concrete_split)\nconcrete_test <- testing(concrete_split)\n\n#Create the model recipe\nconcrete_rec <- recipe(compressive_strength ~ ., data = concrete_train) %>%\n  step_normalize(all_predictors())\n\n#Create the model specification. Parameters were specified from tuning in a previous post.\nxgboost_spec = boost_tree(\n  trees = 1000,\n  min_n = 18,\n  tree_depth = 10,\n  learn_rate = 0.02647525\n) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n\n#Create the modeling workflow\nconcrete_wf <- workflow() %>%\n  add_recipe(concrete_rec) %>%\n  add_model(xgboost_spec)\n\n#Fit model on train and evaluate on test.\nfinal_res <- last_fit(concrete_wf, concrete_split, metrics = metric_set(rmse, rsq, mae))\n\n#Assess final model performance metrics\ncollect_metrics(final_res)\n\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       4.33  Preprocessor1_Model1\n2 rsq     standard       0.945 Preprocessor1_Model1\n3 mae     standard       2.69  Preprocessor1_Model1"
  },
  {
    "objectID": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html#create-the-deployable-model-object",
    "href": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html#create-the-deployable-model-object",
    "title": "Pin a Vetiver Model to an AWS S3 Container",
    "section": "Create the Deployable Model Object",
    "text": "Create the Deployable Model Object\nThe deployable model object is created using the vetiver package. It is really as simple as extracting the workflow and passing it to the vetiver_model function.\n\nv <- final_res %>%\n  extract_workflow() %>%\n  vetiver_model(model_name = \"concrete-xgb\")\n\nv\n\n\n── concrete-xgb ─ <bundled_workflow> model for deployment \nA xgboost regression modeling workflow using 8 features"
  },
  {
    "objectID": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html#pins-and-aws-s3",
    "href": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html#pins-and-aws-s3",
    "title": "Pin a Vetiver Model to an AWS S3 Container",
    "section": "Pins and AWS s3",
    "text": "Pins and AWS s3\nThe pins package allows you to save data, models or R objects to the cloud such as an AWS S3 container. A new S3 container can be set up within AWS. In my case, I just used the default settings for the S3 bucket with the name pins-test-zoller. A security id and access key need to be set up to enable saving of data from your local computer to the S3 container. In your AWS account options under Security Credentials, you can configure your security id and access key and save the file to your local computer. There are multiple options to tell R where to find this information but I preferred to create a shared AWS credentials file in a text editor as follows:\n[default]\naws_access_key_id=your AWS access key\naws_secret_access_key=your AWS secret key\nOn a Windows computer, the file needs to be saved with the name credentials without any extension. The file location needs to be C:\\Users\\[your username]\\.aws\\. You may need to create the .aws directory.\nYou can then connect to the board where you want to place the pin using board_s3 command. Here, we pin the vetiver model for the concrete data.\n\nboard <- board_s3(\"pins-test-zoller\", region = \"us-east-2\")\nboard %>% vetiver_pin_write(v)\n\nCreating new version '20221019T191525Z-b1278'\nWriting to pin 'concrete-xgb'\n\nCreate a Model Card for your published model\n• Model Cards provide a framework for transparent, responsible reporting\n• Use the vetiver `.Rmd` template as a place to start\n\n\nIn the AWS S3 bucket with the name “pins-test-zoller”, a new folder is created with the same name as the model, concrete-xgb. Within this folder, there is a subfolder with the named according to the model version number and, within the subfolder, is the model object in rds form (concrete-xgb.rds) and a data.txt file with summary information about the model object."
  },
  {
    "objectID": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html#summary",
    "href": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html#summary",
    "title": "Pin a Vetiver Model to an AWS S3 Container",
    "section": "Summary",
    "text": "Summary\nAn XGBoost model for the concrete dataset has been converted to a deployable model object using the vetiver package and then uploaded (i.e. pinned) to an AWS S3 bucket. The model object can now be accessed in the cloud for different purposes including creating an API to provide model predictions. The API use case will be discussed further in the next post.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-10-19\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto    @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version date (UTC) lib source\n P broom        * 1.0.1   2022-08-29 [?] CRAN (R 4.2.1)\n P dials        * 1.0.0   2022-06-14 [?] CRAN (R 4.2.1)\n P dplyr        * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats      * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2      * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P infer        * 1.0.3   2022-08-22 [?] CRAN (R 4.2.1)\n   modeldata    * 1.0.1   2022-09-06 [2] CRAN (R 4.2.1)\n   parsnip      * 1.0.2   2022-10-01 [2] CRAN (R 4.2.1)\n P pins         * 1.0.3   2022-09-24 [?] CRAN (R 4.2.1)\n   purrr        * 0.3.5   2022-10-06 [2] CRAN (R 4.2.1)\n   readr        * 2.1.3   2022-10-01 [2] CRAN (R 4.2.1)\n P readxl       * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n   recipes      * 1.0.2   2022-10-16 [2] CRAN (R 4.2.0)\n   rsample      * 1.1.0   2022-08-08 [2] CRAN (R 4.2.1)\n P scales       * 1.2.1   2022-08-20 [?] CRAN (R 4.2.1)\n P sessioninfo  * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr      * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble       * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidymodels   * 1.0.0   2022-07-13 [?] CRAN (R 4.2.1)\n   tidyr        * 1.2.1   2022-09-08 [2] CRAN (R 4.2.1)\n P tidyverse    * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n   tune         * 1.0.1   2022-10-09 [2] CRAN (R 4.2.1)\n P vetiver      * 0.1.8   2022-09-29 [?] CRAN (R 4.2.1)\n   workflows    * 1.1.0   2022-09-26 [2] CRAN (R 4.2.1)\n P workflowsets * 1.0.0   2022-07-12 [?] CRAN (R 4.2.1)\n P xgboost      * 1.6.0.1 2022-04-16 [?] CRAN (R 4.2.1)\n   yardstick    * 1.1.0   2022-09-07 [2] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpILaCM5/renv-library-e38b4b587d\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/Temp/RtmpILaCM5/renv-system-library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html",
    "href": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html",
    "title": "Create a Dockerized API Running on an AWS EC2 instance",
    "section": "",
    "text": "An Application Programming Interface (API) to predict concrete compressive strength is implemented in the cloud using an AWS EC2 instance.\nIn this post, we create an API for the deployable model object pinned to an S3 bucket as described in the previous post. We start by creating an Elastic Compute (EC2) instance on AWS to run the API. The vetiver package is used to write a Dockerfile for running the API inside a Docker container on the EC2 instance. The R script to run a plumber API for the model object is also created using the vetiver package. The EC2 instance is set up with Docker and the container is created from the Dockerfile. The API can then be run to provide compressive strength predictions for different concrete formulations."
  },
  {
    "objectID": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#create-an-ec2-instance-to-run-a-docker-container-with-a-plumber-api",
    "href": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#create-an-ec2-instance-to-run-a-docker-container-with-a-plumber-api",
    "title": "Create a Dockerized API Running on an AWS EC2 instance",
    "section": "Create an EC2 instance to run a Docker container with a Plumber API",
    "text": "Create an EC2 instance to run a Docker container with a Plumber API\nI chose to use the Elastic Compute (EC2) service from AWS to run the API in a docker container. The basics of setting up an EC2 instance are captured well in the The Shiny AWS book. I mostly accepted the defaults offered by AWS as described below:\n\nEnter a name for the instance such as docker-api-test.\nSelect Amazon Machine Image (AMI) for which I chose the free tier eligible, Amazon Linux (Amazon Linux 2 Kernel 5.10 AMI 2.0.20220912.1 x86_64 HVM gp2).\nSelect the Instance Type. Here is chose free tier eligible, t2.micro (1 CPU, 1 GB memory).\nCreate a key-pair for security access (unless you already have one you would like to re-use). This generates a .pem file to save.\nLeave the Network Settings as default for now. These settings will need to be changed later.\nFor Storage, I increased the amount to the free tier limit of 30 GB.\nNo changes to Detailed Settings.\nLaunch the instance."
  },
  {
    "objectID": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#create-a-security-group-to-allow-access-to-the-api",
    "href": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#create-a-security-group-to-allow-access-to-the-api",
    "title": "Create a Dockerized API Running on an AWS EC2 instance",
    "section": "Create a Security Group to Allow Access to the API",
    "text": "Create a Security Group to Allow Access to the API\nThe inbound and outbound rules need to be adjusted for the API to work properly. I found an article by Martin Lukac on “Deploying a plumber API to AWS EC2 instance” which contained the rules which also worked for me.\nInbound rules:\n1. Type: SSH, Protocol: TCP, Port: 22, Source: 0.0.0.0/0 (by default this was added)\n2. Type: HTTP, Protocol: TCP, Port: 80, Source: 0.0.0.0/0\n3. Type: Custom TCP, Protocol: TCP, Port: 8000, Source: 0.0.0.0/0 (for accessing the API)\n4. Type: Custom ICMP Rule IPv4, Protocol: Echo Request, Port: N/A, Source: 0.0.0.0/0 (for testing)\nOutbound rule:\n1. Type: All traffic, Protocol: All, Port Range: All, Destination: 0.0.0.0/0 (by default this was added)\nThe Security Group is then added to the proper EC2 instance."
  },
  {
    "objectID": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#connecting-to-your-ec2-instance-with-putty",
    "href": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#connecting-to-your-ec2-instance-with-putty",
    "title": "Create a Dockerized API Running on an AWS EC2 instance",
    "section": "Connecting to your EC2 instance with PuTTY",
    "text": "Connecting to your EC2 instance with PuTTY\nYou need to connect to your EC2 instance via an SSH client. A convenient option for Windows users is PuTTY an SSH and telnet client originally developed by Simon Tatham. The website for the Alaska Satellite Facility has a nice walkthrough on setting up PuTTY to access your EC2 instance.\n\nGenerate a PuTTY private key file (.ppk)\nIn brief, the steps to create the .ppk file from the AWS .pem file are as follows:\n\nStart the puttygen.exe program.\nClick on Load and find your .pem file you generated in Step 4 of Create an EC2 Instance above.\nMake sure Type of key to generate is set to RSA.\nClick on Save private key, name the file and save the ppk file.\n\n\n\nConfigure PuTTY to connect to your EC2 instance\n\nStart the putty.exe program\nEnter the Host name as ec2-user@your_public_DNS where your_public_DNS is listed in the description for your EC2 on the AWS console. It should be something like ec2-12-345-678-910.compute-1.amazonaws.com. Note: If you use an Ubuntu machine image the host name will begin with ubuntu@ instead of ec2-user@.\nMake sure the Port is set to 22.\nThe Connection type needs to be set to SSH.\nIn the Category pane on the left side of PuTTY configuration window, find the Connection category and expand the SSH options by clicking on the “+” and then click on Auth.\nUnder Private key for authentification, click on the Browse button and load the ppk file you generated above.\nIn the Category pane, click on Session and in the box under Saved Sessions, enter a name for this connection and then click on Save.\n\n\n\nConnect to your EC2\n\nStart the “putty.exe” program.\nClick on the Saved Sessions you named in Step 7 above.\nClick on Open to bring up a terminal session connected to your EC2."
  },
  {
    "objectID": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#load-and-start-docker-on-ec2",
    "href": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#load-and-start-docker-on-ec2",
    "title": "Create a Dockerized API Running on an AWS EC2 instance",
    "section": "Load and Start Docker on EC2",
    "text": "Load and Start Docker on EC2\nThe AWS EC2 instance does not come pre-loaded with Docker so it needs to be installed. A nice walkthrough can be found on the [Workfall Blog] (https://www.workfall.com/learning/blog/how-to-install-and-run-docker-containers-on-amazon-ec2-instance/). You first need to connect to your EC2 instance with PuTTY as described above. The next steps are as follows:\n\nUpdate the installed packages and package cache by running the command:sudo yum update -y\nInstall the most recent Docker package by running the command: sudo amazon-linux-extras install docker\nStart the Docker service: sudo service docker start\nAdd the ec2-user to the Docker group so you don’t need to execute Docker commands with sudo: sudo usermod -a -G Docker ec2-user\nLog out and log back into the EC2 instance and run a command to verify: docker info"
  },
  {
    "objectID": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#write-the-dockerfile-and-plumber-api-script",
    "href": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#write-the-dockerfile-and-plumber-api-script",
    "title": "Create a Dockerized API Running on an AWS EC2 instance",
    "section": "Write the Dockerfile and Plumber API script",
    "text": "Write the Dockerfile and Plumber API script\n\nlibrary(vetiver)\nlibrary(pins)\n\nboard <- board_s3(\"pins-test-zoller\", region = \"us-east-2\")\nv <- vetiver_pin_read(board, name = \"concrete-xgb\", version = \"20221019T191525Z-b1278\")\n\nvetiver_write_plumber(board, name = \"concrete-xgb\", version = \"20221019T191525Z-b1278\", rsconnect = FALSE)\nvetiver_write_docker(v)\n\nThe contents of the Dockerfile\n# Generated by the vetiver package; edit with care\n\nFROM rocker/r-ver:4.2.0\nENV RENV_CONFIG_REPOS_OVERRIDE https://packagemanager.rstudio.com/cran/latest\n\nRUN apt-get update -qq && apt-get install -y --no-install-recommends \\\n  libcurl4-openssl-dev \\\n  libicu-dev \\\n  libsodium-dev \\\n  libssl-dev \\\n  make \\\n  zlib1g-dev \\\n  && apt-get clean\n\nCOPY vetiver_renv.lock renv.lock\nRUN Rscript -e \"install.packages('renv')\"\nRUN Rscript -e \"renv::restore()\"\nCOPY plumber.R /opt/ml/plumber.R\nEXPOSE 8000\nENTRYPOINT [\"R\", \"-e\", \"pr <- plumber::plumb('/opt/ml/plumber.R'); pr$run(host = '0.0.0.0', port = 8000)\"]\nThe contents of the plumber.R file\n# Generated by the vetiver package; edit with care\n\nlibrary(pins)\nlibrary(plumber)\nlibrary(rapidoc)\nlibrary(vetiver)\nb <- board_s3(bucket = \"pins-test-zoller\", region = structure(\"us-east-2\", tags = list(type = \"scalar\")))\nv <- vetiver_pin_read(b, \"concrete-xgb\", version = \"20221019T191525Z-b1278\")\n\n#* @plumber\nfunction(pr) {\n    pr %>% vetiver_api(v)\n}"
  },
  {
    "objectID": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#editing-the-dockerfile-for-amazon-linux-ec2",
    "href": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#editing-the-dockerfile-for-amazon-linux-ec2",
    "title": "Create a Dockerized API Running on an AWS EC2 instance",
    "section": "Editing the Dockerfile for Amazon Linux EC2",
    "text": "Editing the Dockerfile for Amazon Linux EC2\nI found that a couple of edits were necessary to successfully create the Docker container. The libxml2 library needs to be installed in the container which can be added into the RUN apt-get update command line. The paws.storage R library needs to be installed by adding the command RUN Rscript -e \"install.packages('paws.storage')\". This issue has been reported on the vetiver Github as it is not recognized as a dependency when creating the renv lockfile.\nThe edited version should be:\n# Generated by the vetiver package; edit with care\n\nFROM rocker/r-ver:4.2.0\nENV RENV_CONFIG_REPOS_OVERRIDE https://packagemanager.rstudio.com/cran/latest\n\nRUN apt-get update -qq && apt-get install -y --no-install-recommends \\\n  libcurl4-openssl-dev \\\n  libicu-dev \\\n  libsodium-dev \\\n  libssl-dev \\\n  libxml2 \\\n  make \\\n  zlib1g-dev \\\n  && apt-get clean\n\nCOPY vetiver_renv.lock renv.lock\nRUN Rscript -e \"install.packages('renv')\"\nRUN Rscript -e \"renv::restore()\"\nRUN Rscript -e \"install.packages('paws.storage')\"\nCOPY plumber.R /opt/ml/plumber.R\nEXPOSE 8000\nENTRYPOINT [\"R\", \"-e\", \"pr <- plumber::plumb('/opt/ml/plumber.R'); pr$run(host = '0.0.0.0', port = 8000)\"]"
  },
  {
    "objectID": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#copying-files-to-your-ec2-instance",
    "href": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#copying-files-to-your-ec2-instance",
    "title": "Create a Dockerized API Running on an AWS EC2 instance",
    "section": "Copying files to your EC2 instance",
    "text": "Copying files to your EC2 instance\nThe article at this link describes two different methods for copying files from Windows to an EC2 instance using WinSCP and using PuTTY secure copy. I used PuTTY secure copy (PSCP) since PuTTY is already installed on my computer and there are not many files to copy. PSCP is a command line utility\npscp -i your-key.ppk yourfilename ec2-user@yourPublicDNS:/home/ec2-user/\nwhere your-key.ppk is your PuTTY private key file previously generated to connect to your EC2 instance. The ppk file needs to be in the same directory with the files you are copying to the EC2 instance.\nYou need to copy the Dockerfile, vetiver_renv.lock and plumber.R files to your EC2 instance in the ec2-user directory."
  },
  {
    "objectID": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#build-and-run-the-docker-api",
    "href": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#build-and-run-the-docker-api",
    "title": "Create a Dockerized API Running on an AWS EC2 instance",
    "section": "Build and run the Docker API",
    "text": "Build and run the Docker API\n\nLog in to (SSH) the EC2 instance and start the Docker service: sudo service docker start\nBuild the docker container from the Dockerfile: docker build -t concrete-xgb-api .\nStart the API: docker run --rm -p 8000:8000 concrete-xgb-api\nThe API can then be tested by connecting to the rapiddoc Docs on your instance via a web browser:\n\nhttp://yourEC2PublicDNS:8000/__docs__/\nIf you successfully connect to the API, you will see a page such as shown below.\n\n\n\nYour rapiddoc API home page\n\n\nYou can click on GET and TRY to ping the status of your API. You can test the predictive model by clicking on POST and then under REQUEST click on the Example tab. You can then enter different values for the predictors in the format shown in the Example and then click on TRY and the outcome will show below in the Response window. An example for the Concrete API is shown in the figure below.\n\n\n\nExample of Concrete Strength Prediction from the rapiddoc page"
  },
  {
    "objectID": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#summary",
    "href": "posts/2022-10-21_ConcreteAPI/2022-10-21_ConcreteAPI.html#summary",
    "title": "Create a Dockerized API Running on an AWS EC2 instance",
    "section": "Summary",
    "text": "Summary\nIn this post, I have shown how to launch an API inside a Docker Container running on an EC2 instance. The Dockerfile and plumber API script was created using the vetiver package. The API accesses an xgboost model to predict concrete strength from ingredients and age that was versioned and posted to an AWS S3 container as described in the previous post. I am curious about building a Shiny app to utilize this API but this will need to wait for a future post.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-11-04\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P pins        * 1.0.3   2022-09-24 [?] CRAN (R 4.2.1)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P vetiver     * 0.1.8   2022-09-29 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpWoGT29/renv-library-23ec6a0a5974\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/Temp/RtmpWoGT29/renv-system-library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-12-02_SimpleUIConcreteAPI/2022-12-02_SimpleUIConcreteAPI.html",
    "href": "posts/2022-12-02_SimpleUIConcreteAPI/2022-12-02_SimpleUIConcreteAPI.html",
    "title": "A Rudimentary Shiny App for the Concrete API",
    "section": "",
    "text": "A simple, user interface for requesting predictions from the Concrete API has been built using Shiny.\nA simple UI has been built which allows the user to adjust the amounts of the concrete ingredients and age. The user can then send those selections to the Concrete API describes in a previous post running on an Amazon EC2 instance. A prediction of the compressive strength for the concrete formulation and age is then displayed in the Shiny app. The UI also has a button to ping the API and check if it is online. The app is currently deployed on shinyapps.io but the likelihood it is still active when you are reading this post is quite low as it is not an application I will be actively maintaining."
  },
  {
    "objectID": "posts/2022-12-02_SimpleUIConcreteAPI/2022-12-02_SimpleUIConcreteAPI.html#user-interface",
    "href": "posts/2022-12-02_SimpleUIConcreteAPI/2022-12-02_SimpleUIConcreteAPI.html#user-interface",
    "title": "A Rudimentary Shiny App for the Concrete API",
    "section": "User Interface",
    "text": "User Interface\nThe UI for the app is set up using the sidebarLayout with the 8 slider bars on the left for adjusting the concrete ingredient amounts and the age of the concrete. The mainPanel contains the 2 action buttons. The first button is used to ping the status of the API. The second button is to send the sliderInputs to the API and receive back the prediction of concrete strength. A text input is available in case the URL location of the API changes. The results print as text directly below the buttons. The aim was to just get a working Shiny app and, hopefully, the user interface will be improved in a subsequent post.\n\n\nCode\nui <- fluidPage(\n \n  titlePanel(\"Predictions Using the Concrete API\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"cement\", \"Cement (kg)\", min = 100, max = 550, value = 275),\n      sliderInput(\"blast_furnace_slag\", \"Blast Furnace Slag (kg)\", min = 0, max = 375, value = 20),\n      sliderInput(\"fly_ash\", \"Fly Ash (kg)\", min = 0, max = 200, value = 0),\n      sliderInput(\"water\", \"Water (kg)\", min = 100, max = 250, value = 185),\n      sliderInput(\"superplasticizer\", \"Superplasticizer (kg)\", min = 0, max = 35, value = 5),\n      sliderInput(\"coarse_aggregate\", \"Coarse Aggregate (kg)\", min = 800, max = 1150, value = 975),\n      sliderInput(\"fine_aggregate\", \"Fine Aggregate (kg)\", min = 575, max = 1000, value = 775),\n      sliderInput(\"age\", \"Age (days)\", min = 1, max = 365, value = 28),\n    ),\n    mainPanel(\n      textInput(\"api_url\", \"API URL\", \"http://ec2-XX-YYY-ZZZ-AAA.compute-1.amazonaws.com:8000/\"),\n      actionButton(\"go\", \"Ping\"),\n      textOutput(\"status\"),\n      actionButton(\"predict\", \"Predict\"),\n      h1(textOutput(\"strength\"))\n    )\n  )\n)"
  },
  {
    "objectID": "posts/2022-12-02_SimpleUIConcreteAPI/2022-12-02_SimpleUIConcreteAPI.html#server-side",
    "href": "posts/2022-12-02_SimpleUIConcreteAPI/2022-12-02_SimpleUIConcreteAPI.html#server-side",
    "title": "A Rudimentary Shiny App for the Concrete API",
    "section": "Server side",
    "text": "Server side\nThe app uses the httr package to connect to the Concrete API. The httr GET command is used to obtain the API status. The httr status_code function is used to parse the status code from the GET response. The httr POST command is used to send the predictor inputs to the API and receive the output response. The string format for the sending the predictor inputs is shown in the rapidDoc docs by looking the at the curl example. This format is used in the body of the POST command also incorporating the values for the 8 sliderInputs. The prediction for concrete strength is parsed from the content of the POST response. Both the GET and POST commands are contained within an eventReactive tied to their respective action buttons.\n\n\nCode\nserver <- function(input, output) {\n  \n  resp <- eventReactive(input$go, {\n    GET(paste0(input$api_url, \"ping\"))\n  })\n  \n  post_resp <- eventReactive(input$predict, {\n    POST(paste0(input$api_url, \"predict\"), body = paste0('[{\"cement\":',input$cement,\n                                                         ',\"blast_furnace_slag\":', input$blast_furnace_slag,\n                                                         ',\"fly_ash\":', input$fly_ash, \n                                                         ',\"water\":', input$water,\n                                                         ',\"superplasticizer\":', input$superplasticizer,\n                                                         ',\"coarse_aggregate\":', input$coarse_aggregate,\n                                                         ',\"fine_aggregate\":', input$fine_aggregate,\n                                                         ',\"age\":', input$age,'}]'))\n  })\n  output$status <- renderText({\n    paste0(\"Status Code:\", status_code(resp()))\n  })\n  output$strength <- renderText({\n    paste0(content(post_resp(), as = \"parsed\")[[1]][[1]], \" MPa\")\n  })\n}\n\n\nPredictions for different concrete formulations and ages can be accomplished by adjusting the slider inputs and then pressing the “Predict” action button. Some potential improvements to the functioning of this app could include:\n\nClearing the current prediction when the Predict action button is pressed but before the new prediction is received from the API\nError checking to handle cases when the response is not received or received without a prediction\nAdjustment of the sliders to a specific total weight of concrete\nRecording the recent formulations and prediction results in a table for the user"
  },
  {
    "objectID": "posts/2022-12-02_SimpleUIConcreteAPI/2022-12-02_SimpleUIConcreteAPI.html#summary",
    "href": "posts/2022-12-02_SimpleUIConcreteAPI/2022-12-02_SimpleUIConcreteAPI.html#summary",
    "title": "A Rudimentary Shiny App for the Concrete API",
    "section": "Summary",
    "text": "Summary\nA simple app has been built for a user to interface with the Concrete API. The slider bars make it relatively easy to explore the effect of different ingredients and age on the concrete strength compared to manually editing each value in an R script. Hopefully, I will have time to further develop the app with a nicer, user interface and improved functionality in a future post.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-12-06\n pandoc   2.19.2 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.1.189 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpsraoOb/renv-library-102069bf25f5\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/Temp/RtmpsraoOb/renv-system-library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2021-04-27_tt_CEODepartures/2021-04-27_tt_CEODepartures.html",
    "href": "posts/2021-04-27_tt_CEODepartures/2021-04-27_tt_CEODepartures.html",
    "title": "Tidy Tuesday: CEO Departures",
    "section": "",
    "text": "My data visualization based on a dataset of CEO departures from 1500 S&P firms from 2000-2018.\nA quick analysis of the weekly #TidyTuesday dataset organized by the R4DS Online Learning Community. My approach is to apply my data science skills to explore one question I have about the data and generate a visualization that addresses this question. The main purpose for me is to practice and try out new things. I am never completely satisfied with the end result but I do the best I can in a short period of time.\nWhat I learned this week about R and the Tidyverse\nBrief explanation of the dataset\nThis dataset is from DataIsPlural and contains reasons for CEO departures from S&P 1500 firms. Information is provided about the company name, CEO name and date of departure along with notes and links to articles which were used to assign the departure reason. Reasons were assigned to one of 9 different codes. Codes 1-4 were involuntary reasons such as CEO died or was ill and CEO was terminated for poor job performance or legal issues. Codes 5-6 were voluntary reasons such as the CEO retired or decided to leave for a new opportunity. Code 7 is mostly about a change following a merger or acquisition and in some cases the CEO stayed on with the new company. Code 8-9 are missing or data collection error."
  },
  {
    "objectID": "posts/2021-04-27_tt_CEODepartures/2021-04-27_tt_CEODepartures.html#load-libraries-and-data",
    "href": "posts/2021-04-27_tt_CEODepartures/2021-04-27_tt_CEODepartures.html#load-libraries-and-data",
    "title": "Tidy Tuesday: CEO Departures",
    "section": "Load libraries and data",
    "text": "Load libraries and data\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(viridisLite)\n\ntheme_set(theme_minimal(base_family = \"mono\"))\n\n#Load dataset from TidyTuesday repository\ndepartures <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-27/departures.csv')"
  },
  {
    "objectID": "posts/2021-04-27_tt_CEODepartures/2021-04-27_tt_CEODepartures.html#wrangle",
    "href": "posts/2021-04-27_tt_CEODepartures/2021-04-27_tt_CEODepartures.html#wrangle",
    "title": "Tidy Tuesday: CEO Departures",
    "section": "Wrangle",
    "text": "Wrangle\nInitial conclusions from exploring the CEO departures dataset.\n\nCEO departures ranged from 1987 to 2020 in the dataset however not much data before 1995 and data after 2018 was mostly incomplete.\nTop reasons for departures were CEO retired, Other (mostly M&A) and dismissed due to job performance.\nNA in the departure code appears to be an entry where the CEO is still at the company upon last check.\n\nThe following plot shows the breakdown of departure codes vs. fiscal year for the entire dataset.\n\ndepartures %>%\n  mutate(departure_code = as.factor(departure_code)) %>%\n  group_by(fyear, departure_code) %>%\n  summarize(n = n(), .groups = \"drop\") %>%\n  ggplot(aes(x = fyear, y = n, fill = departure_code)) +\n    geom_area() +\n  labs(title = \"CEO Departures by Code\",\n       x = \"Fiscal Year\",\n       y = \"# of Departures\")\n\n\n\n\nAs can be seen from this chart, the data after 2018 contains a great deal of NAs which appear to be just missing data. Also the data before 1995 is pretty lean. Another observation is that just a few codes tend to dominate the chart making the less frequent codes difficult to assess. Based on these observations, I decided to recode the data to capture 5 main categories for CEO departures: 1) Health, 2) Performance, 3) CEO choice, 4) Merger and 5) N/A (unknown).\nThe code for cleaning and wrangling the dataset prior to plotting is given below.\n\ndepartures_clean <- departures %>%\n  filter(fyear < 2019 & fyear > 1994) %>%\n  filter(!is.na(departure_code)) %>%\n  mutate(departure_code = as.character(departure_code)) %>%\n  mutate(departure_code = fct_recode(departure_code, Health = \"1\",\n                                      Health = \"2\",\n                                      Performance = \"3\",\n                                      Performance = \"4\",\n                                      `CEO Choice` = \"5\",\n                                      `CEO Choice` = \"6\",\n                                      Merger = \"7\",\n                                      `N/A` = \"8\",\n                                      `N/A` = \"9\"\n                                      )) %>%\n  group_by(fyear, departure_code) %>%\n  summarize(n = n(), .groups = \"drop\")\n\ndepartures_clean\n\n# A tibble: 120 × 3\n   fyear departure_code     n\n   <dbl> <fct>          <int>\n 1  1995 Health             8\n 2  1995 Performance       34\n 3  1995 CEO Choice       129\n 4  1995 Merger            48\n 5  1995 N/A                2\n 6  1996 Health             4\n 7  1996 Performance       42\n 8  1996 CEO Choice       130\n 9  1996 Merger           139\n10  1996 N/A                2\n# … with 110 more rows\n\n\n\nVisualize\nThe visualization of the tidied and recoded dataset was then performed using a stacked area chart. This type of plot is standard in ggplot2 using geom_area.\n\np1 <- departures_clean %>%\n  ggplot(aes(x = fyear, y = n, fill = departure_code)) +\n  geom_area() +\n  scale_fill_viridis_d(option = \"plasma\", direction = -1) +\n  labs(title = \"Reasons for CEO Departures\",\n       subtitle = \"S&P 1500 Firms 1995-2018\",\n       x = \"Fiscal Year of Event\",\n       y = \"Number of Departures\",\n       fill = NULL,\n       caption = \"Graphic: @datadavidz | Source: DataIsPlural | #TidyTuesday\") +\n  theme(legend.position = \"top\",\n        axis.title.x = element_text(margin = margin(t = 5, r = 0, b = 0, l = 0)),\n        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0)))\n\n\n\n\n\n\n\nData Visualization for CEO Departures\n\n\nThe default margin for the axis title labels for theme_minimal is just too close to the axis labels for my liking. The best way to adjust the spacing of these labels is to use the margin function within the theme element_text for axis.title.x or axis.title.y. The ordering of the elements is a bit weird (top, right, bottom, left) but I guess it goes clockwise from the top. Probably best to assign the labels to avoid confusion. I was a bit lazy with the font choosing the default “mono” font but it seemed like a good fit for a business-related dataset.\n\n\nSummary\nThe stacked area chart for the CEO departure dataset was effective in communicating the main reasons CEOs left their company over the 1995-2018 timeframe. The viridis magma color palette produced a pleasing visualization. The number of departures varied from year-to-year however the ratio among the reasons was fairly consistent."
  },
  {
    "objectID": "posts/2021-04-13_tt_PostOffices/2021-04-13_tt_PostOffices.html",
    "href": "posts/2021-04-13_tt_PostOffices/2021-04-13_tt_PostOffices.html",
    "title": "Tidy Tuesday: U.S. Post Offices",
    "section": "",
    "text": "My data visualization based on a dataset of US Post Offices for 166,140 post offices that operated in the United States between 1639 and 2000.\nA quick analysis of the weekly #TidyTuesday dataset organized by the R4DS Online Learning Community. My approach is to apply my data science skills to explore one question I have about the data and generate a visualization that addresses this question. The main purpose for me is to practice and try out new things. I am never completely satisfied with the end result but I do the best I can in a short period of time.\nWhat I learned this week about R and the Tidyverse\n\nCreating a stacked area chart using geom_area function in ggplot2\nSlight adjustments in placing the axis title using the margin function in the plot theme\n\nBrief explanation of the dataset\nThis dataset is from DataIsPlural and contains reasons for CEO departures from S&P 1500 firms. Information is provided about the company name, CEO name and date of departure along with notes and links to articles which were used to assign the departure reason. Reasons were assigned to one of 9 different codes. Codes 1-4 were involuntary reasons such as CEO died or was ill and CEO was terminated for poor job performance or legal issues. Codes 5-6 were voluntary reasons such as the CEO retired or decided to leave for a new opportunity. Code 7 is mostly about a change following a merger or acquisition and in some cases the CEO stayed on with the new company. Code 8-9 are missing or data collection error.\n\nLoad libraries and data\n\nlibrary(tidyverse)\n\nlibrary(geojsonio)\nlibrary(rgdal)\nlibrary(broom)\nlibrary(rgeos)\n\nlibrary(viridis)\nlibrary(showtext)\n\nfont_add_google(name = \"Oswald\")\n\ntheme_set(theme_minimal())\n\n#Load dataset from TidyTuesday repository\npost_offices <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-13/post_offices.csv')\nus_reps <- read_csv(file = \"../data/us_reps_state.csv\") %>% janitor::clean_names()\n\n\n\nWrangle\nInitial conclusions from exploring the post_offices dataset.\n\nThe established date for post offices ranged from 1877-2000.\n\nThe discontinued date can be missing and would suggest the post office is still in operation as of 2000.\n\nThe state and county is available for almost all of the post offices.\nThe GNIS information including latitude and longitude is present for about 2/3 of the data.\n\nThe assumption is that the missing (NA) discontinued year means the post office is still operating. Based on this assumption, we calculate the number of post offices per state and as a ratio to the number of US House of Representatives for each state.\n\noffice_count <- post_offices %>%\n  select(id, name, state, established, discontinued) %>%\n  filter(is.na(discontinued)) %>%\n  group_by(state) %>%\n  summarize(offices = n(), .groups = \"drop\")\n\noffice_ratio <- us_reps %>%\n  left_join(tibble(state = state.name, id = state.abb), by = \"state\") %>%\n  left_join(office_count, by = c(\"id\" =  \"state\")) %>%\n  mutate(rep_ratio = offices / representatives_number,\n         pop_ratio = pop / offices) %>%\n  arrange(desc(pop_ratio))\n\n\n\nVisualize\nLet’s first look at the states with the most number of post offices.\n\noffice_count %>%\n  arrange(desc(offices)) %>%\n  slice(1:10) %>%\n  mutate(state = fct_reorder(state, offices)) %>%\n  ggplot(aes(x = offices, y = state)) +\n  geom_col(fill = \"steelblue\") +\n  labs(title = \"Top 10 States with the Highest Number of Post Offices\",\n       x = \"# of Post Offices\",\n       y = NULL)\n\n\n\n\nNow let’s look at the ratio of post offices per US representatives for each state.\n\noffice_ratio %>%\n  arrange(desc(rep_ratio)) %>%\n  slice(1:10) %>%\n  mutate(state = fct_reorder(state, rep_ratio)) %>%\n  ggplot(aes(x = rep_ratio, y = state)) +\n  geom_col(fill = \"midnightblue\") +\n  labs(title = \"Top 10 States with the Highest Number of Post Offices per US Rep\",\n       x = \"# of Post Offices\",\n       y = NULL)\n\n\n\n\nThe most post offices per state generally includes the most populous states while the most post offices per US representatives includes some of the less populous states. Kentucky and Iowa show up in both top ten lists. I decided from here that I was most interested in the average population served by the post offices in each state. I wanted to create a map but not using the US map but rather a hexbin representation I had come across previously.\nCreating the hexbin map was more complicated than I thought it would be when I began. The first step was to download a geojson file which I found here. This file contained the boundaries for the hexagons for each state. The file is read using the geojsonio package into a SpatialPolygonsDataFrame class.\n\n#geojsonio package\nspdf <- geojson_read(\"../data/us_states_hexgrid.geojson\",  what = \"sp\")\n\n#reformat the state name\nspdf@data = spdf@data %>%\n  mutate(google_name = gsub(\" \\\\(United States\\\\)\", \"\", google_name))\n\n# Show it (requires rgdal library)\nplot(spdf)\n\n\n\n\nWe can then reformat the spdf data into a standard data frame using the tidy function from the broom package. Next, we need to calculate the center of each hexagon for adding the label with the state abbreviation. The centroid is calculated from the spdf data using a function from the rgeos package.\n\n# reformat as data frame for ggplot\nspdf_fortified <- tidy(spdf, region = \"iso3166_2\")\n\n#calculate centroid of each hexagon for adding label\ncenters <- cbind.data.frame(data.frame(gCentroid(spdf, byid=TRUE), id=spdf@data$iso3166_2))\n\nggplot() +\n  geom_polygon(data = spdf_fortified, aes( x = long, y = lat, group = group), fill=\"skyblue\", color=\"white\") +\n  geom_text(data=centers, aes(x=x, y=y, label=id)) +\n  theme_void() +\n  coord_map()\n\n\n\n\nNext, we add the ratio of state population to number of post offices and segregate into bins.\n\nspdf_fortified <- spdf_fortified %>%\n  left_join(select(office_ratio, id, pop_ratio), by = \"id\") %>%\n  filter(!is.na(pop_ratio))\n\nspdf_fortified$bin <- cut( spdf_fortified$pop_ratio , breaks=c(seq(0, 20000, 5000), Inf), \n                           labels=c(\"< 5K\", \"5-10K\", \"10-15K\", \"15-20K\", \"20K+\") , include.lowest = TRUE )\n\nFinally, we create the hexbin map with the post office data.\n\nshowtext_auto()\nmy_palette <- rev(magma(8))[c(-1,-8)]\n\np1 <- ggplot() +\n  geom_polygon(data = spdf_fortified, aes(fill = bin, x = long, y = lat, group = group) , size=0, alpha=0.9) +\n  geom_text(data=centers, aes(x=x, y=y, label=id), color=\"white\", size=3, alpha=0.6) +\n  theme_void() +\n  scale_fill_manual( \n    values=my_palette, \n    name=\"Average population served per post office\", \n    guide = guide_legend( keyheight = unit(3, units = \"mm\"), keywidth=unit(12, units = \"mm\"), label.position = \"bottom\", title.position = 'top', nrow=1) \n  ) +\n  labs( title = \"Population Served per Post Office ca. 2000\",\n        caption = \"Graphic: @datadavidz | Source: Blevins and Helbock | #TidyTuesday\") +\n  theme(\n    legend.position = c(0.5, 0.9),\n    text = element_text(color = \"#22211d\"),\n    plot.background = element_rect(fill = \"#f5f5f2\", color = NA), \n    panel.background = element_rect(fill = \"#f5f5f2\", color = NA), \n    legend.background = element_rect(fill = \"#f5f5f2\", color = NA),\n    plot.title = element_text(family = \"Oswald\", size= 22, hjust=0.5, color = \"#4e4d47\", margin = margin(b = -0.1, t = 0.4, l = 2, unit = \"cm\")),\n    plot.caption = element_text(hjust = 0.95, vjust = 1)\n  )\n\n\n\n\n ### Summary\nI felt this hexbin map was an effective and aesthetically-pleasing graphic for the analysis. The construction of the hexbin map was more complex than I imagined requiring a website download, multiple packages I don’t normally use and new data formats. I have seen people make similar plots using the geofacet package and I am interested whether this could simplify the process. However, this package doesn’t appear to be able to easily make the hexagon shapes."
  },
  {
    "objectID": "posts/2023-01-05_aoc_Day01_Lists/2023-01-05_aoc_Day01_Lists.html",
    "href": "posts/2023-01-05_aoc_Day01_Lists/2023-01-05_aoc_Day01_Lists.html",
    "title": "Advent of Code Day 1: Working with Lists",
    "section": "",
    "text": "An input file is transformed into a structured list to enable calculations on the dataset."
  },
  {
    "objectID": "posts/2023-01-05_aoc_Day01_Lists/2023-01-05_aoc_Day01_Lists.html#introduction",
    "href": "posts/2023-01-05_aoc_Day01_Lists/2023-01-05_aoc_Day01_Lists.html#introduction",
    "title": "Advent of Code Day 1: Working with Lists",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 1. The scenario is that the elves are asked to record the calories for each consumable item they’ve brought with them. The list for each elf is separated by a blank line. So, the list looks something like this:\n1000\n2000\n3000\n\n4000\n\n5000\n6000\n\n7000\n8000\n9000\n\n10000\nThe first objective is to determine the most calories carried by a single elf. The second objective is to determine the total calories available from the top 3 elves carrying the most calories. There are a number of approaches to answer these questions, for example, by iterating through the data and storing just the highest (or 3 highest) totals in the process. Thinking about real world problems, I was interested in storing the totals for each elf so you could do additional analyses such as which elves are carrying the most calories, what is the distribution of calories amongst the elves and so on. To retain the structure of the original data set but make it more amenable for further analysis, my thought was to read the data into a list where each list item contained a sub-list of the calories for each elf."
  },
  {
    "objectID": "posts/2023-01-05_aoc_Day01_Lists/2023-01-05_aoc_Day01_Lists.html#loading-and-analyzing-input-file",
    "href": "posts/2023-01-05_aoc_Day01_Lists/2023-01-05_aoc_Day01_Lists.html#loading-and-analyzing-input-file",
    "title": "Advent of Code Day 1: Working with Lists",
    "section": "Loading and analyzing input file",
    "text": "Loading and analyzing input file\nThe input file has the structure as described in the introduction. The read_delim function from the readr package is used to load the file into a tibble (dataframe).\n\nlibrary(tidyverse)\n\nfilepath <- here::here(\"./posts/data/aoc/day01_input.txt\")\n\ndata <- read_delim(filepath, delim = \"\\n\", col_names = c(\"calories\"), skip_empty_rows = FALSE)\n\nNext, I determine the number of elves in the list by counting the number of NAs. I need to add 1 since the items for the last elf does not end with an NA.\n\nnum_elves <- data %>%\n  filter(is.na(calories)) %>%\n  count(calories) %>%\n  pull(n) %>%\n  `+`(1)\n\nnum_elves\n\n[1] 251\n\n\nThe data is now read into the list where each list item contains the vector of calories for that elf. I preallocate the list and then loop through the dataframe structure. When an NA is encountered, the list of calories for the current elf (elf_bag) is added to cal_lst at the position specified by elf_id, elf_id is iterated by 1 and elf_bag is reset to an empty vector.\n\nelf_id <- 1 #counter for the current elf\nelf_bag <- vector() #vector for the calories for each consumable item\n\n#Preallocate the list\ncal_lst <- vector(mode = \"list\", length = 251)\n\nfor (i in 1:length(data[[1]])) {\n  if (is.na(data[[i,1]])) {\n    cal_lst[elf_id] <- list(elf_bag)\n    names(cal_lst)[elf_id] <- paste0(\"elf_\", elf_id)\n    elf_id <- elf_id + 1\n    elf_bag <- vector()\n  } else {\n    elf_bag <- c(elf_bag, data[[i, 1]])\n  }\n}\n# Add the final list of items since no NA at the end\ncal_lst[elf_id] <- list(elf_bag)\nnames(cal_lst)[elf_id] <- paste0(\"elf_\", elf_id)\n\n#Show the first three list items\ncal_lst[1:3]\n\n$elf_1\n[1] 11223  6323 10725 10761  3587\n\n$elf_2\n [1] 1274 1041 5566 1759 1372 1619 2228 1283 1981 1885 5894 1321 6081 4407 2992\n\n$elf_3\n [1] 7184 2310 7975 2752 7942 7616 3622 1320 1231 6191"
  },
  {
    "objectID": "posts/2023-01-05_aoc_Day01_Lists/2023-01-05_aoc_Day01_Lists.html#finding-the-objectives",
    "href": "posts/2023-01-05_aoc_Day01_Lists/2023-01-05_aoc_Day01_Lists.html#finding-the-objectives",
    "title": "Advent of Code Day 1: Working with Lists",
    "section": "Finding the objectives",
    "text": "Finding the objectives\nNow, it is easy to find the most calories carried by an elf using sapply and then sort. Since we named the list elements, we can see that elf_186 is caring the most calories.\n\n#total calories for each elf\ncal_by_elf <- sapply(cal_lst, sum)\nsort(cal_by_elf, decreasing = TRUE)[1]\n\nelf_186 \n  72511 \n\n\nSimilarly, the total amount for the top 3 elves carrying the most calories is determined.\n\nsum(sort(cal_by_elf, decreasing = TRUE)[1:3])\n\n[1] 212117\n\n\nThe distribution of calories carried by the elves can be visualized in a histogram.\n\nenframe(cal_by_elf) %>%\n  ggplot(aes(x=value)) +\n  geom_histogram(bins = 35, fill = \"steelblue\") +\n  labs(title = \"Calories of consumable items carried by Santa's elves\",\n       x = \"calories carried\")"
  },
  {
    "objectID": "posts/2023-01-05_aoc_Day01_Lists/2023-01-05_aoc_Day01_Lists.html#summary",
    "href": "posts/2023-01-05_aoc_Day01_Lists/2023-01-05_aoc_Day01_Lists.html#summary",
    "title": "Advent of Code Day 1: Working with Lists",
    "section": "Summary",
    "text": "Summary\nA dataset from the Advent of Code Day 1 problem has been loaded into a structured list. This format made performing the calculations required to meet the objectives quite straightforward. Further analysis such as building to histogram is possible since the calculations are preserved for all of the elves.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-01-05\n pandoc   2.19.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.2.269 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P dplyr       * 1.0.10  2022-09-01 [?] CRAN (R 4.2.2)\n P forcats     * 0.5.2   2022-08-19 [?] CRAN (R 4.2.2)\n P ggplot2     * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n   purrr       * 0.3.5   2022-10-06 [2] CRAN (R 4.2.1)\n   readr       * 2.1.3   2022-10-01 [2] CRAN (R 4.2.1)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble      * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n   tidyr       * 1.2.1   2022-09-08 [2] CRAN (R 4.2.1)\n P tidyverse   * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpeWEZEz/renv-library-2d8446fa353c\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/Temp/RtmpeWEZEz/renv-system-library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-01-11_aoc_Day02_Lookup/2023-01-11_aoc_Day02_Lookup.html",
    "href": "posts/2023-01-11_aoc_Day02_Lookup/2023-01-11_aoc_Day02_Lookup.html",
    "title": "Advent of Code Day 2: Using Lookup Tables",
    "section": "",
    "text": "Rules are defined in lookup tables and joined to the input dataset in order to calculate a score for a Rocks-Paper-Scissors game."
  },
  {
    "objectID": "posts/2023-01-11_aoc_Day02_Lookup/2023-01-11_aoc_Day02_Lookup.html#introduction",
    "href": "posts/2023-01-11_aoc_Day02_Lookup/2023-01-11_aoc_Day02_Lookup.html#introduction",
    "title": "Advent of Code Day 2: Using Lookup Tables",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 2. The scenario is that the elves are playing a game of rock-paper-scissors. You have a list of what your opponent chose where A is for Rock, B is for Paper and C is for Scissors and what you have chosen where X is for Rock, Y is for Paper and Z is for Scissors. So, the list looks something like this:\nA X\nC Y\nB Z\nB Y\nC X\nA X\nC Z\nYou need to determine the total score for the game. The score for each round is determined by what you chose to play (1 point for Rock, 2 points for Paper and 3 points for Scissors) and the result (0 points for a Loss, 3 points for a Draw and 6 points for a Win). In this case, of course, Rock beats Scissor, Paper beats Rock and Scissor beats Paper."
  },
  {
    "objectID": "posts/2023-01-11_aoc_Day02_Lookup/2023-01-11_aoc_Day02_Lookup.html#loading-and-analyzing-input-file",
    "href": "posts/2023-01-11_aoc_Day02_Lookup/2023-01-11_aoc_Day02_Lookup.html#loading-and-analyzing-input-file",
    "title": "Advent of Code Day 2: Using Lookup Tables",
    "section": "Loading and analyzing input file",
    "text": "Loading and analyzing input file\nThe input file has the structure as described in the introduction. The read_delim function from the readr package is used to load the file into a tibble (dataframe).\n\nlibrary(tidyverse)\nlibrary(gt)\n\nfilepath <- here::here(\"./posts/data/aoc/day02_input.txt\")\n\ndata <- read_delim(filepath, delim = \"\\n\", col_names = c(\"rounds\"))\n\nInitially, the data is loaded into a single column as a string with two characters separated by a space (e.g. “C Y”). I would prefer to separate the opponents choice and my choice into separate columns.\n\ndata <- data |> separate(rounds, into = c(\"opponent\", \"me\"), sep = \" \")\n\ndata |> slice(1:6) |> gt()\n\n\n\n\n\n  \n  \n    \n      opponent\n      me\n    \n  \n  \n    C\nY\n    B\nZ\n    B\nZ\n    C\nY\n    B\nY\n    C\nZ\n  \n  \n  \n\n\n\n\nNow, we define the scoring rules in lookup tables. The first table is for the points depending upon what you chose to play.\n\ntt_shape <- tribble(~me, ~shape_score,\n        \"X\", 1,\n        \"Y\", 2,\n        \"Z\", 3)\n\ntt_shape |> gt()\n\n\n\n\n\n  \n  \n    \n      me\n      shape_score\n    \n  \n  \n    X\n1\n    Y\n2\n    Z\n3\n  \n  \n  \n\n\n\n\nNext, we define a lookup table for the round result to define whether you win (W), lose (L) or draw (D).\n\ntt_result <- tribble(~opponent, ~me, ~result,\n        \"A\", \"X\", \"D\",\n        \"A\", \"Y\", \"W\",\n        \"A\", \"Z\", \"L\",\n        \"B\", \"X\", \"L\",\n        \"B\", \"Y\", \"D\",\n        \"B\", \"Z\", \"W\",\n        \"C\", \"X\", \"W\",\n        \"C\", \"Y\", \"L\",\n        \"C\", \"Z\", \"D\"\n        )\n\ntt_result |> gt()\n\n\n\n\n\n  \n  \n    \n      opponent\n      me\n      result\n    \n  \n  \n    A\nX\nD\n    A\nY\nW\n    A\nZ\nL\n    B\nX\nL\n    B\nY\nD\n    B\nZ\nW\n    C\nX\nW\n    C\nY\nL\n    C\nZ\nD\n  \n  \n  \n\n\n\n\nThe scoring for the win, loss and draw result is then defined.\n\ntt_rscore <- tribble(~result, ~result_score,\n        \"L\", 0,\n        \"D\", 3,\n        \"W\", 6)\n\ntt_rscore |> gt()\n\n\n\n\n\n  \n  \n    \n      result\n      result_score\n    \n  \n  \n    L\n0\n    D\n3\n    W\n6"
  },
  {
    "objectID": "posts/2023-01-11_aoc_Day02_Lookup/2023-01-11_aoc_Day02_Lookup.html#part-one-solution",
    "href": "posts/2023-01-11_aoc_Day02_Lookup/2023-01-11_aoc_Day02_Lookup.html#part-one-solution",
    "title": "Advent of Code Day 2: Using Lookup Tables",
    "section": "Part One: Solution",
    "text": "Part One: Solution\nThe goal for Part One is to calculate the total score for the rock-paper-scissors rounds contained in the input file. We can achieve this goal by applying the lookup tables to the input file data. A series of left joins is applied using dplyr is applied to the dataset, the score for each round is determined and then the scores for each round are summed to determine the total score.\n\ndata %>%\n  left_join(tt_shape, by = \"me\") %>%\n  left_join(tt_result, by = c(\"opponent\", \"me\")) %>%\n  left_join(tt_rscore, by = \"result\") %>%\n  rowwise() %>%\n  mutate(round_score = shape_score + result_score) %>%\n  ungroup() %>%\n  summarize(total_score = sum(round_score))\n\n# A tibble: 1 × 1\n  total_score\n        <dbl>\n1       13268"
  },
  {
    "objectID": "posts/2023-01-11_aoc_Day02_Lookup/2023-01-11_aoc_Day02_Lookup.html#part-two-solution",
    "href": "posts/2023-01-11_aoc_Day02_Lookup/2023-01-11_aoc_Day02_Lookup.html#part-two-solution",
    "title": "Advent of Code Day 2: Using Lookup Tables",
    "section": "Part Two: Solution",
    "text": "Part Two: Solution\nIn Part Two, we find out that the second column in the dataset containing the ‘X’, ‘Y’ and ‘Z’s do not refer to the choice played but rather the desired result for the round. So, we need to update the rules so that ’X’ is a loss, ‘Y’ is a draw and ‘Z’ is a draw therefore a new lookup table is created as shown below.\n\ntt_update <- tribble(~me, ~result,\n        \"X\", \"L\",\n        \"Y\", \"D\",\n        \"Z\", \"W\")\n\ntt_update |> gt()\n\n\n\n\n\n  \n  \n    \n      me\n      result\n    \n  \n  \n    X\nL\n    Y\nD\n    Z\nW\n  \n  \n  \n\n\n\n\nNow, we apply the rules update to determine the result and then the original tables are applied to determine the choice to play, the score for playing that choice and the score for the round result. Similarly to Part One, each round score is then calculated and summed to determine the total score.\n\ndata %>%\n  left_join(tt_update, by = \"me\") %>%\n  select(-me) %>%  #drop this column because the definition has changed\n  left_join(tt_result, by = c(\"opponent\", \"result\")) %>% #determine the new \"me\" column\n  left_join(tt_shape, by = \"me\") %>%\n  left_join(tt_rscore, by = \"result\") %>%\n  rowwise() %>%\n  mutate(round_score = shape_score + result_score) %>%\n  ungroup() %>%\n  summarize(total_score = sum(round_score))\n\n# A tibble: 1 × 1\n  total_score\n        <dbl>\n1       15508"
  },
  {
    "objectID": "posts/2023-01-11_aoc_Day02_Lookup/2023-01-11_aoc_Day02_Lookup.html#summary",
    "href": "posts/2023-01-11_aoc_Day02_Lookup/2023-01-11_aoc_Day02_Lookup.html#summary",
    "title": "Advent of Code Day 2: Using Lookup Tables",
    "section": "Summary",
    "text": "Summary\nIn this Advent of Code problem, lookup tables were created to represent the rules for scoring a rock-paper-scissors game in this fictitious scenario. The advantage of using lookup tables was seen in a subsequent rules change where a minimal amount of changes were required to calculate the new score.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-01-19\n pandoc   2.19.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.2.269 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P dplyr       * 1.0.10  2022-09-01 [?] CRAN (R 4.2.2)\n P forcats     * 0.5.2   2022-08-19 [?] CRAN (R 4.2.2)\n P ggplot2     * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P gt          * 0.8.0   2022-11-16 [?] CRAN (R 4.2.2)\n   purrr       * 0.3.5   2022-10-06 [2] CRAN (R 4.2.1)\n   readr       * 2.1.3   2022-10-01 [2] CRAN (R 4.2.1)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble      * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n   tidyr       * 1.2.1   2022-09-08 [2] CRAN (R 4.2.1)\n P tidyverse   * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpUFTZqi/renv-library-30cc735c13bd\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/Temp/RtmpUFTZqi/renv-system-library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-01-23_aoc_Day03_Strings/2023-01-23_aoc_Day03_Strings.html",
    "href": "posts/2023-01-23_aoc_Day03_Strings/2023-01-23_aoc_Day03_Strings.html",
    "title": "Advent of Code Day 3: Manipulating Strings",
    "section": "",
    "text": "Lists of elf items are contained within strings which are manipulated with the stringr package to determine loading priorities."
  },
  {
    "objectID": "posts/2023-01-23_aoc_Day03_Strings/2023-01-23_aoc_Day03_Strings.html#introduction",
    "href": "posts/2023-01-23_aoc_Day03_Strings/2023-01-23_aoc_Day03_Strings.html#introduction",
    "title": "Advent of Code Day 3: Manipulating Strings",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 3. This time the elves need to load items into rucksacks and the list of items is represented by strings such as:\n\nlibrary(tidyverse)\n\nfilepath <- here::here(\"./posts/data/aoc/day03_input.txt\")\n\ndata <- read_delim(filepath, delim = \"\\n\", col_names = c(\"items\"))\ndata |> slice(1:5)\n\nEach character in each string represents an item. The first half of the items in the string go into the first compartment and the second half of the items go into the second compartment. The item which is in both the first half and second half of the string sets the priority where:\nThe lowercase letters from a to z have priorities 1 through 26.\nThe uppercase letters from A to Z have priorities 27 through 52.\nThis priority table is rather easily defined in R using the letters constant for lowercase a-z and LETTERS for uppercase A-Z.\n\nitem_priority <- tibble(item_name = c(letters, LETTERS),\n                        item_prio = 1:52)"
  },
  {
    "objectID": "posts/2023-01-23_aoc_Day03_Strings/2023-01-23_aoc_Day03_Strings.html#solution-to-part-one",
    "href": "posts/2023-01-23_aoc_Day03_Strings/2023-01-23_aoc_Day03_Strings.html#solution-to-part-one",
    "title": "Advent of Code Day 3: Manipulating Strings",
    "section": "Solution to Part One",
    "text": "Solution to Part One\nThe goal of part one is to determine the sum of the priorities for all of the item lists. Again, the priority is defined by the common item in the first and second half of the string. The first step is to separate each string into two halves. The midpoint is determined by dividing the string length by 2. Functions from the stringr package are used to find the string length (str_length) and then create the sub-strings (str_sub).\n\nsplit_data <- data |>\n  mutate(item_len = str_length(items)) |>\n  rowwise() |>\n  mutate(items_1 = str_sub(items, 1, item_len / 2)) |>\n  mutate(items_2 = str_sub(items, (item_len / 2) + 1, item_len)) |>\n  ungroup()\n\nhead(split_data)\n\n# A tibble: 6 × 4\n  items                                            item_len items_1      items_2\n  <chr>                                               <int> <chr>        <chr>  \n1 zBBtHnnHtwwHplmlRlzPLCpp                               24 zBBtHnnHtwwH plmlRl…\n2 vvhJccJFGFcNsdNNJbhJsJQplQMRLQMlfdfTPCLfQQCT           44 vvhJccJFGFc… QplQMR…\n3 GPhjcjhZDjWtnSVH                                       16 GPhjcjhZ     DjWtnS…\n4 BNhHVhrGNVTbDHdDJdJRPJdSQQSJwPjR                       32 BNhHVhrGNVT… JdJRPJ…\n5 lvtsfbsqzwSnJcvjSm                                     18 lvtsfbsqz    wSnJcv…\n6 MftttFLftZMLgtgMbltMqZzbDNrTpVGhNWrDTrpTGNpZGZhD       48 MftttFLftZM… DNrTpV…\n\n\nThe common item in both compartments is identified by using two more stringr package functions. The str_split function is used to create a list of the characters in each sub-string. The str_unique function (new to stringr 1.5.0) is used to create a list of only the unique characters in the substring. Finally, the intersect function from dplyr is used to find the common character is both lists.\n\ncommon_items <- split_data |>  \n  rowwise() |>\n  mutate(split_1 = str_split(items_1, \"\")) |>\n  mutate(split_2 = str_split(items_2, \"\")) |>\n  mutate(unique_1 = list(str_unique(unlist(split_1)))) |>\n  mutate(unique_2 = list(str_unique(unlist(split_2)))) |>\n  mutate(common_item = intersect(unique_1, unique_2)) |>\n  ungroup() |>\n  select(items_1, items_2, common_item)\n\ncommon_items |> slice(1:6)\n\n# A tibble: 6 × 3\n  items_1                  items_2                  common_item\n  <chr>                    <chr>                    <chr>      \n1 zBBtHnnHtwwH             plmlRlzPLCpp             z          \n2 vvhJccJFGFcNsdNNJbhJsJ   QplQMRLQMlfdfTPCLfQQCT   d          \n3 GPhjcjhZ                 DjWtnSVH                 j          \n4 BNhHVhrGNVTbDHdD         JdJRPJdSQQSJwPjR         d          \n5 lvtsfbsqz                wSnJcvjSm                v          \n6 MftttFLftZMLgtgMbltMqZzb DNrTpVGhNWrDTrpTGNpZGZhD Z          \n\n\nThe common_items are then joined with item_priority and then the priorities are summed to find the total priority.\n\nleft_join(common_items, item_priority, by = c(\"common_item\" = \"item_name\")) %>%\n  summarize(total_priority = sum(item_prio))\n\n# A tibble: 1 × 1\n  total_priority\n           <int>\n1           7597"
  },
  {
    "objectID": "posts/2023-01-23_aoc_Day03_Strings/2023-01-23_aoc_Day03_Strings.html#solution-to-part-two",
    "href": "posts/2023-01-23_aoc_Day03_Strings/2023-01-23_aoc_Day03_Strings.html#solution-to-part-two",
    "title": "Advent of Code Day 3: Manipulating Strings",
    "section": "Solution to Part Two",
    "text": "Solution to Part Two\nIn Part Two, we are informed that each elf is part of group consisting of three elves and each group has an identifying badge. Every set of three lines in the dataset represents one group and the common item among all three lines is the badge. Priorities for the items are the same as defined in Part One where a-z is 1 to 26 and A-Z is 27 to 52. The question is what is the sum of the priorities for the group badges.\nIn this case, we do not need to split the strings in half but we do need to identify the unique characters in each string similar to Part One.\n\nunique_data <- data |>\n  rowwise() |>\n  mutate(items_split = str_split(items, \"\")) |>\n  mutate(items_unique = list(str_unique(unlist(items_split)))) |>\n  ungroup()\n\nunique_data |> slice(1:6)\n\n# A tibble: 6 × 3\n  items                                            items_split items_unique\n  <chr>                                            <list>      <list>      \n1 zBBtHnnHtwwHplmlRlzPLCpp                         <chr [24]>  <chr [13]>  \n2 vvhJccJFGFcNsdNNJbhJsJQplQMRLQMlfdfTPCLfQQCT     <chr [44]>  <chr [20]>  \n3 GPhjcjhZDjWtnSVH                                 <chr [16]>  <chr [13]>  \n4 BNhHVhrGNVTbDHdDJdJRPJdSQQSJwPjR                 <chr [32]>  <chr [18]>  \n5 lvtsfbsqzwSnJcvjSm                               <chr [18]>  <chr [15]>  \n6 MftttFLftZMLgtgMbltMqZzbDNrTpVGhNWrDTrpTGNpZGZhD <chr [48]>  <chr [20]>  \n\n\nNow we need to identify the groups by introducing a new id column, elf_group, for each set of three lines. We then group_by this new column and find the intersection between the first two lines and the result is intersected with the last line to find the common item. The Reduce function is used to iteratively apply the function with two arguments at a time.\n\nbadge_data <- unique_data |>\n  mutate(elf_group = rep(1:(nrow(data) %/% 3), each = 3)) |>\n  mutate(elf_group = as.factor(elf_group)) |>\n  group_by(elf_group) |>\n  summarize(badge = Reduce(intersect, items_unique), .groups = \"drop\")\n\nbadge_data |> slice(1:6)\n\n# A tibble: 6 × 2\n  elf_group badge\n  <fct>     <chr>\n1 1         P    \n2 2         b    \n3 3         Z    \n4 4         S    \n5 5         q    \n6 6         c    \n\n\nThe total priority for the badge items is then calculated by joining with item_priority.\n\nleft_join(badge_data, item_priority, by = c(\"badge\" = \"item_name\")) %>%\n  summarize(total_priority = sum(item_prio))\n\n# A tibble: 1 × 1\n  total_priority\n           <int>\n1           2607"
  },
  {
    "objectID": "posts/2023-01-23_aoc_Day03_Strings/2023-01-23_aoc_Day03_Strings.html#summary",
    "href": "posts/2023-01-23_aoc_Day03_Strings/2023-01-23_aoc_Day03_Strings.html#summary",
    "title": "Advent of Code Day 3: Manipulating Strings",
    "section": "Summary",
    "text": "Summary\nThis Advent of Code problem provided a great opportunity to brush up on some useful stringr functions. It was also interesting to keep all of the data transformations in tibble columns. A nice application of the Reduce function (base R!) was used to find the common item among several strings.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-02-02\n pandoc   2.19.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.2.269 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P dplyr       * 1.0.10  2022-09-01 [?] CRAN (R 4.2.2)\n P forcats     * 0.5.2   2022-08-19 [?] CRAN (R 4.2.2)\n P ggplot2     * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n   purrr       * 0.3.5   2022-10-06 [2] CRAN (R 4.2.1)\n   readr       * 2.1.3   2022-10-01 [2] CRAN (R 4.2.1)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.5.0   2022-12-02 [?] CRAN (R 4.2.2)\n P tibble      * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n   tidyr       * 1.2.1   2022-09-08 [2] CRAN (R 4.2.1)\n P tidyverse   * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpUHj1Wl/renv-library-24087b9d1284\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/Temp/RtmpUHj1Wl/renv-system-library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-02-07_aoc_Day04_Separate/2023-02-07_aoc_Day04_Separate.html",
    "href": "posts/2023-02-07_aoc_Day04_Separate/2023-02-07_aoc_Day04_Separate.html",
    "title": "Advent of Code Day 4: Separate",
    "section": "",
    "text": "Contiguous sections to be cleaned by the elves are parsed from the input file to identify overlapping assignments."
  },
  {
    "objectID": "posts/2023-02-07_aoc_Day04_Separate/2023-02-07_aoc_Day04_Separate.html#introduction",
    "href": "posts/2023-02-07_aoc_Day04_Separate/2023-02-07_aoc_Day04_Separate.html#introduction",
    "title": "Advent of Code Day 4: Separate",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 4. The contiguous sections for each set of elf cleaning crews is listed as one line separated by a comma. Each contiguous section for each crew is listed as first section to be cleaned and last section to be cleaned separated by a hyphen.\n\nlibrary(tidyverse)\n\nfilepath <- here::here(\"./posts/data/aoc/day04_input.txt\")\n\ndata <- read_delim(filepath, delim = \"\\n\", col_names = c(\"assignments\"))\n\nSo, in the first line, crew 1 is assigned to clean section 31 (i.e. first and last section to be cleaned). Crew 2 is assigned to clean from section 32 to section 40. We can parse these input lines using the separate function from the tidyr package.\n\nsplit_data <- data |>\n  separate(assignments, c(\"clean_1\", \"clean_2\"), sep = \",\") |>\n  separate(clean_1, c(\"clean_1_start\", \"clean_1_end\"), sep = \"-\") |>\n  separate(clean_2, c(\"clean_2_start\", \"clean_2_end\"), sep = \"-\") |>\n  mutate_if(is.character, as.integer)\n\nsplit_data |> slice(1:5)\n\n# A tibble: 5 × 4\n  clean_1_start clean_1_end clean_2_start clean_2_end\n          <int>       <int>         <int>       <int>\n1            31          31            32          40\n2            26          92            13          91\n3             9          90            29          91\n4            72          72            25          73\n5            28          79            79          79"
  },
  {
    "objectID": "posts/2023-02-07_aoc_Day04_Separate/2023-02-07_aoc_Day04_Separate.html#solution-to-part-one",
    "href": "posts/2023-02-07_aoc_Day04_Separate/2023-02-07_aoc_Day04_Separate.html#solution-to-part-one",
    "title": "Advent of Code Day 4: Separate",
    "section": "Solution to Part One",
    "text": "Solution to Part One\nThe first question is to find for how many assignments are the assigned sections for one cleaning crew completely contained within the assigned sections for the other cleaning crew. For example, if cleaning crew 1 is assigned sections 2-8 and cleaning crew 2 is assigned sections 3-5 then the assignment for crew 2 is contained within the assignment for crew 1.\n\noverlap_data <- split_data |>\n  rowwise() |>\n  mutate(overlap = (clean_1_start <= clean_2_start) & (clean_1_end >= clean_2_end) |     #clean 2 contained within clean 1 or\n                   (clean_1_start >= clean_2_start) & (clean_1_end <= clean_2_end)) |>   #clean 1 contained within clean 2\n  ungroup()\n\noverlap_data |> slice(1:5)\n\n# A tibble: 5 × 5\n  clean_1_start clean_1_end clean_2_start clean_2_end overlap\n          <int>       <int>         <int>       <int> <lgl>  \n1            31          31            32          40 FALSE  \n2            26          92            13          91 FALSE  \n3             9          90            29          91 FALSE  \n4            72          72            25          73 TRUE   \n5            28          79            79          79 TRUE   \n\n\nYou can then add up all of rows where overlap is TRUE to find the answer to Part One.\n\noverlap_data |>\n  summarize(total_overlaps = sum(overlap))\n\n# A tibble: 1 × 1\n  total_overlaps\n           <int>\n1            556"
  },
  {
    "objectID": "posts/2023-02-07_aoc_Day04_Separate/2023-02-07_aoc_Day04_Separate.html#solution-to-part-two",
    "href": "posts/2023-02-07_aoc_Day04_Separate/2023-02-07_aoc_Day04_Separate.html#solution-to-part-two",
    "title": "Advent of Code Day 4: Separate",
    "section": "Solution to Part Two",
    "text": "Solution to Part Two\nThe second question is for how many of the assignments do the sections for crew 1 and crew 2 overlap at all. The easier way to answer this question is to find the assignments with no overlap. You just need to check if the ending section for crew 1 is less than the starting section for crew 2 or the ending section for crew 2 is less than the starting section for crew 1.\n\npartial_overlap_data <- split_data |>\n  rowwise() |>\n  mutate(no_overlap = (clean_1_end < clean_2_start) | (clean_2_end < clean_1_start)) |>\n  ungroup()\n\npartial_overlap_data |> slice(1:5)\n\n# A tibble: 5 × 5\n  clean_1_start clean_1_end clean_2_start clean_2_end no_overlap\n          <int>       <int>         <int>       <int> <lgl>     \n1            31          31            32          40 TRUE      \n2            26          92            13          91 FALSE     \n3             9          90            29          91 FALSE     \n4            72          72            25          73 FALSE     \n5            28          79            79          79 FALSE     \n\n\nYou can then add up all the rows where no_overlap is TRUE and subtract from the total number of rows (assignments).\n\npartial_overlap_data |>\n  summarize(total_partial_overlap = nrow(partial_overlap_data) - sum(no_overlap))\n\n# A tibble: 1 × 1\n  total_partial_overlap\n                  <int>\n1                   876"
  },
  {
    "objectID": "posts/2023-02-07_aoc_Day04_Separate/2023-02-07_aoc_Day04_Separate.html#summary",
    "href": "posts/2023-02-07_aoc_Day04_Separate/2023-02-07_aoc_Day04_Separate.html#summary",
    "title": "Advent of Code Day 4: Separate",
    "section": "Summary",
    "text": "Summary\nThe problem for Day 4 was pretty straightforward in implementing the logical tests once the input data was tidied. The separate function made it easy to parse the input data into a tibble and then perform row-wise calculations.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-02-07\n pandoc   2.19.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.2.269 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P dplyr       * 1.0.10  2022-09-01 [?] CRAN (R 4.2.2)\n P forcats     * 0.5.2   2022-08-19 [?] CRAN (R 4.2.2)\n P ggplot2     * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n   purrr       * 0.3.5   2022-10-06 [2] CRAN (R 4.2.1)\n   readr       * 2.1.3   2022-10-01 [2] CRAN (R 4.2.1)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.5.0   2022-12-02 [?] CRAN (R 4.2.2)\n P tibble      * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n   tidyr       * 1.2.1   2022-09-08 [2] CRAN (R 4.2.1)\n P tidyverse   * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpGugBWK/renv-library-38782e7835ab\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/Temp/RtmpGugBWK/renv-system-library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-04-14_aoc_Day05_Stacks/2023-04-14_aoc_Day05_Stacks.html",
    "href": "posts/2023-04-14_aoc_Day05_Stacks/2023-04-14_aoc_Day05_Stacks.html",
    "title": "Advent of Code Day 5: Stacking Crates",
    "section": "",
    "text": "Using base R to read a custom, input file format and manipulate the data in lists"
  },
  {
    "objectID": "posts/2023-04-14_aoc_Day05_Stacks/2023-04-14_aoc_Day05_Stacks.html#introduction",
    "href": "posts/2023-04-14_aoc_Day05_Stacks/2023-04-14_aoc_Day05_Stacks.html#introduction",
    "title": "Advent of Code Day 5: Stacking Crates",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 5. Supplies for the elves’ expedition are loaded in crates which are contained in nine stacks. Unfortunately, the order of the crates in each stack is not correct and a crane operator needs to move the crates between the stacks until all the crates are in the correct order for unloading."
  },
  {
    "objectID": "posts/2023-04-14_aoc_Day05_Stacks/2023-04-14_aoc_Day05_Stacks.html#loading-the-input-file",
    "href": "posts/2023-04-14_aoc_Day05_Stacks/2023-04-14_aoc_Day05_Stacks.html#loading-the-input-file",
    "title": "Advent of Code Day 5: Stacking Crates",
    "section": "Loading the input file",
    "text": "Loading the input file\nThe input file consists of two sections. The first section contains a visual representation of the positioning of the crates in each of the nine stacks. The second section contains the instructions for the crane operator to move crates from one stack to another. The text for the first section is shown below.\n[V]     [B]                     [F]\n[N] [Q] [W]                 [R] [B]\n[F] [D] [S]     [B]         [L] [P]\n[S] [J] [C]     [F] [C]     [D] [G]\n[M] [M] [H] [L] [P] [N]     [P] [V]\n[P] [L] [D] [C] [T] [Q] [R] [S] [J]\n[H] [R] [Q] [S] [V] [R] [V] [Z] [S]\n[J] [S] [N] [R] [M] [T] [G] [C] [D]\n 1   2   3   4   5   6   7   8   9 \nThe first step is to read the first 8 lines containing the crate locations into character strings. In this case, the length of the 8 strings are the same which makes it easier to handle. We also define the position of the crate labels (i.e. letter) as the brackets do not add much other than helping with the visual representation.\n\nfilepath <- here::here(\"./posts/data/aoc/day05_input.txt\")\n\nboxlines <- readLines(filepath, n=8L)\n\ncpositions <- c(2, 6, 10, 14, 18, 22, 26, 30, 34)\n\nNext, I defined two functions. The first function, extract_chars extracts the letters from one of the strings at the correct positions. The second function, extract_lines uses extract_chars to extract all of the lines and merge into one list containing all nine stack labels.\n\nextract_chars <- function(stringToExtract, charPositions) {\n  char_list <- list()\n  for (n in 1:length(charPositions)) {\n    char_list <- c(char_list, substring(stringToExtract, charPositions[n], charPositions[n]))\n  }\n  return(char_list)\n}\n\nextract_lines <- function(stringsToExtract, charPositions) {\n  list1 <- extract_chars(stringsToExtract[1], charPositions)\n  for (i in 2:length(stringsToExtract)) {\n    list2 <- extract_chars(stringsToExtract[i], charPositions)\n    list1 <- mapply(c, list1, list2, SIMPLIFY = FALSE)\n  }\n  return(list1)\n}\n\nThe lines read from the input file and the positions of the crate labels are used as the arguments for extract_lines to generate the desired list object.\n\nbox_lists <- extract_lines(boxlines, cpositions)\n\nstr(box_lists)\n\nList of 9\n $ : chr [1:8] \"V\" \"N\" \"F\" \"S\" ...\n $ : chr [1:8] \" \" \"Q\" \"D\" \"J\" ...\n $ : chr [1:8] \"B\" \"W\" \"S\" \"C\" ...\n $ : chr [1:8] \" \" \" \" \" \" \" \" ...\n $ : chr [1:8] \" \" \" \" \"B\" \"F\" ...\n $ : chr [1:8] \" \" \" \" \" \" \"C\" ...\n $ : chr [1:8] \" \" \" \" \" \" \" \" ...\n $ : chr [1:8] \" \" \"R\" \"L\" \"D\" ...\n $ : chr [1:8] \"F\" \"B\" \"P\" \"G\" ...\n\n\nThe blank spaces need to removed from the character vectors as they are not needed and would only interfere with subsequent manipulations. A simple for loop is used for this purpose.\n\nfor (i in 1:length(box_lists)) {\n  box_lists[[i]] <- box_lists[[i]][box_lists[[i]] != \" \"]\n}\n\nstr(box_lists)\n\nList of 9\n $ : chr [1:8] \"V\" \"N\" \"F\" \"S\" ...\n $ : chr [1:7] \"Q\" \"D\" \"J\" \"M\" ...\n $ : chr [1:8] \"B\" \"W\" \"S\" \"C\" ...\n $ : chr [1:4] \"L\" \"C\" \"S\" \"R\"\n $ : chr [1:6] \"B\" \"F\" \"P\" \"T\" ...\n $ : chr [1:5] \"C\" \"N\" \"Q\" \"R\" ...\n $ : chr [1:3] \"R\" \"V\" \"G\"\n $ : chr [1:7] \"R\" \"L\" \"D\" \"P\" ...\n $ : chr [1:8] \"F\" \"B\" \"P\" \"G\" ...\n\n\nThe box_lists object now has the crates labels for each stack in order from top to bottom. Now, we need to read the crane operator instructions which is beneath the first section and separated by a blank line. The script for reading the lines containing the instructions is listed below. There is a flag, start_collecting to indicate where the second section begins (after the blank line).\n\nstart_collecting <- 0\nbox_moves <- list()\nline_number <- 0\n\ncon = file(filepath, \"r\")\nwhile (TRUE) {\n  # line_number <- line_number + 1\n  oneLine = readLines(con, n = 1)\n  # print(oneLine)\n  if (length(oneLine) == 0) {\n    break\n  }\n  if (start_collecting == 0 & nchar(oneLine) == 0) {\n    start_collecting <- 1\n    # print(\"Started collecting\")\n  }\n  if (start_collecting == 1) {\n    if (nchar(oneLine) > 0)  \n    box_moves <- c(box_moves, oneLine)\n  }\n}\nclose(con)\n\nhead(box_moves, 5)\n\n[[1]]\n[1] \"move 1 from 8 to 4\"\n\n[[2]]\n[1] \"move 1 from 7 to 8\"\n\n[[3]]\n[1] \"move 1 from 6 to 3\"\n\n[[4]]\n[1] \"move 2 from 6 to 5\"\n\n[[5]]\n[1] \"move 8 from 5 to 1\"\n\n\nThe instruction explain how many crates to move and then from which stack and to which stack. For example, the first instruction listed above indicates to move 1 crate from the top of stack 8 to the top of stack 4. The instructions are parsed so that the numbers can accessed directly.\n\nbox_moves_parsed <- sapply(box_moves, function(x) strsplit(x, split = \" \"))\n\nhead(box_moves_parsed, 5)\n\n[[1]]\n[1] \"move\" \"1\"    \"from\" \"8\"    \"to\"   \"4\"   \n\n[[2]]\n[1] \"move\" \"1\"    \"from\" \"7\"    \"to\"   \"8\"   \n\n[[3]]\n[1] \"move\" \"1\"    \"from\" \"6\"    \"to\"   \"3\"   \n\n[[4]]\n[1] \"move\" \"2\"    \"from\" \"6\"    \"to\"   \"5\"   \n\n[[5]]\n[1] \"move\" \"8\"    \"from\" \"5\"    \"to\"   \"1\""
  },
  {
    "objectID": "posts/2023-04-14_aoc_Day05_Stacks/2023-04-14_aoc_Day05_Stacks.html#moving-the-crates",
    "href": "posts/2023-04-14_aoc_Day05_Stacks/2023-04-14_aoc_Day05_Stacks.html#moving-the-crates",
    "title": "Advent of Code Day 5: Stacking Crates",
    "section": "Moving the Crates",
    "text": "Moving the Crates\nNow that both sections of the input file have been loaded into R objects, the crate operator instructions can be applied. The input file contained 504 instruction steps! A new function is created to adjust the stack lists contained in box_lists based on the instruction step. An important part, that I initially missed, is that for the first “9000 model” crane the crates are moved one at a time even when more than one crate is listed in the step. The top crate is moved and then the subsequent crates one-by-one. This results in the crates being moved ending up in reverse order on top of the stack they are moved to.\n\napply_instruction_step <- function(current_box_lists, move_step, model = \"9000\") {\n  num_boxes <- as.numeric(move_step[2])\n  #print(num_boxes)\n  from_loc <- as.numeric(move_step[4])\n  #print(from_loc)\n  to_loc <- as.numeric(move_step[6])\n  #print(to_loc)\n  if (model == \"9000\") {\n    current_box_lists[[to_loc]] <- c(current_box_lists[[from_loc]][(num_boxes:1)], current_box_lists[[to_loc]])\n  } else {\n    current_box_lists[[to_loc]] <- c(current_box_lists[[from_loc]][(1:num_boxes)], current_box_lists[[to_loc]])\n  }\n  #print(current_box_lists[[to_loc]])\n  current_box_lists[[from_loc]] <- current_box_lists[[from_loc]][-(1:num_boxes)]\n  #print(current_box_lists[[from_loc]])\n  return(current_box_lists)\n}\n\nNow, I run the function for all of the instructions. Once all steps are completed, the top crate for each stack is identified and the puzzle answer.\n\ntemp <- box_lists\nfor (i in 1:length(box_moves_parsed)) {\n  temp <- apply_instruction_step(temp, box_moves_parsed[[i]], model = \"9000\")\n}\n\n# Show the label for the top crate for each stack\nsapply(temp, '[[', 1)\n\n[1] \"S\" \"B\" \"P\" \"Q\" \"R\" \"S\" \"C\" \"D\" \"F\"\n\n\nPart two of the puzzle involves an upgrade of the crane to “model 9001”. The new crane can now move multiple boxes instead of one at a time. So, now the crates are moved in order rather than the reverse order for the “model 9000” crane. We already have built this argument into our apply_instruction_step function.\n\ntemp <- box_lists\nfor (i in 1:length(box_moves_parsed)) {\n  temp <- apply_instruction_step(temp, box_moves_parsed[[i]], model = \"9001\")\n}\n\n# Show the label for the top crate for each stack\nsapply(temp, '[[', 1)\n\n[1] \"R\" \"G\" \"L\" \"V\" \"R\" \"C\" \"Q\" \"S\" \"B\""
  },
  {
    "objectID": "posts/2023-04-14_aoc_Day05_Stacks/2023-04-14_aoc_Day05_Stacks.html#summary",
    "href": "posts/2023-04-14_aoc_Day05_Stacks/2023-04-14_aoc_Day05_Stacks.html#summary",
    "title": "Advent of Code Day 5: Stacking Crates",
    "section": "Summary",
    "text": "Summary\nThe Day 5 puzzle required quite a bit of thinking about how to load the input file in order to solve the puzzle. I chose to load the data into strings for manipulation using base R commands. Initially, I missed the difference between the model 9000 and model 9001 cranes in my first read which led to some head-scratching on why my part one solution wasn’t right. Always read the instructions thoroughly!\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-04-14\n pandoc   2.19.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.2.335 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpET91Kg/renv-library-9d037ae2570\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/Temp/RtmpET91Kg/renv-system-library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-04-25_aoc_Day06_Decode/2023-04-25_aoc_Day06_Decode.html",
    "href": "posts/2023-04-25_aoc_Day06_Decode/2023-04-25_aoc_Day06_Decode.html",
    "title": "Advent of Code Day 6: Decoding Signals",
    "section": "",
    "text": "Find a start-of-packet marker in a string of characters"
  },
  {
    "objectID": "posts/2023-04-25_aoc_Day06_Decode/2023-04-25_aoc_Day06_Decode.html#introduction",
    "href": "posts/2023-04-25_aoc_Day06_Decode/2023-04-25_aoc_Day06_Decode.html#introduction",
    "title": "Advent of Code Day 6: Decoding Signals",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 6. You have a defective communication device that you need to fix in order to communicate with the other elves. You must write a subroutine to identify a start-of-packet marker in the incoming datastream."
  },
  {
    "objectID": "posts/2023-04-25_aoc_Day06_Decode/2023-04-25_aoc_Day06_Decode.html#loading-the-input-file",
    "href": "posts/2023-04-25_aoc_Day06_Decode/2023-04-25_aoc_Day06_Decode.html#loading-the-input-file",
    "title": "Advent of Code Day 6: Decoding Signals",
    "section": "Loading the input file",
    "text": "Loading the input file\nThe input file consists of a long, string of characters (4096 characters long). I used the file_read function from the readr package in order to read the whole file directly into a string object. I then parsed the string by character using the str_split from the stringr package as shown below.\n\nlibrary(readr)\nlibrary(stringr)\n\n\nfilepath <- here::here(\"./posts/data/aoc/day06_input.txt\")\nfile_str <- read_file(filepath)\n\nfile_split <- str_split(file_str, boundary(\"character\"))[[1]]"
  },
  {
    "objectID": "posts/2023-04-25_aoc_Day06_Decode/2023-04-25_aoc_Day06_Decode.html#finding-the-start-of-packet-marker",
    "href": "posts/2023-04-25_aoc_Day06_Decode/2023-04-25_aoc_Day06_Decode.html#finding-the-start-of-packet-marker",
    "title": "Advent of Code Day 6: Decoding Signals",
    "section": "Finding the start-of-packet marker",
    "text": "Finding the start-of-packet marker\nThe goal of the first part of the puzzle is to identify the first occurrence of the start-of-packet marker. This marker is denoted by four characters which are all different. For example, in the string below:\nmjqjpqmgbljsphdztnvjfqwrcgsmlb\nThe first time four different characters are detected in the sequence is after the 7th character is received and you have the sequence “jpqm”. The goal is to report the number of characters processed before this start-of-packet marker is found in the input data. The subroutine can be written quite easily by determining the number of unique characters in the 4 character buffer as it traverses the input data string. There are different ways to handle the first three characters. The way it is handled below is the buffer is initiated by replicating the first character 3 times. It could be handled in other ways such as not analyzing for uniqueness until the first four characters are loaded or initialize the buffer with the first four characters and begin the loop at the fifth character.\n\nmarker_length <- 4\nbuffer <- rep(file_split[1], marker_length-1)\ncposition <- 0\n\nfor (i in 1:length(file_split)) {\n  cposition <- cposition + 1\n  if (length(unique(c(buffer, file_split[i]))) == marker_length) {\n    break\n  } else {\n    buffer <- c(buffer[2:(marker_length-1)], file_split[i])\n  }\n}\n\ncposition\n\n[1] 1300\n\n\nIn this case, cposition gives the number of characters processed before the start-of-packet marker sequence is detected. The buffer is updated by adding the new character and checking to see if the unique characters is equal to 4. If equal to 4, the for loop is stopped and cposition is reported. If not equal to 4, the first (oldest) character in the buffer is removed and the loop is continued. For my input data file, 1300 characters are processed before the start-of-packet marker is detected."
  },
  {
    "objectID": "posts/2023-04-25_aoc_Day06_Decode/2023-04-25_aoc_Day06_Decode.html#finding-the-start-of-message-marker",
    "href": "posts/2023-04-25_aoc_Day06_Decode/2023-04-25_aoc_Day06_Decode.html#finding-the-start-of-message-marker",
    "title": "Advent of Code Day 6: Decoding Signals",
    "section": "Finding the start-of-message marker",
    "text": "Finding the start-of-message marker\nThe goal of the second part of the puzzle is to find the start-of-message marker which is represented by 14 distinct characters instead of the 4 distinct characters from the first part. Detecting this marker just requires updating the value of the marker_length to 14 in the code used in part one.\n\nmarker_length <- 14\nbuffer <- rep(file_split[1], marker_length-1)\ncposition <- 0\n\nfor (i in 1:length(file_split)) {\n  cposition <- cposition + 1\n  if (length(unique(c(buffer, file_split[i]))) == marker_length) {\n    break\n  } else {\n    buffer <- c(buffer[2:(marker_length-1)], file_split[i])\n  }\n}\n\ncposition\n\n[1] 3986\n\n\nFor part two, 3986 characters are processed before the start-of-message marker is detected. Since the code is reused with only a change in the value of marker_length a function could be created with an argument to pass any marker length value. I did not do this step here as the puzzle answers had already been determined."
  },
  {
    "objectID": "posts/2023-04-25_aoc_Day06_Decode/2023-04-25_aoc_Day06_Decode.html#summary",
    "href": "posts/2023-04-25_aoc_Day06_Decode/2023-04-25_aoc_Day06_Decode.html#summary",
    "title": "Advent of Code Day 6: Decoding Signals",
    "section": "Summary",
    "text": "Summary\nThe Day 6 puzzle was quite straightforward especially with the use of the unique function in base R. Functions from readr and stringr made it easy to process the input file into the desired character vector.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-05-08\n pandoc   2.19.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.2.335 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n   readr       * 2.1.3   2022-10-01 [2] CRAN (R 4.2.1)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.5.0   2022-12-02 [?] CRAN (R 4.2.2)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpiAoZ4Q/renv-library-2e3455614438\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/Temp/RtmpiAoZ4Q/renv-system-library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-05-05_aoc_Day07_Filepaths/2023-05-05_aoc_Day07_Filepaths.html",
    "href": "posts/2023-05-05_aoc_Day07_Filepaths/2023-05-05_aoc_Day07_Filepaths.html",
    "title": "Advent of Code Day 7: Tracking file paths",
    "section": "",
    "text": "Track the directory paths and file sizes from a series of OS commands"
  },
  {
    "objectID": "posts/2023-05-05_aoc_Day07_Filepaths/2023-05-05_aoc_Day07_Filepaths.html#introduction",
    "href": "posts/2023-05-05_aoc_Day07_Filepaths/2023-05-05_aoc_Day07_Filepaths.html#introduction",
    "title": "Advent of Code Day 7: Tracking file paths",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 7. The communication device that you need to communicate with the other elves has now run out of file space. You must determine the file size of each directory and then identify the directory to delete in order to meet the proper operating requirements."
  },
  {
    "objectID": "posts/2023-05-05_aoc_Day07_Filepaths/2023-05-05_aoc_Day07_Filepaths.html#loading-the-input-file",
    "href": "posts/2023-05-05_aoc_Day07_Filepaths/2023-05-05_aoc_Day07_Filepaths.html#loading-the-input-file",
    "title": "Advent of Code Day 7: Tracking file paths",
    "section": "Loading the input file",
    "text": "Loading the input file\nThe input file consists of 964 lines of operating system commands and output. Simply using readLines function from base R does a fine job of reading each command as a string in a string vector containing all of the commands.\n\nlibrary(stringr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nfilepath <- here::here(\"./posts/data/aoc/day07_input.txt\")\ncommands <- readLines(filepath)\n\nThe input file structure looks similar to what is shown below. The OS commands begin with a “$” with cd and ls commands for change directory and list files respectively. After the ls command, the sub-directories and files are listed for the current directory with directories starting with dir and files listing the file size followed by the file name.\n$ cd /\n$ ls\ndir a\n14848514 b.txt\n8504156 c.dat\ndir d\n$ cd a\n$ ls\ndir e\n29116 f\n2557 g\n62596 h.lst\n$ cd e\n$ ls\n584 i\n$ cd ..\n$ cd ..\n$ cd d\n$ ls\n4060174 j\n8033020 d.log\n5626152 d.ext\n7214296 k"
  },
  {
    "objectID": "posts/2023-05-05_aoc_Day07_Filepaths/2023-05-05_aoc_Day07_Filepaths.html#part-one-directory-sizes",
    "href": "posts/2023-05-05_aoc_Day07_Filepaths/2023-05-05_aoc_Day07_Filepaths.html#part-one-directory-sizes",
    "title": "Advent of Code Day 7: Tracking file paths",
    "section": "Part One: Directory sizes",
    "text": "Part One: Directory sizes\nThe first goal is to determine the size of each directory from the commands and output listed in the input file. The directory paths can be tracked from the commands containing cd. There are three types of cd commands: 1) $ cd \\ to go back to the root directory, 2) $ cd .. to go back (up) one directory and 3) $ cd[dir name]. The ls commands can be ignored as the subsequent output can be handled directly. The dir output can also be ignored since the directories are tracked with the cd commands. The file size and name output must be saved to their corresponding directory path.\n\npath <- \"root\"\n# Initialize tibble to store the file info\nfiledf <- tibble(dirpath = character(), filesize = character(), filename = character())\n\nfor (command in commands) {\n  # Split each command into a string vector\n  comm_spl <- str_split(command, pattern = \" \", simplify = TRUE)\n  \n  if (comm_spl[[1]] == \"$\") {\n    \n    if (comm_spl[[2]] == \"ls\") {\n      # do nothing\n    } else {\n      # Otherwise it is a cd command so update the path\n      if (comm_spl[[3]] == \"/\") {\n        path <- path[1]\n      } else if (comm_spl[[3]] == \"..\") {\n        path <- path[1:length(path)-1]\n      } else {\n        path <- c(path, comm_spl[[3]])\n      }\n    }\n  } else if (comm_spl[[1]] == \"dir\") {\n    # do nothing\n  } else  {\n    # Otherwise it is a number and a file name\n    # For loop to save the file size for current directory and all parent directories\n    for (i in 1:length(path)) {\n      filedf <- bind_rows(filedf, tibble(dirpath = paste(path[1:i], collapse = \"/\"), filesize = comm_spl[[1]], filename = comm_spl[[2]]))\n    }\n  }\n}\n\nThe filedf tibble now contains a row for each file in each directory (including files in the sub-directories of the directory). The total directory size can now be determined by grouping on the directory path and using the dplyr summarize function.\n\ndirsizes <- filedf |>\n  mutate(filesize = as.numeric(filesize)) |>\n  group_by(dirpath) |> \n  summarize(total = sum(filesize), .groups = \"drop\")\n\nhead(dirsizes)\n\n# A tibble: 6 × 2\n  dirpath                                   total\n  <chr>                                     <dbl>\n1 root                                   42805968\n2 root/cmwrq                             10449487\n3 root/cmwrq/dtbzzl                       2919892\n4 root/cmwrq/dtbzzl/wwpnn                 2877389\n5 root/cmwrq/dtbzzl/wwpnn/lwqgsbg         1927095\n6 root/cmwrq/dtbzzl/wwpnn/lwqgsbg/dtbzzl   252091\n\n\nNow, we need to find the total size for all directories with size less than or equal to 100000. This total is found rather easily by filtering and summarizing using the corresponding functions from the dplyr package as shown below.\n\ndirsizes |>\n  filter(total <= 100000) |>\n  summarize(sum(total))\n\n# A tibble: 1 × 1\n  `sum(total)`\n         <dbl>\n1       919137"
  },
  {
    "objectID": "posts/2023-05-05_aoc_Day07_Filepaths/2023-05-05_aoc_Day07_Filepaths.html#part-two-select-a-directory-to-delete",
    "href": "posts/2023-05-05_aoc_Day07_Filepaths/2023-05-05_aoc_Day07_Filepaths.html#part-two-select-a-directory-to-delete",
    "title": "Advent of Code Day 7: Tracking file paths",
    "section": "Part Two: Select a directory to delete",
    "text": "Part Two: Select a directory to delete\nThe communication device file system has a total capacity of 70000000. In order to run an update, we need at least 30000000 free space on the device. As shown from Part One above, the root directory size is 42805968. We need to delete a directory with a size of at least 2805968 to free up enough space.\n\nmax_size <- 70000000\nfree_size <- 30000000\nunused_size <- max_size - dirsizes[[1, \"total\"]]  #max_size - root dir size\ndelete_size <- free_size - unused_size\n\ndelete_size\n\n[1] 2805968\n\n\nThe directory closest to this size but not under was found from the dirsizes tibble. The difference between the dir size and the delete size is determined for each directory, the differences below 0 are filtered out and the tibble is arranged so the least difference is at the top.\n\ndirsizes |>\n  mutate(distance = total - delete_size) |>\n  filter(distance >= 0) |>\n  arrange(distance) |>\n  slice(1)\n\n# A tibble: 1 × 3\n  dirpath                   total distance\n  <chr>                     <dbl>    <dbl>\n1 root/cmwrq/dtbzzl/wwpnn 2877389    71421\n\n\nThe question for Part Two was to enter the total size for this directory."
  },
  {
    "objectID": "posts/2023-05-05_aoc_Day07_Filepaths/2023-05-05_aoc_Day07_Filepaths.html#summary",
    "href": "posts/2023-05-05_aoc_Day07_Filepaths/2023-05-05_aoc_Day07_Filepaths.html#summary",
    "title": "Advent of Code Day 7: Tracking file paths",
    "section": "Summary",
    "text": "Summary\nThe Day 7 puzzle was solved easily once the means for tracking the filepaths was found. Storing all of the files in a tibble made it straightforward to find the answers using dplyr functions to group and summarize the directories appropriately.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-05-08\n pandoc   2.19.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.2.335 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P dplyr       * 1.0.10  2022-09-01 [?] CRAN (R 4.2.2)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.5.0   2022-12-02 [?] CRAN (R 4.2.2)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpqwuFJX/renv-library-27cc5ad71235\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/Temp/RtmpqwuFJX/renv-system-library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-05-17_aoc_Day08_Matrices/2023-05-17_aoc_Day08_Matrices.html",
    "href": "posts/2023-05-17_aoc_Day08_Matrices/2023-05-17_aoc_Day08_Matrices.html",
    "title": "Advent of Code Day 8: Matrices",
    "section": "",
    "text": "Assessing a grid of trees by performing positional calculations on a representative matrix"
  },
  {
    "objectID": "posts/2023-05-17_aoc_Day08_Matrices/2023-05-17_aoc_Day08_Matrices.html#introduction",
    "href": "posts/2023-05-17_aoc_Day08_Matrices/2023-05-17_aoc_Day08_Matrices.html#introduction",
    "title": "Advent of Code Day 8: Matrices",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 8. The elves are searching for an ideal location for their treehouse. The trees are planted in a grid and the elves have created a map of the tree heights assessed from their quadcopter."
  },
  {
    "objectID": "posts/2023-05-17_aoc_Day08_Matrices/2023-05-17_aoc_Day08_Matrices.html#loading-the-input-file",
    "href": "posts/2023-05-17_aoc_Day08_Matrices/2023-05-17_aoc_Day08_Matrices.html#loading-the-input-file",
    "title": "Advent of Code Day 8: Matrices",
    "section": "Loading the input file",
    "text": "Loading the input file\nThe input file consists of 99 rows each containing 99 numeric digits. Simply using readLines function from base R does a fine job of reading each row into a string however, ultimately, we would like to convert the data into a matrix. The next step is to convert each row (string) into a character vector using the str_split_1 function from the stringr package and at the same time convert the characters into numeric values. These numeric values are stored in one, long, numeric vector. The numeric vector is then shaped into a matrix with 99 rows and 99 columns as shown below.\n\nlibrary(stringr)\n\nfilepath &lt;- here::here(\"./posts/data/aoc/day08_input.txt\")\nraw_data &lt;- readLines(filepath)\n\n#convert each line into one long numeric vector\nraw_trees &lt;- numeric()\n\nfor (i in 1:length(raw_data)) {\n  raw_trees &lt;- c(raw_trees, as.numeric(str_split_1(raw_data[i], \"\")))\n}\n\n#convert numeric vector to a matrix\ntrees &lt;- matrix(raw_trees, nrow = 99, ncol = 99, byrow = TRUE)"
  },
  {
    "objectID": "posts/2023-05-17_aoc_Day08_Matrices/2023-05-17_aoc_Day08_Matrices.html#part-one-visibility",
    "href": "posts/2023-05-17_aoc_Day08_Matrices/2023-05-17_aoc_Day08_Matrices.html#part-one-visibility",
    "title": "Advent of Code Day 8: Matrices",
    "section": "Part One: Visibility",
    "text": "Part One: Visibility\nThe goal of part one is to determine the number of trees which are visible from outside the grid. All of the trees on the outside edge of the grid are visible. For the rest of the trees, the tree is visible if all the trees between it and the edge of the grid are shorter than it. Only the tree in the same row or column are considered when determining if the tree is visible. If the tree is visible from any direction (i.e. top, bottom, left or right) then it is considered visible. As an example, a smaller grid is shown below.\n30373\n25512\n65332\n33549\n35390\nAll of the trees on the outside edge are visible leaving just the inner nine trees to determine. The top-left 5 is visible from the top and left but not from the bottom or right due to trees of same height blocking visibility. The top-right 1 is invisible from all directions since all of the trees around it are taller.\nA check_visibility function has been created to check the visibility from each direction and return TRUE if it is visible from any direction.\n\n# assumes a 99 x99 tree matrix\ncheck_visibility &lt;- function(tree_mat, row_pos, col_pos){\n  if (row_pos == 1 | row_pos == 99 | col_pos == 1 | col_pos == 99) {\n    visibility &lt;- TRUE\n  } else {\n    tree_height &lt;- tree_mat[row_pos, col_pos]\n    north_vis &lt;- tree_height &gt; max(tree_mat[1:(row_pos-1), col_pos])\n    south_vis &lt;- tree_height &gt; max(tree_mat[(row_pos+1):99, col_pos])\n    west_vis &lt;- tree_height &gt; max(tree_mat[row_pos, 1:(col_pos-1)])\n    east_vis &lt;- tree_height &gt; max(tree_mat[row_pos, (col_pos+1):99])\n    \n    visibility &lt;- north_vis | south_vis | west_vis | east_vis\n  }\n  \n    return(visibility)\n}\n\nAll of the trees are then checked using for loops to cycle through all of the tree locations. The results are stored in a matrix which is initialized with -1 values.\n\nvis_mat &lt;- matrix(rep(-1, 99*99), nrow = 99, ncol = 99)\n\nfor (i in 1:99) {\n  for (j in 1:99) {\n    vis_mat[i, j] &lt;- check_visibility(trees, i, j)\n  }\n}\n\nThe total number of trees from outside the grid is determined by a sum of the resulting matrix since TRUE is captured as 1 and FALSE as 0.\n\nsum(vis_mat)\n\n[1] 1805"
  },
  {
    "objectID": "posts/2023-05-17_aoc_Day08_Matrices/2023-05-17_aoc_Day08_Matrices.html#part-two-tree-cover",
    "href": "posts/2023-05-17_aoc_Day08_Matrices/2023-05-17_aoc_Day08_Matrices.html#part-two-tree-cover",
    "title": "Advent of Code Day 8: Matrices",
    "section": "Part Two: Tree Cover",
    "text": "Part Two: Tree Cover\nThe elves are now looking for the best place to build their tree house and they like to see a lot of trees. The number of trees visible in each direction from the prospective tree house location is determined by the number of trees before reaching a tree of same height or taller as the tree at this location. Trees that are taller at more distant locations are not counted since the tree house has large eaves which prevent viewing them. Tree house locations on the perimeter will have see 0 trees in at least one direction.\nThe total scenic score is obtained by multiplying together the number of trees counted in each direction. For the example given below:\n30373\n25512\n65332\n33549\n35390\nThe 5 in the middle of the fourth row can see 2 trees to the north, 2 trees to the west, 1 tree to the south and 2 trees to the east. So, the total scenic score is 2 * 2 * 1 * 2 which equals 8.\nFour functions are created to count the number of trees viewed from each direction. These functions could be perhaps more efficiently put into a single function but I preferred the simplicity over brevity in this case.\n\nlook_north &lt;- function(tree_mat, row_pos, col_pos) {\n  \n  num_trees &lt;- 0\n  if (row_pos == 1) {\n    #num_trees &lt;- 0\n  } else {\n    tree_height &lt;- tree_mat[row_pos, col_pos]\n    for (i in (row_pos-1):1) {\n      if (tree_mat[i, col_pos] &gt;= tree_height){\n        num_trees &lt;- num_trees + 1\n        break\n      } else {\n        num_trees &lt;- num_trees + 1\n      }\n    }\n  }\n  return(num_trees)\n}\n\n\nlook_south &lt;- function(tree_mat, row_pos, col_pos) {\n  \n  num_trees &lt;- 0\n  if (row_pos == 99) {\n    #num_trees &lt;- 0\n  } else {\n    tree_height &lt;- tree_mat[row_pos, col_pos]\n    for (i in (row_pos+1):99) {\n      if (tree_mat[i, col_pos] &gt;= tree_height){\n        num_trees &lt;- num_trees + 1\n        break\n      } else {\n        num_trees &lt;- num_trees + 1\n      }\n    }\n  }\n  return(num_trees)\n}\n\n\nlook_west &lt;- function(tree_mat, row_pos, col_pos) {\n  \n  num_trees &lt;- 0\n  if (col_pos == 99) {\n    #num_trees &lt;- 0\n  } else {\n    tree_height &lt;- tree_mat[row_pos, col_pos]\n    for (i in (col_pos+1):99) {\n      if (tree_mat[row_pos, i] &gt;= tree_height){\n        num_trees &lt;- num_trees + 1\n        break\n      } else {\n        num_trees &lt;- num_trees + 1\n      }\n    }\n  }\n  return(num_trees)\n}\n\n\nlook_east &lt;- function(tree_mat, row_pos, col_pos) {\n  \n  num_trees &lt;- 0\n  if (col_pos == 1) {\n    #num_trees &lt;- 0\n  } else {\n    tree_height &lt;- tree_mat[row_pos, col_pos]\n    for (i in (col_pos-1):1) {\n      if (tree_mat[row_pos, i] &gt;= tree_height){\n        num_trees &lt;- num_trees + 1\n        break\n      } else {\n        num_trees &lt;- num_trees + 1\n      }\n    }\n  }\n  return(num_trees)\n}\n\nEach function returns the number of trees viewed. The scenic score is calculated using the count_trees function which utilizes the four functions previously mentioned.\n\ncount_trees &lt;- function(tree_mat, row_pos, col_pos){\n  scenic_score &lt;- \n    look_north(trees, row_pos, col_pos) *\n    look_south(trees, row_pos, col_pos) *\n    look_west(trees, row_pos, col_pos) *\n    look_east(trees, row_pos, col_pos)\n  \n  return(scenic_score)\n}\n\nThe scenic scores are determined by cycling through all of the possible tree house locations. A matrix initialized with -1 values is used again to store all of the scenic scores.\n\nscenic_mat &lt;- matrix(rep(-1, 99*99), nrow = 99, ncol = 99)\n\nfor (i in 1:99) {\n  for (j in 1:99) {\n    scenic_mat[i, j] &lt;- count_trees(trees, i, j)\n  }\n}\n\nThe highest scenic score is obtained by searching for the maximum of the matrix.\n\nmax(scenic_mat)\n\n[1] 444528\n\n\nAs a bonus, the location for the maximum scenic score can be obtained as shown below.\n\nwhich(scenic_mat == max(scenic_mat), arr.ind = TRUE)\n\n     row col\n[1,]  78  43"
  },
  {
    "objectID": "posts/2023-05-17_aoc_Day08_Matrices/2023-05-17_aoc_Day08_Matrices.html#summary",
    "href": "posts/2023-05-17_aoc_Day08_Matrices/2023-05-17_aoc_Day08_Matrices.html#summary",
    "title": "Advent of Code Day 8: Matrices",
    "section": "Summary",
    "text": "Summary\nLoading the data into an R matrix enabled straightforward calculations to answer the Day 8 puzzle. Breaking the calculation into simple functions resulted in very readable and understandable code.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-06-06\n pandoc   2.19.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.2.335 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.5.0   2022-12-02 [?] CRAN (R 4.2.2)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpqwfTxY/renv-library-1c4079f8156e\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/Temp/RtmpqwfTxY/renv-system-library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-07-10_aoc_Day09_Tracking/2023-07-10_aoc_Day09_Tracking.html",
    "href": "posts/2023-07-10_aoc_Day09_Tracking/2023-07-10_aoc_Day09_Tracking.html",
    "title": "Advent of Code Day 9: Tracking",
    "section": "",
    "text": "Creating a map to track a “head” knot as it moves according to a series of instructions"
  },
  {
    "objectID": "posts/2023-07-10_aoc_Day09_Tracking/2023-07-10_aoc_Day09_Tracking.html#introduction",
    "href": "posts/2023-07-10_aoc_Day09_Tracking/2023-07-10_aoc_Day09_Tracking.html#introduction",
    "title": "Advent of Code Day 9: Tracking",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 9. You need to cross a rope bridge and figure out where to step. There are knots indicating the head and tail of the rope. The head and tail knots must be touching (adjacent) at all times. Based on the movement of the head knot you need to figure out all of the positions occupied by the tail knot."
  },
  {
    "objectID": "posts/2023-07-10_aoc_Day09_Tracking/2023-07-10_aoc_Day09_Tracking.html#loading-the-input-file",
    "href": "posts/2023-07-10_aoc_Day09_Tracking/2023-07-10_aoc_Day09_Tracking.html#loading-the-input-file",
    "title": "Advent of Code Day 9: Tracking",
    "section": "Loading the input file",
    "text": "Loading the input file\nThe input file contains instructions for the movement of the head knot. Each line consists of a letter followed by a space and a number. The letters are either U, D, L or R corresponding to up, down, left and right. The number is how many steps in that direction the head knot is supposed to move. Another case where readLines works perfectly fine to read the input file into a character vector.\n\nlibrary(stringr)\n\nfilepath &lt;- here::here(\"./posts/data/aoc/day09_input.txt\")\ncommands &lt;- readLines(filepath)"
  },
  {
    "objectID": "posts/2023-07-10_aoc_Day09_Tracking/2023-07-10_aoc_Day09_Tracking.html#part-one-tracking-the-head-and-tail-knots",
    "href": "posts/2023-07-10_aoc_Day09_Tracking/2023-07-10_aoc_Day09_Tracking.html#part-one-tracking-the-head-and-tail-knots",
    "title": "Advent of Code Day 9: Tracking",
    "section": "Part One: Tracking the head and tail knots",
    "text": "Part One: Tracking the head and tail knots\nThe first goal is to track the movement of the head knot and update the position of the tail knot so that it stays adjacent to the head. It is rather involved to explain all of the scenarios. In general, the tail will move whenever the head is more than 2 steps away in any one direction. In some cases, a diagonal move is required when it is more than 2 steps away in one direction and another 1 step away in a different direction. I started by figuring out the minimum and maximum movement in both the vertical and horizontal directions in order to create a map (matrix) of the correct size to track the movements. These min and max values are determined by the following code:\n\npos_x &lt;- 0\npos_y &lt;- 0\nmax_x &lt;- 0\nmin_x &lt;- 0\nmax_y &lt;- 0\nmin_y &lt;- 0\n\nfor (command in commands) {\n  move &lt;- unlist(str_split(command, \" \"))\n  if (move[1] == \"D\") {\n    pos_y &lt;- pos_y + as.numeric(move[2])\n    if (pos_y &gt; max_y) max_y &lt;- pos_y \n  } else if (move[1] == \"U\") {\n    pos_y &lt;- pos_y - as.numeric(move[2])\n    if (pos_y &lt; min_y) min_y &lt;- pos_y\n  } else if (move[1] == \"R\") {\n    pos_x &lt;- pos_x + as.numeric(move[2])\n    if (pos_x &gt; max_x) max_x &lt;- pos_x\n  } else if (move[1] == \"L\") {\n    pos_x &lt;- pos_x - as.numeric(move[2])\n    if (pos_x &lt; min_x) min_x &lt;- pos_x\n  } else {}\n}\n\nprint(paste0(\"min_x = \", min_x, \"   max_x = \", max_x))\n\n[1] \"min_x = -262   max_x = 37\"\n\nprint(paste0(\"min_y = \", min_y, \"   min_y = \", max_y))\n\n[1] \"min_y = -145   min_y = 55\"\n\n\nAs can be seen from the min and max values, there is more movement to the left and up (i.e. “negative” directions) than to the right and down (i.e. “positive” directions). We use these values then to pre-allocate a matrix that will serve as a map for the tail locations. We initiate all of the matrix values to zero and will change to a one when the tail knot occupies that location.\n\nnum_cols &lt;- max_x - min_x + 1\nnum_rows &lt;- max_y - min_y + 1\n\nrope_loc &lt;- matrix(rep(0, num_rows * num_cols), nrow = num_rows, ncol = num_cols)\n\nA function called move_tail is created to update the tail knot position relative to the head knot movement. The current head and tail locations are passed to the function which then checks if the tail knot is 2 or more steps away from the tail in any direction. If so, the tail location is moved one step in that direction also checking whether a diagonal move is needed. The function then returns the new tail knot location.\n\nmove_tail &lt;- function(head_x, head_y, tail_x, tail_y) {\n  if ((head_x - tail_x) &gt; 1) {\n    tail_x &lt;- tail_x + 1\n    if ((head_y - tail_y) &gt; 0) tail_y &lt;- tail_y + 1\n    if ((head_y - tail_y) &lt; 0) tail_y &lt;- tail_y - 1\n  } else if ((head_x - tail_x) &lt; -1) {\n    tail_x &lt;- tail_x - 1\n    if ((head_y - tail_y) &gt; 0) tail_y &lt;- tail_y + 1\n    if ((head_y - tail_y) &lt; 0) tail_y &lt;- tail_y - 1\n  } else if ((head_y - tail_y) &gt; 1) {\n    tail_y &lt;- tail_y + 1\n    if ((head_x - tail_x) &gt; 0) tail_x &lt;- tail_x + 1\n    if ((head_x - tail_x) &lt; 0) tail_x &lt;- tail_x - 1\n  } else if ((head_y - tail_y) &lt; -1) {\n    tail_y &lt;- tail_y - 1\n    if ((head_x - tail_x) &gt; 0) tail_x &lt;- tail_x + 1\n    if ((head_x - tail_x) &lt; 0) tail_x &lt;- tail_x - 1\n  } else {}\n  return(c(tail_x, tail_y))\n}\n\nNext, we initiate the starting position for the overlapping head and tail knots within the rope_loc matrix and assign 1 to that position. A 1 value will indicate a position that the tail knot occupied.\n\n#Starting point to stay within the map\nhpos_x &lt;- -min_x + 1\nhpos_y &lt;- -min_y + 1\ntpos_x &lt;- -min_x + 1\ntpos_y &lt;- -min_y + 1\n\nrope_loc[tpos_y, tpos_x] &lt;- 1\n\nNow, we iterate through the commands provided to move the head knot. The order of actions is to 1) move the head knot location, 2) find the updated tail knot location using the move_tail function, 3) save the updated tail knot location and 4) set the matrix location for the tail knot to 1.\n\nfor (command in commands) {\n  move &lt;- unlist(str_split(command, \" \"))\n  if (move[1] == \"D\") {\n    for (i in 1:(as.numeric(move[2]))) {\n      hpos_y &lt;- hpos_y + 1\n      tail_pos &lt;- move_tail(hpos_x, hpos_y, tpos_x, tpos_y)\n      tpos_x &lt;- tail_pos[1]\n      tpos_y &lt;- tail_pos[2]\n      rope_loc [tpos_y, tpos_x] &lt;- 1\n    } \n  } else if (move[1] == \"U\") {\n    for (i in 1:(as.numeric(move[2]))) {\n      hpos_y &lt;- hpos_y - 1\n      tail_pos &lt;- move_tail(hpos_x, hpos_y, tpos_x, tpos_y)\n      tpos_x &lt;- tail_pos[1]\n      tpos_y &lt;- tail_pos[2]\n      rope_loc [tpos_y, tpos_x] &lt;- 1\n    }\n  } else if (move[1] == \"R\") {\n    for (i in 1:(as.numeric(move[2]))) {\n      hpos_x &lt;- hpos_x + 1\n      tail_pos &lt;- move_tail(hpos_x, hpos_y, tpos_x, tpos_y)\n      tpos_x &lt;- tail_pos[1]\n      tpos_y &lt;- tail_pos[2]\n      rope_loc [tpos_y, tpos_x] &lt;- 1\n    }\n  } else if (move[1] == \"L\") {\n    for (i in 1:(as.numeric(move[2]))) {\n      hpos_x &lt;- hpos_x - 1\n      tail_pos &lt;- move_tail(hpos_x, hpos_y, tpos_x, tpos_y)\n      tpos_x &lt;- tail_pos[1]\n      tpos_y &lt;- tail_pos[2]\n      rope_loc [tpos_y, tpos_x] &lt;- 1\n    }\n  } else {}\n}\n\nThe good part about setting up the mapping of the tail knot location using a matrix of ones is that it is easy to answer the part one question: “How many places does the tail knot visit at least once?” To calculate the answer, you simply need to find the sum of the matrix.\n\nsum(rope_loc)\n\n[1] 5930"
  },
  {
    "objectID": "posts/2023-07-10_aoc_Day09_Tracking/2023-07-10_aoc_Day09_Tracking.html#part-two-rope-with-10-knots",
    "href": "posts/2023-07-10_aoc_Day09_Tracking/2023-07-10_aoc_Day09_Tracking.html#part-two-rope-with-10-knots",
    "title": "Advent of Code Day 9: Tracking",
    "section": "Part Two: Rope with 10 knots",
    "text": "Part Two: Rope with 10 knots\nIn Part Two, the rope now has 10 knots instead of just two. In this case, the position for each knot is updated based on the movement of the knot before it. As for Part One, the commands provided in the input file instruct the movement of the head knot only and we are only interested in tracking the position of the tail knot (i.e. knot 10). Several parts can be reused from Part One. The matrix size is the same but it needs to be initialized back to zeros.\n\n#Re-initialize the map\nnum_cols &lt;- max_x - min_x + 1\nnum_rows &lt;- max_y - min_y + 1 \n\nrope_loc &lt;- matrix(rep(0, num_rows * num_cols), nrow = num_rows, ncol = num_cols)\n\nThe position of all 10 knots is now stored in a list and all knots are initialized to the same starting position. The starting position is also set to 1 in rope_loc.\n\ntpos &lt;- replicate(10, list(c(-min_x+1, -min_y+1)))\n\nrope_loc[tpos[[10]][2], tpos[[10]][1]] &lt;- 1\n\nThe move_tail function does not need to be changed. We do need to cycle through all of the knots with each step using a for loop and the move_tail function. Only the location of knot 10 is used to update the map by setting its location to a value of 1 in rope_loc. The script below is used to cycle through the commands and update the tail knot location.\n\nfor (command in commands) {\n  move &lt;- unlist(str_split(command, \" \"))\n  if (move[1] == \"D\") {\n    for (i in 1:(as.numeric(move[2]))) {\n      tpos[[1]][2] &lt;- tpos[[1]][2] + 1\n      for (j in 2:10) {\n        tail_pos &lt;- move_tail(tpos[[j-1]][1], tpos[[j-1]][2], tpos[[j]][1], tpos[[j]][2])\n        tpos[[j]][1] &lt;- tail_pos[1]\n        tpos[[j]][2] &lt;- tail_pos[2]\n      }\n      rope_loc[tpos[[10]][2], tpos[[10]][1]] &lt;- 1\n    } \n  } else if (move[1] == \"U\") {\n    for (i in 1:(as.numeric(move[2]))) {\n      tpos[[1]][2] &lt;- tpos[[1]][2] - 1\n      for (j in 2:10) {\n        tail_pos &lt;- move_tail(tpos[[j-1]][1], tpos[[j-1]][2], tpos[[j]][1], tpos[[j]][2])\n        tpos[[j]][1] &lt;- tail_pos[1]\n        tpos[[j]][2] &lt;- tail_pos[2]\n      }\n      rope_loc[tpos[[10]][2], tpos[[10]][1]] &lt;- 1\n    }\n  } else if (move[1] == \"R\") {\n    for (i in 1:(as.numeric(move[2]))) {\n      tpos[[1]][1] &lt;- tpos[[1]][1] + 1\n      for (j in 2:10) {\n        tail_pos &lt;- move_tail(tpos[[j-1]][1], tpos[[j-1]][2], tpos[[j]][1], tpos[[j]][2])\n        tpos[[j]][1] &lt;- tail_pos[1]\n        tpos[[j]][2] &lt;- tail_pos[2]\n      }\n      rope_loc[tpos[[10]][2], tpos[[10]][1]] &lt;- 1\n    }\n  } else if (move[1] == \"L\") {\n    for (i in 1:(as.numeric(move[2]))) {\n      tpos[[1]][1] &lt;- tpos[[1]][1] - 1\n      for (j in 2:10) {\n        tail_pos &lt;- move_tail(tpos[[j-1]][1], tpos[[j-1]][2], tpos[[j]][1], tpos[[j]][2])\n        tpos[[j]][1] &lt;- tail_pos[1]\n        tpos[[j]][2] &lt;- tail_pos[2]\n      }\n      rope_loc[tpos[[10]][2], tpos[[10]][1]] &lt;- 1\n    }\n  } else {}\n}\n\nIn the same way as Part One, we only need to find the sum of rope_loc in order to calculate the number of positions the tail knot has occupied at least once.\n\nsum(rope_loc)\n\n[1] 2443"
  },
  {
    "objectID": "posts/2023-07-10_aoc_Day09_Tracking/2023-07-10_aoc_Day09_Tracking.html#summary",
    "href": "posts/2023-07-10_aoc_Day09_Tracking/2023-07-10_aoc_Day09_Tracking.html#summary",
    "title": "Advent of Code Day 9: Tracking",
    "section": "Summary",
    "text": "Summary\nThis problem was easy to solve once the logic for updating the tail knot movement was coded. It was important to identify what each direction meant as far as the matrix (“map”) was concerned. In this case, I decided an “UP” move was actually moving down in row number which was a bit tricky to wrap my head around. Finding the size of the matrix in advance was also used but perhaps there is a way to automatically adjust the size as you read more commands while maintaining the efficiency and readability.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-07-18\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.353 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.5.0   2022-12-02 [?] CRAN (R 4.2.2)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/Rtmp2Zotb7/renv-use-libpath-12e0aa541f0\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/R/cache/R/renv/sandbox/R-4.2/x86_64-w64-mingw32/30182023\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-07-18_aoc_Day10_HiddenMessage/2023-07-18_aoc_Day10_HiddenMessage.html",
    "href": "posts/2023-07-18_aoc_Day10_HiddenMessage/2023-07-18_aoc_Day10_HiddenMessage.html",
    "title": "Advent of Code Day 10: Hidden Message",
    "section": "",
    "text": "Following a series of instructions to display a hidden series of letters"
  },
  {
    "objectID": "posts/2023-07-18_aoc_Day10_HiddenMessage/2023-07-18_aoc_Day10_HiddenMessage.html#introduction",
    "href": "posts/2023-07-18_aoc_Day10_HiddenMessage/2023-07-18_aoc_Day10_HiddenMessage.html#introduction",
    "title": "Advent of Code Day 10: Hidden Message",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 10. The communication device fell into the river and got damaged. The device consists of a CPU which processes a series of instructions according to a fixed cycle rate. The instructions are used to set a single register “X”. A CRT display is controlled by the CPU to light certain pixels depending upon the current cycle and the register X."
  },
  {
    "objectID": "posts/2023-07-18_aoc_Day10_HiddenMessage/2023-07-18_aoc_Day10_HiddenMessage.html#loading-the-input-file",
    "href": "posts/2023-07-18_aoc_Day10_HiddenMessage/2023-07-18_aoc_Day10_HiddenMessage.html#loading-the-input-file",
    "title": "Advent of Code Day 10: Hidden Message",
    "section": "Loading the input file",
    "text": "Loading the input file\nThe input file contains instructions for the CPU to set the register X. There are only two types of instructions, noop and addx. The “noop” instruction takes one cycle to execute and has no other effect. The “addx V” instruction takes two cycle to execute and adds the value “V” to register X at the end of the second cycle.\n\nlibrary(stringr)\nlibrary(purrr)\n\nWarning: package 'purrr' was built under R version 4.2.3\n\nfilepath &lt;- here::here(\"./posts/data/aoc/day10_input.txt\")\ncommands &lt;- readLines(filepath)\n\nhead(commands)\n\n[1] \"noop\"   \"noop\"   \"noop\"   \"addx 5\" \"noop\"   \"addx 1\""
  },
  {
    "objectID": "posts/2023-07-18_aoc_Day10_HiddenMessage/2023-07-18_aoc_Day10_HiddenMessage.html#part-one-cpu-instructions",
    "href": "posts/2023-07-18_aoc_Day10_HiddenMessage/2023-07-18_aoc_Day10_HiddenMessage.html#part-one-cpu-instructions",
    "title": "Advent of Code Day 10: Hidden Message",
    "section": "Part One: CPU Instructions",
    "text": "Part One: CPU Instructions\nThe instructions from the input file are executed as described above. A key point is that the update to the X register value does not occur until after the end of the cycle. The question for Part One is to find the signal strength during each cycle and record the signal strength for the 20th cycle and every 40 cycles after that (i.e. 60th, 100th, 140th, 180th and 220th cycles). Signal strength is calculated by multiplying the current cycle by the X register.\n\n#X Register\nreg_X &lt;- 1\n#Cycle\ni &lt;- 1\n#Signal Strength\nsig_str &lt;- list()\n\nfor (command in commands) {\n  move &lt;- unlist(str_split(command, \" \"))\n  if (move[1] == \"noop\") {\n    #sig_str &lt;- c(sig_str, list(c(command, i, reg_X, i * reg_X)))\n    sig_str &lt;- c(sig_str, list(i * reg_X))\n    i &lt;- i + 1\n\n  } else if (move[1] == \"addx\") {\n    #sig_str &lt;- c(sig_str, list(c(command, i, reg_X, i * reg_X)))\n    sig_str &lt;- c(sig_str, list(i * reg_X))    \n    i &lt;- i + 1\n\n    \n    #sig_str &lt;- c(sig_str, list(c(command, i, reg_X, i * reg_X)))\n    sig_str &lt;- c(sig_str, list(i * reg_X))\n    reg_X &lt;- reg_X + as.numeric(move[2])\n    i &lt;- i + 1\n\n  } else {}\n}\n\nThe walk function from the purrr package can be used to print the signal strength for each of the cycles of interest.\n\ncycle &lt;- c(20, 60, 100, 140, 180, 220)\n\ncycle |&gt; walk(function(cycle) print(paste(\"Signal Strength for Cycle\", cycle, \"is\", sig_str[[cycle]])))\n\n[1] \"Signal Strength for Cycle 20 is 440\"\n[1] \"Signal Strength for Cycle 60 is 1260\"\n[1] \"Signal Strength for Cycle 100 is 1600\"\n[1] \"Signal Strength for Cycle 140 is 2940\"\n[1] \"Signal Strength for Cycle 180 is 3780\"\n[1] \"Signal Strength for Cycle 220 is 3740\"\n\n\nThe answer to Part One is to add the signal strengths for the cycles of interest.\n\nsum(unlist(sig_str[cycle]))\n\n[1] 13760"
  },
  {
    "objectID": "posts/2023-07-18_aoc_Day10_HiddenMessage/2023-07-18_aoc_Day10_HiddenMessage.html#part-two-crt-display",
    "href": "posts/2023-07-18_aoc_Day10_HiddenMessage/2023-07-18_aoc_Day10_HiddenMessage.html#part-two-crt-display",
    "title": "Advent of Code Day 10: Hidden Message",
    "section": "Part Two: CRT Display",
    "text": "Part Two: CRT Display\nIt has become evident that the X Register controls the position of a sprite on a CRT screen. The sprite is 3 pixels wide and the X register sets the middle position of the sprite. The CRT screen is 40 pixels wide and has 6 rows. The left most pixel in each row is in position 0 and the right most pixel is at position 39. A pixel is lit with each clock cycle of the CPU. The pixel is light (#) if the sprite overlaps with the current clock cycle and dark (.) if the sprite does not overlap.\nThe logic to determine whether the sprite is overlapping with the current cycle requires two operations: 1) subtract 1 from the current cycle since cycle 1 would correspond with sprite position 0 and 2) perform modulo 40 operation since each row of the CRT contains the same positioning relative to the sprite.\n\nreg_X &lt;- 1\ni &lt;- 1\nsprite &lt;- (reg_X-1):(reg_X+1)\nsig_str &lt;- list()\ncrt &lt;- character()\n\nfor (command in commands) {\n  move &lt;- unlist(str_split(command, \" \"))\n  if (move[1] == \"noop\") {\n    #sig_str &lt;- c(sig_str, list(c(command, i, reg_X, i * reg_X)))\n    crt &lt;- c(crt, ifelse(((i-1) %% 40) %in% sprite, \"#\", \".\"))\n    sig_str &lt;- c(sig_str, list(i * reg_X))\n    i &lt;- i + 1\n\n  } else if (move[1] == \"addx\") {\n    #sig_str &lt;- c(sig_str, list(c(command, i, reg_X, i * reg_X)))\n    crt &lt;- c(crt, ifelse(((i-1) %% 40) %in% sprite, \"#\", \".\"))\n    sig_str &lt;- c(sig_str, list(i * reg_X))    \n    i &lt;- i + 1\n\n    #sig_str &lt;- c(sig_str, list(c(command, i, reg_X, i * reg_X)))\n    crt &lt;- c(crt, ifelse(((i-1) %% 40) %in% sprite, \"#\", \".\"))\n    sig_str &lt;- c(sig_str, list(i * reg_X))\n    reg_X &lt;- reg_X + as.numeric(move[2])\n    sprite &lt;- (reg_X-1):(reg_X+1)\n    i &lt;- i + 1\n\n  } else {}\n}\n\nThe CRT can then be viewed by converting the character array to a matrix with the size dimensions of the screen. A simple print command without the quotes allows for the letters to be displayed.\n\nview_crt &lt;- matrix(crt, nrow = 6, ncol = 40, byrow = TRUE)\nprint(view_crt, quote = FALSE)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n[1,] #    #    #    .    .    #    #    #    #    .     #     .     .     #    \n[2,] #    .    .    #    .    #    .    .    .    .     #     .     #     .    \n[3,] #    .    .    #    .    #    #    #    .    .     #     #     .     .    \n[4,] #    #    #    .    .    #    .    .    .    .     #     .     #     .    \n[5,] #    .    #    .    .    #    .    .    .    .     #     .     #     .    \n[6,] #    .    .    #    .    #    .    .    .    .     #     .     .     #    \n     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n[1,] .     #     #     #     #     .     .     #     #     .     .     #    \n[2,] .     .     .     .     #     .     #     .     .     #     .     #    \n[3,] .     .     .     #     .     .     #     .     .     .     .     #    \n[4,] .     .     #     .     .     .     #     .     .     .     .     #    \n[5,] .     #     .     .     .     .     #     .     .     #     .     #    \n[6,] .     #     #     #     #     .     .     #     #     .     .     #    \n     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n[1,] #     #     .     .     #     #     #     #     .     #     #     #    \n[2,] .     .     #     .     #     .     .     .     .     #     .     .    \n[3,] .     .     #     .     #     #     #     .     .     #     #     #    \n[4,] #     #     .     .     #     .     .     .     .     #     .     .    \n[5,] .     .     .     .     #     .     .     .     .     #     .     .    \n[6,] .     .     .     .     #     #     #     #     .     #     .     .    \n     [,39] [,40]\n[1,] #     .    \n[2,] .     .    \n[3,] .     .    \n[4,] .     .    \n[5,] .     .    \n[6,] .     .    \n\n\nThe eight, hidden letters are revealed to be: “R F K Z C P E F”."
  },
  {
    "objectID": "posts/2023-07-18_aoc_Day10_HiddenMessage/2023-07-18_aoc_Day10_HiddenMessage.html#summary",
    "href": "posts/2023-07-18_aoc_Day10_HiddenMessage/2023-07-18_aoc_Day10_HiddenMessage.html#summary",
    "title": "Advent of Code Day 10: Hidden Message",
    "section": "Summary",
    "text": "Summary\nThe trickiest part of this day’s problem was figuring out whether the X register was updated during the cycle or after the cycle. Using the test data and solution, I was able to determine that it was not updated during the cycle. The other part that was a bit annoying was the CRT where the first pixel was at position 0. Must have been a Python user who wrote this problem. Once the problem was understood, the solution was straightforward to implement in R.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-07-21\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.353 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P purrr       * 1.0.1   2023-01-10 [?] CRAN (R 4.2.3)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.5.0   2022-12-02 [?] CRAN (R 4.2.2)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpIBYgSQ/renv-use-libpath-10c43791652\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/R/cache/R/renv/sandbox/R-4.2/x86_64-w64-mingw32/30182023\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-07-25_aoc_Day11_MonkeyBusiness/2023-07-25_aoc_Day11_MonkeyBusiness.html",
    "href": "posts/2023-07-25_aoc_Day11_MonkeyBusiness/2023-07-25_aoc_Day11_MonkeyBusiness.html",
    "title": "Advent of Code Day 11: Monkey Business",
    "section": "",
    "text": "Monkeys have taken my items and we need to determine how much monkey business is going on."
  },
  {
    "objectID": "posts/2023-07-25_aoc_Day11_MonkeyBusiness/2023-07-25_aoc_Day11_MonkeyBusiness.html#introduction",
    "href": "posts/2023-07-25_aoc_Day11_MonkeyBusiness/2023-07-25_aoc_Day11_MonkeyBusiness.html#introduction",
    "title": "Advent of Code Day 11: Monkey Business",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 11. Apparently, monkeys have taken my items and are passing them around between the monkeys. The items are labeled with a value for how worried I am about each items. This “worry level” changes as a monkey inspects the item by an Operation. The monkey then uses a Test to decide what to do with the item based on the new worry level. If the result of the Test is true, the monkey throws it one monkey and, if the Test result is false, the monkey throws it to a different monkey. A round of “monkey business” requires cycling through each monkey and each item. We need to find the status of the items after a specific number of rounds."
  },
  {
    "objectID": "posts/2023-07-25_aoc_Day11_MonkeyBusiness/2023-07-25_aoc_Day11_MonkeyBusiness.html#loading-the-input-file",
    "href": "posts/2023-07-25_aoc_Day11_MonkeyBusiness/2023-07-25_aoc_Day11_MonkeyBusiness.html#loading-the-input-file",
    "title": "Advent of Code Day 11: Monkey Business",
    "section": "Loading the input file",
    "text": "Loading the input file\nThe input file is in a custom format and certainly not a tidy format. I used the read_lines function from the readr package to read the lines into an array of strings and taking advantage of the option to not read in the blank lines. The str_trim function from the stringr package is used to trim the leading and trailing whitespace from the character strings.\n\nlibrary(readr)\nlibrary(stringr)\n\n\nfilepath &lt;- here::here(\"./posts/data/aoc/day11_input.txt\")\n\ncommands &lt;- read_lines(filepath, skip_empty_rows = TRUE)\ncommands &lt;- str_trim(commands)\ncommands\n\n [1] \"Monkey 0:\"                                     \n [2] \"Starting items: 77, 69, 76, 77, 50, 58\"        \n [3] \"Operation: new = old * 11\"                     \n [4] \"Test: divisible by 5\"                          \n [5] \"If true: throw to monkey 1\"                    \n [6] \"If false: throw to monkey 5\"                   \n [7] \"Monkey 1:\"                                     \n [8] \"Starting items: 75, 70, 82, 83, 96, 64, 62\"    \n [9] \"Operation: new = old + 8\"                      \n[10] \"Test: divisible by 17\"                         \n[11] \"If true: throw to monkey 5\"                    \n[12] \"If false: throw to monkey 6\"                   \n[13] \"Monkey 2:\"                                     \n[14] \"Starting items: 53\"                            \n[15] \"Operation: new = old * 3\"                      \n[16] \"Test: divisible by 2\"                          \n[17] \"If true: throw to monkey 0\"                    \n[18] \"If false: throw to monkey 7\"                   \n[19] \"Monkey 3:\"                                     \n[20] \"Starting items: 85, 64, 93, 64, 99\"            \n[21] \"Operation: new = old + 4\"                      \n[22] \"Test: divisible by 7\"                          \n[23] \"If true: throw to monkey 7\"                    \n[24] \"If false: throw to monkey 2\"                   \n[25] \"Monkey 4:\"                                     \n[26] \"Starting items: 61, 92, 71\"                    \n[27] \"Operation: new = old * old\"                    \n[28] \"Test: divisible by 3\"                          \n[29] \"If true: throw to monkey 2\"                    \n[30] \"If false: throw to monkey 3\"                   \n[31] \"Monkey 5:\"                                     \n[32] \"Starting items: 79, 73, 50, 90\"                \n[33] \"Operation: new = old + 2\"                      \n[34] \"Test: divisible by 11\"                         \n[35] \"If true: throw to monkey 4\"                    \n[36] \"If false: throw to monkey 6\"                   \n[37] \"Monkey 6:\"                                     \n[38] \"Starting items: 50, 89\"                        \n[39] \"Operation: new = old + 3\"                      \n[40] \"Test: divisible by 13\"                         \n[41] \"If true: throw to monkey 4\"                    \n[42] \"If false: throw to monkey 3\"                   \n[43] \"Monkey 7:\"                                     \n[44] \"Starting items: 83, 56, 64, 58, 93, 91, 56, 65\"\n[45] \"Operation: new = old + 5\"                      \n[46] \"Test: divisible by 19\"                         \n[47] \"If true: throw to monkey 1\"                    \n[48] \"If false: throw to monkey 0\"                   \n\n\nI decided to parse the information needed to determine the monkey business into a list called monkey_ops which contains a sub-list for each monkey. The needed information was the monkey’s name, items, operation, test and monkey to throw to if test is true and if test is false. The operation is divided into the operator (e.g. “+”, “*”) and operator value. One of the operations is captured as “old * old” which was substituted with the “^” operator and an operator value of 2.\n\ncurrent_monkey &lt;- 0\nmonkey_ops &lt;- list()\n\n#temp &lt;- unlist(str_split(commands[4], \":\"))\n\nfor (i in 1:length(commands)) {\n\n  temp &lt;- unlist(str_split(commands[i], \":\"))\n  \n  if (str_detect(temp[1],\"^Monkey\")) {\n    current_monkey &lt;- as.numeric(unlist(str_split(temp[1], \" \"))[2]) + 1\n    monkey_ops[[current_monkey]] &lt;- list(name = current_monkey-1, inspections = 0)\n  }\n  \n  if (str_detect(temp[1], \"^Starting\")) {\n    current_items &lt;- as.numeric(unlist(str_split(temp[2], \", \")))\n    monkey_ops[[current_monkey]] &lt;- c(monkey_ops[[current_monkey]], items = list(current_items))\n  }\n  \n  if (str_detect(temp[1], \"^Operation\")) {\n    \n    if (unlist(str_split(temp[2], \" \"))[6] == \"old\") {\n      current_opaction &lt;- \"^\"\n      current_opvalue &lt;- 2\n    } else {\n      current_opaction &lt;- unlist(str_split(temp[2], \" \"))[5]\n      current_opvalue &lt;- as.numeric(unlist(str_split(temp[2], \" \"))[6])\n    }\n    \n    monkey_ops[[current_monkey]] &lt;- c(monkey_ops[[current_monkey]], opaction = current_opaction, opvalue = current_opvalue)\n  }\n  \n  if (str_detect(temp[1],\"^Test\")) {\n    current_test &lt;- as.numeric(unlist(str_split(temp[2], \" \"))[4])\n    monkey_ops[[current_monkey]] &lt;- c(monkey_ops[[current_monkey]], test = current_test)\n  }\n  \n  if (str_detect(temp[1], \"^If true\")) {\n    current_iftrue &lt;- as.numeric(unlist(str_split(temp[2], \" \"))[5])\n    monkey_ops[[current_monkey]] &lt;- c(monkey_ops[[current_monkey]], iftrue = current_iftrue)\n  }\n  \n  if (str_detect(temp[1], \"^If false\")) {\n    current_iffalse &lt;- as.numeric(unlist(str_split(temp[2], \" \"))[5])\n    monkey_ops[[current_monkey]] &lt;- c(monkey_ops[[current_monkey]], iffalse = current_iffalse)\n  }\n}\n\nmonkey_ops[[1]]\n\n$name\n[1] 0\n\n$inspections\n[1] 0\n\n$items\n[1] 77 69 76 77 50 58\n\n$opaction\n[1] \"*\"\n\n$opvalue\n[1] 11\n\n$test\n[1] 5\n\n$iftrue\n[1] 1\n\n$iffalse\n[1] 5"
  },
  {
    "objectID": "posts/2023-07-25_aoc_Day11_MonkeyBusiness/2023-07-25_aoc_Day11_MonkeyBusiness.html#part-one-20-rounds-with-bounded-worry-level",
    "href": "posts/2023-07-25_aoc_Day11_MonkeyBusiness/2023-07-25_aoc_Day11_MonkeyBusiness.html#part-one-20-rounds-with-bounded-worry-level",
    "title": "Advent of Code Day 11: Monkey Business",
    "section": "Part One: 20 Rounds with Bounded Worry Level",
    "text": "Part One: 20 Rounds with Bounded Worry Level\nIn the first part, monkey business is tracked for 20 rounds. After each inspection, the worry level is divided by 3 and rounded down to the nearest integer. This division by three and only going 20 rounds keeps the worry level from becoming extremely large numbers. First, we create a function to calculate the worry level.\n\ncalc_worry &lt;- function(worry_init, operation, value) {\n  if (operation == \"^\") worry &lt;- worry_init^value\n  else if (operation == \"+\") worry &lt;- worry_init + value\n  else if (operation == \"*\") worry &lt;- worry_init * value\n\n  return(floor(worry / 3))\n}\n\nNext, we create a function to perform an entire round of monkey business. The number of item inspections for each monkey is also needed to answer the Part One question. The Test asks if the worry level is divisible by a certain number which is accomplished by using the modulo operator. If the worry level modulo Test number is 0, the test is true otherwise it is false.\n\nperform_round &lt;- function(monkey_ops) {\n  for (i in 1:length(monkey_ops)) {\n  \n    if (length(monkey_ops[[i]]$items) &gt; 0) {\n  \n      for (j in 1:length(monkey_ops[[i]]$items)) {\n        \n        worry_level &lt;- calc_worry(monkey_ops[[i]]$items[1], monkey_ops[[i]]$opaction, monkey_ops[[i]]$opvalue)\n        monkey_ops[[i]]$inspections &lt;- monkey_ops[[i]]$inspections + 1\n\n        if ((worry_level %% monkey_ops[[i]]$test) == 0) {\n          monkey_ops[[monkey_ops[[i]]$iftrue+1]]$items &lt;- c(monkey_ops[[monkey_ops[[i]]$iftrue+1]]$items, worry_level)\n        } else {\n          monkey_ops[[monkey_ops[[i]]$iffalse+1]]$items &lt;- c(monkey_ops[[monkey_ops[[i]]$iffalse+1]]$items, worry_level)\n        }\n        \n        monkey_ops[[i]]$items &lt;- monkey_ops[[i]]$items[-1]\n      }\n    }\n  }\n  return(monkey_ops)\n}\n\nNow, we perform the 20 rounds.\n\nmonkey_round &lt;- monkey_ops\n\nfor (k in 1:20) {\n  monkey_round &lt;- perform_round(monkey_round)\n}\n\nAnother function is created to calculate the monkey business which is defined as the number of inspections by the monkeys with the top 2 highest number of inspections and multiplying those 2 numbers together.\n\ncalc_monkey_business &lt;- function(monkey_final) {\n  first &lt;- 0\n  second &lt;- 0\n  \n  for (i in 1:length(monkey_final)) {\n    if (monkey_final[[i]]$inspections &gt; first) {\n      second &lt;- first\n      first &lt;- monkey_final[[i]]$inspections\n    } else if (monkey_final[[i]]$inspections &gt; second) {\n      second &lt;- monkey_final[[i]]$inspections\n    } else {\n    #do nothing  \n    }\n  }\n  return(first * second)\n}\n\nSolution to Part One\n\ncalc_monkey_business(monkey_round)\n\n[1] 57838"
  },
  {
    "objectID": "posts/2023-07-25_aoc_Day11_MonkeyBusiness/2023-07-25_aoc_Day11_MonkeyBusiness.html#part-two-10000-rounds-and-unbounded-worry-level",
    "href": "posts/2023-07-25_aoc_Day11_MonkeyBusiness/2023-07-25_aoc_Day11_MonkeyBusiness.html#part-two-10000-rounds-and-unbounded-worry-level",
    "title": "Advent of Code Day 11: Monkey Business",
    "section": "Part Two: 10,000 Rounds and Unbounded Worry Level",
    "text": "Part Two: 10,000 Rounds and Unbounded Worry Level\nThe changes in Part Two are that the worry level is no longer divided by 3 after each inspection. In addition, we know want to calculate the monkey business after 10,000 rounds and not just 20 rounds. Considering that some of the operations involve squaring the worry level after each inspection, it becomes clear that the worry level values will quickly exceed the numbers which can be reasonably handled using normal computer options. The problem statement rather vaguely tells us we need to find another way to keep the worry levels from exponentially growing over the 10,000 rounds. We create a new function to calculate worry level without the divisor.\n\ncalc_worry2 &lt;- function(worry_init, operation, value) {\n  if (operation == \"^\") worry &lt;- worry_init^value\n  else if (operation == \"+\") worry &lt;- worry_init + value\n  else if (operation == \"*\") worry &lt;- worry_init * value\n\n  return(worry)\n}\n\nSo how to do this? The key is to realize that the feature that needs to be maintained is whether the worry level can be divided by any of the test operation values. This can be achieved by taking the least common multiple of those values. In this case, the values: 5, 17, 2, 7, 3, 11, 13, 19 are all prime numbers. The least common multiple in this case is obtained by multiplying all 8 values together.\n\nsupermodulo &lt;- 1\n\nfor (k in 1:length(monkey_ops)) {\n  supermodulo &lt;- supermodulo * monkey_ops[[k]]$test  \n}\n\nsupermodulo\n\n[1] 9699690\n\n\nA minor change to the perform_round function is used to apply the “supermodulo” (i.e. least common multiple) to the worry level to keep it from becoming exponentially large.\n\nperform_round2 &lt;- function(monkey_ops, supermodulo) {\n  for (i in 1:length(monkey_ops)) {\n  \n    if (length(monkey_ops[[i]]$items) &gt; 0) {\n  \n      for (j in 1:length(monkey_ops[[i]]$items)) {\n        \n        worry_level &lt;- calc_worry2(monkey_ops[[i]]$items[1], monkey_ops[[i]]$opaction, monkey_ops[[i]]$opvalue)\n        worry_level &lt;- worry_level %% supermodulo\n        monkey_ops[[i]]$inspections &lt;- monkey_ops[[i]]$inspections + 1\n        \n        if ((worry_level %% monkey_ops[[i]]$test) == 0) {\n          monkey_ops[[monkey_ops[[i]]$iftrue+1]]$items &lt;- c(monkey_ops[[monkey_ops[[i]]$iftrue+1]]$items, worry_level)\n        } else {\n          monkey_ops[[monkey_ops[[i]]$iffalse+1]]$items &lt;- c(monkey_ops[[monkey_ops[[i]]$iffalse+1]]$items, worry_level)\n        }\n        \n        monkey_ops[[i]]$items &lt;- monkey_ops[[i]]$items[-1]\n      }\n    }\n  }\n  return(monkey_ops)\n}\n\nNow, we perform the monkey business for 10,000 rounds.\n\nmonkey_round &lt;- monkey_ops\n\nfor (k in 1:10000) {\n  monkey_round &lt;- perform_round2(monkey_round, supermodulo)\n}\n\nThen, we calculate the monkey business using the same function as in Part One.\n\ncalc_monkey_business(monkey_round)\n\n[1] 15050382231"
  },
  {
    "objectID": "posts/2023-07-25_aoc_Day11_MonkeyBusiness/2023-07-25_aoc_Day11_MonkeyBusiness.html#summary",
    "href": "posts/2023-07-25_aoc_Day11_MonkeyBusiness/2023-07-25_aoc_Day11_MonkeyBusiness.html#summary",
    "title": "Advent of Code Day 11: Monkey Business",
    "section": "Summary",
    "text": "Summary\nThe difficult part of the Day 11 problem is to figure out how to keep the worry level from exponentially growing over the 10,000 rounds of Part Two which cannot be handled by normal computation. Once the trick of using the least common multiple (“supermodulo”) to contain the worry levels without affecting the logic of the monkey inspections is identified, the solution can be found in a straightforward manner. Using the list to store the monkey information worked well although I was curious about making a monkey class instead. Perhaps, we will save this for a future post.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-07-25\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.353 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P readr       * 2.1.4   2023-02-10 [?] CRAN (R 4.2.3)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.5.0   2022-12-02 [?] CRAN (R 4.2.2)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/Rtmp6NO10I/renv-use-libpath-2ba45f5253be\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/R/cache/R/renv/sandbox/R-4.2/x86_64-w64-mingw32/30182023\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-07-28_aoc_Day12_BreadthFirst/2023-07-28_aoc_Day12_BreadthFirst.html",
    "href": "posts/2023-07-28_aoc_Day12_BreadthFirst/2023-07-28_aoc_Day12_BreadthFirst.html",
    "title": "Advent of Code Day 12: Breadth-First Search",
    "section": "",
    "text": "Finding the best path using a breadth-first search algorithm."
  },
  {
    "objectID": "posts/2023-07-28_aoc_Day12_BreadthFirst/2023-07-28_aoc_Day12_BreadthFirst.html#introduction",
    "href": "posts/2023-07-28_aoc_Day12_BreadthFirst/2023-07-28_aoc_Day12_BreadthFirst.html#introduction",
    "title": "Advent of Code Day 12: Breadth-First Search",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 12. You need to find a path to a location at a higher elevation in order to contact the elves with your communication device. You are able to use the device to get a height-map of the surrounding area which is in a similar format as shown below.\nSabqponm\nabcryxxl\naccszExk\nacctuvwj\nabdefghi\nIn this height-map, the lowest elevation is given by “a”, the highest elevation is given by “z” and the intermediate elevations ascend in alphabetical order. Your starting position is given by “S” and the goal is to reach “E”. You can only ascend at most one level in each step but you can descend more than one level."
  },
  {
    "objectID": "posts/2023-07-28_aoc_Day12_BreadthFirst/2023-07-28_aoc_Day12_BreadthFirst.html#loading-the-input-file",
    "href": "posts/2023-07-28_aoc_Day12_BreadthFirst/2023-07-28_aoc_Day12_BreadthFirst.html#loading-the-input-file",
    "title": "Advent of Code Day 12: Breadth-First Search",
    "section": "Loading the input file",
    "text": "Loading the input file\n\nlibrary(stringr)\n\nfilepath &lt;- here::here(\"./posts/data/aoc/day12_input.txt\")\n\ncommands &lt;- readLines(filepath)\n\n#find the size of the map\nnum_rows &lt;- length(commands)\nnum_cols &lt;- nchar(commands[[1]])\n\nThe height-map data is then parsed into a matrix using a similar approach as I used in Day 8 using the str_split_1 function from the stringr package.\n\nraw_map &lt;- character()\n\nfor (i in 1:length(commands)) {\n  raw_map &lt;- c(raw_map, as.character(str_split_1(commands[i], \"\")))\n}\n\ncontour_map &lt;- matrix(raw_map, nrow = num_rows, ncol = num_cols, byrow = TRUE)"
  },
  {
    "objectID": "posts/2023-07-28_aoc_Day12_BreadthFirst/2023-07-28_aoc_Day12_BreadthFirst.html#part-one-find-best-path-from-start-to-end",
    "href": "posts/2023-07-28_aoc_Day12_BreadthFirst/2023-07-28_aoc_Day12_BreadthFirst.html#part-one-find-best-path-from-start-to-end",
    "title": "Advent of Code Day 12: Breadth-First Search",
    "section": "Part One: Find best path from Start to End",
    "text": "Part One: Find best path from Start to End\nWe can find the start and end points in the matrix using the which function. Once the locations have been identified, we can convert the “S” to an “a” and “E” to a “z” to make the path finding code more straightforward.\n\nstart &lt;- which(contour_map == \"S\", arr.ind = T)\nend &lt;- which(contour_map == \"E\", arr.ind = T)\n\ncontour_map[start[1], start[2]] &lt;- \"a\"\ncontour_map[end[1], end[2]] &lt;- \"z\"\n\nA breadth-first search algorithm was used to find the best path. A queue is used to store the active path end locations and the number of steps which have been traversed. The key checks before adding a new path end to the queue are the following:\n\nCheck if new location is on the map\nCheck if new location has not already been visited\nCheck if new location is not more than 1 height difference from current location\nCheck if new location is already present in the queue\n\nIf all checks pass, the new path end location and distance is added to the queue as captured in the following function:\n\nadd_to_queue &lt;- function(con_map, cur_loc, new_loc, queue) {\n  if (new_loc[1] &lt; 1 | new_loc[1] &gt; num_rows) {\n    #do nothing if off the map\n  } else if (new_loc[2] &lt; 1 | new_loc[2] &gt; num_cols) {\n    #do nothing if off the map\n  } else if (con_map[new_loc[1], new_loc[2]] == \"*\") {\n    #do nothing if already visited\n  } else if (match(con_map[new_loc[1], new_loc[2]], letters) - match(con_map[cur_loc[1], cur_loc[2]], letters) &gt; 1) {\n    #do nothing if inaccessible\n  } else {\n    #add to queue\n    if (list(new_loc) %in% queue == FALSE){\n      queue &lt;- c(queue, list(new_loc))\n    }\n  }\n  return(queue)\n}\n\nBeginning at the start location, new points in each direction are checked with the add_to_queue function and added to the end of the queue if passing the checks. The points are then iteratively checked until the end location is reached. A while loop is used to continue this process until reached_end is TRUE. A loop count was added for troubleshooting and keep the loop from running forever if the logic was not quite right.\n\n#initialization\nreached_end &lt;- FALSE\ncon_map &lt;- contour_map\nloop_count &lt;- 1\nd &lt;- 0\nqueue &lt;- list(c(start[1], start[2], d))\n\nwhile (reached_end == FALSE & loop_count &lt; 3000) {\n\n  #points to N, E, S, W\n  cur_loc &lt;- queue[[1]]\n  temp &lt;- list(c(cur_loc[1]-1, cur_loc[2], cur_loc[3]+1), \n               c(cur_loc[1], cur_loc[2]+1, cur_loc[3]+1),\n               c(cur_loc[1]+1, cur_loc[2], cur_loc[3]+1),\n               c(cur_loc[1], cur_loc[2]-1, cur_loc[3]+1))\n  \n  #check points in each direction from current location  \n  for (location in temp) {\n    queue &lt;- add_to_queue(con_map, cur_loc, location, queue)\n  }\n  \n  #remove current location from queue\n  queue &lt;- queue[-1]\n  #set location at * meaning already visited\n  con_map[cur_loc[1], cur_loc[2]] &lt;- \"*\"\n  \n  #check if end location is in queue and, if so, set reached_end to TRUE\n  for (i in 1:length(queue)){\n    if (sum(queue[[i]][1:2] == c(end[1], end[2])) == 2) {\n      reached_end &lt;- TRUE\n      d &lt;- queue[[i]][3]\n    }\n  }\n  loop_count &lt;- loop_count + 1\n}\n\nThe distance is saved to d.\n\nprint(paste(\"The number of steps to the end location is:\", d))\n\n[1] \"The number of steps to the end location is: 370\""
  },
  {
    "objectID": "posts/2023-07-28_aoc_Day12_BreadthFirst/2023-07-28_aoc_Day12_BreadthFirst.html#part-two-best-path-starting-from-any-a-location",
    "href": "posts/2023-07-28_aoc_Day12_BreadthFirst/2023-07-28_aoc_Day12_BreadthFirst.html#part-two-best-path-starting-from-any-a-location",
    "title": "Advent of Code Day 12: Breadth-First Search",
    "section": "Part Two: Best path starting from any “a” location",
    "text": "Part Two: Best path starting from any “a” location\nThe next part asks which path has the least number of steps from to the end goal but, this time, starting from any “a” location on the height-map. One could consider checking the number of steps from every “a” location in the map and then find the minimum however this is not the optimal approach. Instead, you can reverse the search starting at the end location and finding the minimum steps to any “a”.\nThe add_to_queue function is modified to now that we are descending rather than ascending which only requires changing one line. This modified function is named add_to_queue2 however it should be noted that this approach would also work for Part One.\n\nadd_to_queue2 &lt;- function(con_map, cur_loc, new_loc, queue) {\n  if (new_loc[1] &lt; 1 | new_loc[1] &gt; num_rows) {\n    #do nothing if off the map\n  } else if (new_loc[2] &lt; 1 | new_loc[2] &gt; num_cols) {\n    #do nothing if off the map\n  } else if (con_map[new_loc[1], new_loc[2]] == \"*\") {\n    #do nothing if already visited\n  } else if (match(con_map[new_loc[1], new_loc[2]], letters) - match(con_map[cur_loc[1], cur_loc[2]], letters) &lt; -1) {\n    #do nothing if inaccessible - logic was changed as we are now backtracking\n  } else {\n    #add to queue\n    if (list(new_loc) %in% queue == FALSE){\n      queue &lt;- c(queue, list(new_loc))\n    }\n  }\n  return(queue)\n}\n\nThe code for finding the best path is very similar as for Part One. There are two main changes: 1) the starting point is changed to the end location and 2) the check to see whether the goal is reached is performed by checking if the location in the queue is an “a”.\n\n#initialization\nreached_end &lt;- FALSE\ncon_map &lt;- contour_map\nloop_count &lt;- 1\nd &lt;- 0\n\n#need to start at \"E\" - the desired end point\nqueue &lt;- list(c(end[1], end[2], d))\n\nwhile (reached_end == FALSE & loop_count &lt; 3000) {\n\n  #points to N, E, S, W\n  cur_loc &lt;- queue[[1]]\n  temp &lt;- list(c(cur_loc[1]-1, cur_loc[2], cur_loc[3]+1), \n               c(cur_loc[1], cur_loc[2]+1, cur_loc[3]+1),\n               c(cur_loc[1]+1, cur_loc[2], cur_loc[3]+1),\n               c(cur_loc[1], cur_loc[2]-1, cur_loc[3]+1))\n    \n  for (location in temp) {\n    #now using the backtracking function\n    queue &lt;- add_to_queue2(con_map, cur_loc, location, queue)\n  }\n  \n  #remove current location from queue\n  queue &lt;- queue[-1]\n  #set location at * meaning already visited\n  con_map[cur_loc[1], cur_loc[2]] &lt;- \"*\"\n\n  #check if queue location is \"a\" and, if so, set reached_end to TRUE\n  for (i in 1:length(queue)){\n    if (con_map[queue[[i]][1], queue[[i]][2]] == \"a\") {\n      reached_end &lt;- TRUE\n      d &lt;- queue[[i]][3]\n    }\n  }\n  loop_count &lt;- loop_count + 1\n}\n\n\nprint(paste(\"The minimum number of steps from any 'a' location to the end location is:\", d))\n\n[1] \"The minimum number of steps from any 'a' location to the end location is: 363\""
  },
  {
    "objectID": "posts/2023-07-28_aoc_Day12_BreadthFirst/2023-07-28_aoc_Day12_BreadthFirst.html#bonus-what-actually-is-the-path",
    "href": "posts/2023-07-28_aoc_Day12_BreadthFirst/2023-07-28_aoc_Day12_BreadthFirst.html#bonus-what-actually-is-the-path",
    "title": "Advent of Code Day 12: Breadth-First Search",
    "section": "Bonus: what actually is the path?",
    "text": "Bonus: what actually is the path?\nAfter solving the Day 12 problem, I had this burning question of how would you actually find the best path using this approach. The solution to the problem so far had only required knowing the number of steps to get to the end goal and not the route to take. In order to answer this additional question, the steps to reach the end location needs to be saved for each active path end. This requires a change to the queue where we now save each path as a list within a list. The add_to_queue function needs to be modified again related to checking if the new series of steps in already in the queue. As written, another check is needed to see if the current location has been already visited.\n\nadd_to_queue3 &lt;- function(con_map, cur_loc, new_loc, queue) {\n  if (new_loc[1] &lt; 1 | new_loc[1] &gt; num_rows) {\n    #do nothing if off the map\n  } else if (new_loc[2] &lt; 1 | new_loc[2] &gt; num_cols) {\n    #do nothing if off the map\n  } else if (con_map[new_loc[1], new_loc[2]] == \"*\") {\n    #do nothing if already visited\n  } else if (con_map[cur_loc[1], cur_loc[2]] == \"*\") {\n    #do nothing if already visited\n  } else if (match(con_map[new_loc[1], new_loc[2]], letters) - match(con_map[cur_loc[1], cur_loc[2]], letters) &gt; 1) {\n    #do nothing if inaccessible\n  } else {\n    #add path to end of queue if not already existing\n    if (sum(c(queue[[1]], list(new_loc)) %in% queue) == 0){\n      queue &lt;- c(queue, list(c(queue[[1]], list(new_loc))))\n      #print(queue)\n    }\n  }\n  return(queue)\n}\n\nThe best path for Part One is now determined taking into account the new list within a list structure.\n\nreached_end &lt;- FALSE\ncon_map &lt;- contour_map\nloop_count &lt;- 1\nd &lt;- 0\nwhereinq &lt;- 0\n#now the path are nested\nqueue &lt;- list(list(c(start[1], start[2], d)))\n\nwhile (reached_end == FALSE & loop_count &lt; 30000) {\n\n  #points to N, E, S, W\n  cur_loc &lt;- queue[[1]][[length(queue[[1]])]] #new nested scheme - take last location in path\n  temp &lt;- list(c(cur_loc[1]-1, cur_loc[2], cur_loc[3]+1), \n               c(cur_loc[1], cur_loc[2]+1, cur_loc[3]+1),\n               c(cur_loc[1]+1, cur_loc[2], cur_loc[3]+1),\n               c(cur_loc[1], cur_loc[2]-1, cur_loc[3]+1))\n    \n  for (location in temp) {\n    #print(location)\n    queue &lt;- add_to_queue3(con_map, cur_loc, location, queue)\n  }\n  \n  queue &lt;- queue[-1]\n  con_map[cur_loc[1], cur_loc[2]] &lt;- \"*\"\n  \n  # if (list(c(end_row, end_col)) %in% queue) {\n  #   reached_end &lt;- TRUE\n  # }\n  \n  for (i in 1:length(queue)){\n    if (sum(queue[[i]][[length(queue[[i]])]][1:2] == c(end[1], end[2])) == 2) {\n      reached_end &lt;- TRUE\n      d &lt;- queue[[i]][[length(queue[[i]])]][3]\n      whereinq &lt;- i\n    }\n  }\n  loop_count &lt;- loop_count + 1\n}\n\nLet’s plot the best path using the original height-map layout. We will accomplish this plot by converting the list containing the best path into a table and the matrix containing the height-map into a long-form table.\n\n#Load tidyverse libraries\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n#Long-form table for the height-map\ncontour_data &lt;- contour_map |&gt;\n  as_tibble() |&gt;\n  mutate(row_id = row_number()) |&gt;\n  pivot_longer(cols = starts_with(\"V\"), names_to = \"name\", values_to = \"value\") |&gt;\n  mutate(col_id = as.integer(sub(\"V\", \"\", name)))|&gt;\n  select(row_id, col_id, value)\n\nhead(contour_data)\n\n# A tibble: 6 × 3\n  row_id col_id value\n   &lt;int&gt;  &lt;int&gt; &lt;chr&gt;\n1      1      1 a    \n2      1      2 b    \n3      1      3 a    \n4      1      4 a    \n5      1      5 c    \n6      1      6 c    \n\n\n\nbest_path &lt;- as_tibble(queue[[length(queue)]], .name_repair = \"minimal\")\nbest_path &lt;- as_tibble(t(best_path), .name_repair = \"minimal\")\nnames(best_path) &lt;- c(\"row_id\", \"col_id\", \"d\")\n\nhead(best_path)\n\n# A tibble: 6 × 3\n  row_id col_id     d\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     21      1     0\n2     20      1     1\n3     19      1     2\n4     18      1     3\n5     17      1     4\n6     16      1     5\n\n\nNow, a left-join is used to merge the best path with the original height-map for plotting purposes.\n\nplot_data &lt;- left_join(contour_data, best_path, by = c(\"row_id\", \"col_id\"))\n\nFinally, we visualize the path taken.\n\np1 &lt;- plot_data |&gt;\n  rowwise() |&gt;\n  mutate(onpath = ifelse(is.na(d), \"no\", \"yes\")) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = col_id, y = row_id, label = value, color = factor(onpath))) +\n    geom_text() +\n    scale_y_reverse() +\n    labs(color = \"On the path?\") +\n    theme_void()\np1"
  },
  {
    "objectID": "posts/2023-07-28_aoc_Day12_BreadthFirst/2023-07-28_aoc_Day12_BreadthFirst.html#summary",
    "href": "posts/2023-07-28_aoc_Day12_BreadthFirst/2023-07-28_aoc_Day12_BreadthFirst.html#summary",
    "title": "Advent of Code Day 12: Breadth-First Search",
    "section": "Summary",
    "text": "Summary\nThe day 12 problem was solved by implementing a breadth-first search algorithm in R. The key to solving Part Two was to realize the path could be solved backwards from the end goal to identify the first time an “a” was found. It would have taken a great deal more computing power to find the path from every “a” in the input dataset as there were many. As a bonus, Part One was implemented in a manner so that the actual path could be saved and plotted. There are some inefficiencies in the code but it did the job and found the correct solutions.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-08-01\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.353 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P dplyr       * 1.1.2   2023-04-20 [?] CRAN (R 4.2.3)\n P ggplot2     * 3.4.2   2023-04-03 [?] CRAN (R 4.2.3)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.5.0   2022-12-02 [?] CRAN (R 4.2.2)\n P tidyr       * 1.3.0   2023-01-24 [?] CRAN (R 4.2.3)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpQbQ1zv/renv-use-libpath-1dd84e02a8e\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/R/cache/R/renv/sandbox/R-4.2/x86_64-w64-mingw32/30182023\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-08-02_aoc_Day13_Recursion/2023-08-02_aoc_Day13_Recursion.html",
    "href": "posts/2023-08-02_aoc_Day13_Recursion/2023-08-02_aoc_Day13_Recursion.html",
    "title": "Advent of Code Day 13: Recursion",
    "section": "",
    "text": "Creating a recursive function to compare nested lists."
  },
  {
    "objectID": "posts/2023-08-02_aoc_Day13_Recursion/2023-08-02_aoc_Day13_Recursion.html#introduction",
    "href": "posts/2023-08-02_aoc_Day13_Recursion/2023-08-02_aoc_Day13_Recursion.html#introduction",
    "title": "Advent of Code Day 13: Recursion",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 13. The task is to sort packets from a distress signal that have been decoded out of order. The packets are provided in pairs and whether they are in the correct order. The packets resemble a nested list format as shown in the examples below.\n[1,1,3,1,1]\n[1,1,5,1,1]\n\n[[1],[2,3,4]]\n[[1],4]\n\n[9]\n[[8,7,6]]\n\n[[4,4],4,4]\n[[4,4],4,4,4]\n\n[7,7,7,7]\n[7,7,7]\n\n[]\n[3]\n\n[[[]]]\n[[]]\n\n[1,[2,[3,[4,[5,6,7]]]],8,9]\n[1,[2,[3,[4,[5,6,0]]]],8,9]"
  },
  {
    "objectID": "posts/2023-08-02_aoc_Day13_Recursion/2023-08-02_aoc_Day13_Recursion.html#loading-the-input-file",
    "href": "posts/2023-08-02_aoc_Day13_Recursion/2023-08-02_aoc_Day13_Recursion.html#loading-the-input-file",
    "title": "Advent of Code Day 13: Recursion",
    "section": "Loading the input file",
    "text": "Loading the input file\nThe data is read into strings using read_lines from the readr package which has the handy option to ignore the blank rows. In addition, any white space before or after the packet is removed using str_trim.\n\nlibrary(readr)\nlibrary(stringr)\n\nfilepath &lt;- here::here(\"./posts/data/aoc/day13_input.txt\")\n\npackets &lt;- read_lines(filepath, skip_empty_rows = TRUE)\npackets &lt;- str_trim(packets)\n\nIn Python, the packet string can be directly evaluated to a list however, in R, the packet string needs to be modified first. In R, a list is declared using the list function. For this problem, a packet_to_list function was created to substitute all left brackets with list( and all right brackets with ). The modified string was then parsed and evaluated to create a nested list for each packet.\n\npacket_to_list &lt;- function(packet1) {\n  temp &lt;- gsub(\"\\\\[\", \"list\\\\(\", packet1)\n  temp &lt;- gsub(\"\\\\]\", \"\\\\)\", temp)\n  return(eval(parse(text = temp)))\n}"
  },
  {
    "objectID": "posts/2023-08-02_aoc_Day13_Recursion/2023-08-02_aoc_Day13_Recursion.html#part-one-ordering-of-the-packets",
    "href": "posts/2023-08-02_aoc_Day13_Recursion/2023-08-02_aoc_Day13_Recursion.html#part-one-ordering-of-the-packets",
    "title": "Advent of Code Day 13: Recursion",
    "section": "Part One: Ordering of the Packets",
    "text": "Part One: Ordering of the Packets\nThe order of each pair of packets needs to assessed as to whether it is correct or not. Multiple rules are provided to determine whether the order is correct. The contents of the lists in each packet are compared element by element. If both elements are integers, the integer on the left (from first packet) should be less than the integer on the right (from second packet). If one element is an integer and one is a list, the integer is converted to a list with the integer as the only list element. If the left list runs out of items before the right list, the packets are in the correct order. The first element shown to be in the correct or incorrect order determines whether the order for the entire packet is correct. If a decision cannot be made on the elements, the next elements are compared.\nSince the number of nested lists within each packet is not the same, a recursive function is useful to handle these comparisons. A function called packet_compare has been developed below to apply the ordering rules. If the integer for the left (first) packet is less than the integer in right (second) packet, the function returns a negative value. So, negative value means correct order and positive value means an incorrect order. If all the integers are the same, the length of the left packet minus the length of the right packet is returned. In this case, a negative value is also the correct order.\n\npacket_compare &lt;- function(packet1, packet2) {\n  \n  if (class(packet1) == \"numeric\") {\n    if(class(packet2) == \"numeric\") {\n      return(packet1 - packet2)\n    } else {\n      return(packet_compare(list(packet1), packet2))\n    }\n  } else {\n    if (class(packet2) == \"numeric\") {\n      return(packet_compare(packet1, list(packet2)))\n    }\n  }\n  #handle cases where packet1 is zero length\n  if (length(packet1) == 0 & length(packet2) &gt; 0) return(-1)\n  if (length(packet1) == 0 & length(packet2) == 0) return(0)\n\n  for (i in 1:length(packet1)) {\n    if (length(packet2) &gt;= i) {\n      v &lt;- packet_compare(packet1[[i]], packet2[[i]])\n      if (v != 0) {\n        return(v)\n      }\n    }\n  }\n  return(length(packet1) - length(packet2))\n}\n\nThe solution to Part One is found by comparing each pair of packets, adding the indices for the packets in the correct order and reporting the total. In this case, we iterate by 2 and adjust the indices accordingly since the first and second packet correspond to an index of 1.\n\nresult &lt;- 0\n\nfor (i in seq(1, length(packets), 2)) {\n  test &lt;- packet_compare(packet_to_list(packets[i]), packet_to_list(packets[i+1]))\n  if (test &lt; 0) {\n    result &lt;- result + (i+1)/2\n  }\n}\n\nprint(result)\n\n[1] 5366"
  },
  {
    "objectID": "posts/2023-08-02_aoc_Day13_Recursion/2023-08-02_aoc_Day13_Recursion.html#part-two-finding-indices-for-2-and-6",
    "href": "posts/2023-08-02_aoc_Day13_Recursion/2023-08-02_aoc_Day13_Recursion.html#part-two-finding-indices-for-2-and-6",
    "title": "Advent of Code Day 13: Recursion",
    "section": "Part Two: Finding indices for [[2]] and [[6]]",
    "text": "Part Two: Finding indices for [[2]] and [[6]]\nThe problem in Part Two is presented a bit misleadingly. We are asked to order all of the packets in the input dataset while also adding the packets [[2]] and [[6]]. We are asked to place all of the packets in order and report the indices for [[2]] and [[6]]. The misleading part is that you really don’t need to order all of the packets to determine the solution. You simply need to first compare each packet to [[2]] and if it is lower order then the index for [[2]] and [[6]] both are incremented by 1. If the packet is higher order than [[2]], a second comparison is made to [[6]] and if it is lower then the index for [[6]] is incremented by 1 otherwise neither index is incremented. The same packet_compare function from Part One is used here.\n\nind2 &lt;- 1\nind6 &lt;- 2\n\nfor(i in 1:length(packets)) {\n  test &lt;- packet_compare(packet_to_list(packets[i]), list(list(2)))\n  if (test &lt; 0) {\n    ind2 &lt;- ind2 + 1\n    ind6 &lt;- ind6 + 1\n  } else {\n    test2 &lt;- packet_compare(packet_to_list(packets[i]), list(list(6)))\n    if (test2 &lt; 0) {\n      ind6 &lt;- ind6 + 1\n    }\n  }\n}\n\nind2*ind6\n\n[1] 23391\n\n\nThe solution to Part Two is the product of the indices for [[2]] and [[6]] after checking all of the packets."
  },
  {
    "objectID": "posts/2023-08-02_aoc_Day13_Recursion/2023-08-02_aoc_Day13_Recursion.html#summary",
    "href": "posts/2023-08-02_aoc_Day13_Recursion/2023-08-02_aoc_Day13_Recursion.html#summary",
    "title": "Advent of Code Day 13: Recursion",
    "section": "Summary",
    "text": "Summary\nThe Day 13 problem was an excellent use case for a recursive function. The recursive function really simplified the comparisons of the packets without determining the number of nested lists in each packet upfront. I imagine this function also was quite a bit faster than pre-checking the number of nested lists in each packet and iterating through each one.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-08-07\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.353 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P readr       * 2.1.4   2023-02-10 [?] CRAN (R 4.2.3)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.5.0   2022-12-02 [?] CRAN (R 4.2.2)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/Rtmp4Qh6kX/renv-use-libpath-3ff07a925278\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/R/cache/R/renv/sandbox/R-4.2/x86_64-w64-mingw32/30182023\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-08-09_aoc_Day14_FallingSand/2023-08-09_aoc_Day14_FallingSand.html",
    "href": "posts/2023-08-09_aoc_Day14_FallingSand/2023-08-09_aoc_Day14_FallingSand.html",
    "title": "Advent of Code Day 14: Falling Sand",
    "section": "",
    "text": "Creating a recursive function to track the path of falling sand in a cavern"
  },
  {
    "objectID": "posts/2023-08-09_aoc_Day14_FallingSand/2023-08-09_aoc_Day14_FallingSand.html#introduction",
    "href": "posts/2023-08-09_aoc_Day14_FallingSand/2023-08-09_aoc_Day14_FallingSand.html#introduction",
    "title": "Advent of Code Day 14: Falling Sand",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 14. In this problem, you scan a cavern with a device which provides you with the rock structure. Sand is falling from a specific location at the top of the cavern. Sand collects on the rocks as it descends into the cavern until reaching the edge of cavern where it falls into an abyss. You are provided a map of the rocks which are represented as points which are connected by straight horizontal or vertical lines as shown below:\n498,4 -&gt; 498,6 -&gt; 496,6\n503,4 -&gt; 502,4 -&gt; 502,9 -&gt; 494,9\nWhich can be used to generate a map as shown below:\n  4     5  5\n  9     0  0\n  4     0  3\n0 ......+...\n1 ..........\n2 ..........\n3 ..........\n4 ....#...##\n5 ....#...#.\n6 ..###...#.\n7 ........#.\n8 ........#.\n9 #########.\nWhere “.” are open spaces and “#” are representing the rock structure. The sand comes out in units which fall straight down until it hits a rock or another sand unit. If it is blocked, the sand unit will try to go diagonally down and to the left. If still blocked, the sand unit will try to go diagonally down and the right. If all three directions are blocked, the sand comes to rest at its current location and is marked with an “o”. If the sand goes past the furthest rock mapped to the left or right then it falls into the abyss."
  },
  {
    "objectID": "posts/2023-08-09_aoc_Day14_FallingSand/2023-08-09_aoc_Day14_FallingSand.html#loading-the-input-file",
    "href": "posts/2023-08-09_aoc_Day14_FallingSand/2023-08-09_aoc_Day14_FallingSand.html#loading-the-input-file",
    "title": "Advent of Code Day 14: Falling Sand",
    "section": "Loading the input file",
    "text": "Loading the input file\nThe data is read into strings using read_lines from the readr package which has the handy option to ignore the blank rows. In addition, any white space before or after the packet is removed using str_trim.\n\nlibrary(readr)\nlibrary(stringr)\nlibrary(purrr)\n\nfilepath &lt;- here::here(\"./posts/data/aoc/day14_input.txt\") \n\nrocks &lt;- read_lines(filepath, skip_empty_rows = TRUE)\nrocks &lt;- str_trim(rocks)\nhead(rocks)\n\n[1] \"494,132 -&gt; 494,134 -&gt; 487,134 -&gt; 487,139 -&gt; 502,139 -&gt; 502,134 -&gt; 499,134 -&gt; 499,132\"                                                                                                                                                                                      \n[2] \"509,82 -&gt; 514,82\"                                                                                                                                                                                                                                                          \n[3] \"511,113 -&gt; 511,116 -&gt; 509,116 -&gt; 509,120 -&gt; 522,120 -&gt; 522,116 -&gt; 516,116 -&gt; 516,113\"                                                                                                                                                                                      \n[4] \"493,36 -&gt; 493,33 -&gt; 493,36 -&gt; 495,36 -&gt; 495,35 -&gt; 495,36 -&gt; 497,36 -&gt; 497,33 -&gt; 497,36 -&gt; 499,36 -&gt; 499,35 -&gt; 499,36 -&gt; 501,36 -&gt; 501,34 -&gt; 501,36 -&gt; 503,36 -&gt; 503,34 -&gt; 503,36 -&gt; 505,36 -&gt; 505,26 -&gt; 505,36 -&gt; 507,36 -&gt; 507,31 -&gt; 507,36 -&gt; 509,36 -&gt; 509,33 -&gt; 509,36\"\n[5] \"527,101 -&gt; 527,99 -&gt; 527,101 -&gt; 529,101 -&gt; 529,98 -&gt; 529,101 -&gt; 531,101 -&gt; 531,92 -&gt; 531,101 -&gt; 533,101 -&gt; 533,91 -&gt; 533,101 -&gt; 535,101 -&gt; 535,99 -&gt; 535,101 -&gt; 537,101 -&gt; 537,100 -&gt; 537,101 -&gt; 539,101 -&gt; 539,93 -&gt; 539,101\"                                             \n[6] \"493,36 -&gt; 493,33 -&gt; 493,36 -&gt; 495,36 -&gt; 495,35 -&gt; 495,36 -&gt; 497,36 -&gt; 497,33 -&gt; 497,36 -&gt; 499,36 -&gt; 499,35 -&gt; 499,36 -&gt; 501,36 -&gt; 501,34 -&gt; 501,36 -&gt; 503,36 -&gt; 503,34 -&gt; 503,36 -&gt; 505,36 -&gt; 505,26 -&gt; 505,36 -&gt; 507,36 -&gt; 507,31 -&gt; 507,36 -&gt; 509,36 -&gt; 509,33 -&gt; 509,36\"\n\n\nThe idea was to save each rock structure in its own list where each list element would contain the horizontal and vertical coordinate points in a vector. Three steps are required to create this data structure: 1) remove the arrows (“-&gt;”) and replace with a single space, 2) split the coordinate pairs into their own lists and 3) reformat each coordinate pair string into a numeric vector.\n\n# Remove the arrows\nstructure &lt;- list()\nfor (rock in rocks) {\n  structure &lt;- c(structure, unlist(str_replace_all(rock, \" -&gt; \", \" \")))\n}\n# Split the coordinate pairs into their own lists\nfor (i in 1:length(structure)) {\n  structure[[i]] &lt;- str_split(structure[[i]], \" \")\n}\n# Reformat the coordinate pairs into numeric vectors\nfor (i in 1:length(structure)) {\n  structure[[i]] &lt;- str_split(unlist(structure[[i]]), \",\")\n  structure[[i]] &lt;- map(structure[[i]], function(x) as.numeric(x))\n}\nstructure[[1]]\n\n[[1]]\n[1] 494 132\n\n[[2]]\n[1] 494 134\n\n[[3]]\n[1] 487 134\n\n[[4]]\n[1] 487 139\n\n[[5]]\n[1] 502 139\n\n[[6]]\n[1] 502 134\n\n[[7]]\n[1] 499 134\n\n[[8]]\n[1] 499 132"
  },
  {
    "objectID": "posts/2023-08-09_aoc_Day14_FallingSand/2023-08-09_aoc_Day14_FallingSand.html#part-one-falling-into-the-abyss",
    "href": "posts/2023-08-09_aoc_Day14_FallingSand/2023-08-09_aoc_Day14_FallingSand.html#part-one-falling-into-the-abyss",
    "title": "Advent of Code Day 14: Falling Sand",
    "section": "Part One: Falling into the Abyss",
    "text": "Part One: Falling into the Abyss\nIn Part One, the coordinates in the input file define the overall map size. If the sand travels outside this map, it falls into the “abyss”. The goal of this part is to determine how many units of sand fall before sand begins to fall into the abyss. The first step is to determine the size of the map.\n\nmap_size &lt;- list(min_x = 9999, max_x = 0, min_y = 9999, max_y = 0)\n\nfor (i in 1:length(structure)) {\n  for (j in 1: length(structure[[i]])) {\n    if (structure[[i]][[j]][1] &lt; map_size[[\"min_x\"]]) {\n      map_size[[\"min_x\"]] &lt;- structure[[i]][[j]][1]\n    } else if (structure[[i]][[j]][1] &gt; map_size[[\"max_x\"]]) {\n      map_size[[\"max_x\"]] &lt;-structure[[i]][[j]][1]\n    } else {\n      #do nothing\n    }\n    \n    if (structure[[i]][[j]][2] &lt; map_size[[\"min_y\"]]) {\n      map_size[[\"min_y\"]] &lt;- structure[[i]][[j]][2]\n    } else if (structure[[i]][[j]][2] &gt; map_size[[\"max_y\"]]) {\n      map_size[[\"max_y\"]] &lt;-structure[[i]][[j]][2]\n    } else {\n      #do nothing\n    } \n  }\n}\nmap_size\n\n$min_x\n[1] 483\n\n$max_x\n[1] 554\n\n$min_y\n[1] 14\n\n$max_y\n[1] 164\n\n\nThe minimum and maximum values can then be used to construct a matrix to serve as a map for the rock structures where the x coordinates correspond to columns and y coordinates correspond to rows. The matrix is initiated with “.” characters and the rock structure is indicated with “#” characters.\n\nnum_rows &lt;- map_size[[\"max_y\"]] + 1  #because the first column is \"0\" \nnum_cols &lt;- map_size[[\"max_x\"]] - map_size[[\"min_x\"]] + 1\n\nblank_map &lt;- matrix(rep(\".\", num_rows*num_cols), nrow = num_rows, ncol = num_cols)\n\n\nrock_map &lt;- blank_map\n\nfor (i in 1:length(structure)) {\n  for (j in 1:(length(structure[[i]])-1)) {\n    start_pt &lt;- structure[[i]][[j]]\n    end_pt &lt;- structure[[i]][[j+1]]\n    for (m in start_pt[1]:end_pt[1]) {\n      for(n in start_pt[2]:end_pt[2]) {\n        rock_map[n+1, m-map_size[[\"min_x\"]]+1] &lt;- \"#\"\n      }\n    } \n  }\n}\n\nPlot the map\n\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nplot_start &lt;- rock_map |&gt; \n  as_tibble() |&gt;\n  mutate(row_id = row_number()) |&gt;\n  pivot_longer(cols = starts_with(\"V\"), names_to = \"name\", values_to = \"value\") |&gt;\n  mutate(col_id = as.integer(sub(\"V\", \"\", name)))|&gt;\n  select(row_id, col_id, value)\n\n\np1 &lt;- plot_start |&gt;\n  rowwise() |&gt;\n  mutate(rock = ifelse(value == \"#\", \"yes\", \"no\")) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = col_id, y = row_id, label = value, color = factor(rock))) +\n    geom_text() +\n    scale_y_reverse() +\n    theme_void() +\n    theme(legend.position = \"none\") +\n    scale_color_manual(values = c(\"#D3D3D3\", \"#3D3D3D\"))\np1\n\n\n\n\nFinding the resting locations for each sand unit is performed using a recursive function. The recursive function helps in these cases where the number of steps in any direction are difficult to calculate upfront. Here, we can just code the different options and send the new location back to the function until it finds its resting spot. An overflow\n\nfalling_sand &lt;- function(c_loc, r_map) {\n  #try down\n  if (r_map[c_loc[1]+1, c_loc[2]] == \".\") {\n    return(falling_sand(c(c_loc[1]+1, c_loc[2]), r_map))\n  } else {\n    #try left\n    if (c_loc[2]-1 &lt; 1) return(list(r_map, overflow = TRUE))\n    if (r_map[c_loc[1]+1, c_loc[2]-1] == \".\")  {\n      return(falling_sand(c(c_loc[1]+1, c_loc[2]-1), r_map))\n    } else {\n      #try right\n      if(c_loc[2]+1 &gt; ncol(r_map)) return(list(r_map, overflow = TRUE))\n      if (r_map[c_loc[1]+1, c_loc[2]+1] == \".\") {\n        return(falling_sand(c(c_loc[1]+1, c_loc[2]+1), r_map))\n      } else {\n        r_map[c_loc[1], c_loc[2]] &lt;- \"o\"\n      }\n    }\n  }\n  return(list(r_map, overflow = FALSE))\n}\n\nNow, the falling of the sand begins! We initiate units of sand until the sand falls into the abyss (overflow = TRUE). A counter is also included to determine then number of sand units dropped and also to try to avoid an infinite loop.\n\nloop_count &lt;- 0\ntemp &lt;- list(rock_map, overflow = FALSE)\n\nwhile (temp[[\"overflow\"]] == FALSE & loop_count &lt; 10000) {\n  loop_count &lt;- loop_count + 1\n  temp &lt;- falling_sand(c(1, 500-map_size[[\"min_x\"]]+1), temp[[1]])\n}\n\nprint(paste(\"Sand bags dropped = \", loop_count-1))\n\n[1] \"Sand bags dropped =  1016\"\n\n\n\nplot_data &lt;- temp[[1]] |&gt; \n  as_tibble() |&gt;\n  mutate(row_id = row_number()) |&gt;\n  pivot_longer(cols = starts_with(\"V\"), names_to = \"name\", values_to = \"value\") |&gt;\n  mutate(col_id = as.integer(sub(\"V\", \"\", name)))|&gt;\n  select(row_id, col_id, value)\n\np2 &lt;- plot_data |&gt;\n  ggplot(aes(x = col_id, y = row_id, label = value, color = factor(value))) +\n    geom_text() +\n    scale_y_reverse() +\n    theme_void() +\n    theme(legend.position = \"none\") +\n    scale_color_manual(values = c(\"#3D3D3D\", \"#D3D3D3\", \"#B3AC85\"))\np2"
  },
  {
    "objectID": "posts/2023-08-09_aoc_Day14_FallingSand/2023-08-09_aoc_Day14_FallingSand.html#part-two-the-hidden-cave-floor",
    "href": "posts/2023-08-09_aoc_Day14_FallingSand/2023-08-09_aoc_Day14_FallingSand.html#part-two-the-hidden-cave-floor",
    "title": "Advent of Code Day 14: Falling Sand",
    "section": "Part Two: The Hidden Cave Floor",
    "text": "Part Two: The Hidden Cave Floor\nIn Part Two, we realize that there isn’t actually an abyss but rather there is a cavern floor which extends two coordinates below the lowest rock structure in the map. The floor is assumed to extend endlessly in both directions. The goal of part two is to find out how many sand units fall until the inlet for the sand will be clogged. It is straightforward to increase the number of rows by two for the cavern floor but how to determine the number of columns required? The maximum size will be a triangle down to the bottom row which can be figured as twice the number of rows plus 1 (actually 2 less is required since the inlet is clogged on the second row).\n\nnum_rows &lt;- map_size[[\"max_y\"]] + 3  #because the first column is \"0\" \nnum_cols &lt;- (num_rows * 2) + 1\n\nblank_map &lt;- matrix(rep(\".\", num_rows*num_cols), nrow = num_rows, ncol = num_cols)\n\nThe rock map is created in the same manner as in Part One however the rock line at the bottom needs to be added for the cavern floor.\n\nrock_map &lt;- blank_map\n\nfor (i in 1:length(structure)) {\n  for (j in 1:(length(structure[[i]])-1)) {\n    start_pt &lt;- structure[[i]][[j]]\n    end_pt &lt;- structure[[i]][[j+1]]\n    for (m in start_pt[1]:end_pt[1]) {\n      for(n in start_pt[2]:end_pt[2]) {\n        rock_map[n+1, m-num_cols+3] &lt;- \"#\"\n      }\n    } \n  }\n}\n#Add the line at the bottom\nfor (i in 1:num_cols) {\n  rock_map[num_rows, i] &lt;- \"#\"\n}\n\nA similar recursive function is used but with a different meaning for “overflow”. In this case, overflow means that the sand outlet is clogged rather than sand falling into the abyss. The modifications are shown below.\n\nfalling_sand2 &lt;- function(c_loc, r_map) {\n  #check if outlet is clogged\n  if (r_map[1, 168] == \"o\") {\n    return(list(r_map, overflow = TRUE))\n  } else {\n    #try down\n    if (r_map[c_loc[1]+1, c_loc[2]] == \".\") {\n    return(falling_sand2(c(c_loc[1]+1, c_loc[2]), r_map))\n    } else {\n      #try left\n      #if (c_loc[2]-1 &lt; 1) return(list(r_map, overflow = TRUE))\n      if (r_map[c_loc[1]+1, c_loc[2]-1] == \".\")  {\n        return(falling_sand2(c(c_loc[1]+1, c_loc[2]-1), r_map))\n      } else {\n        #try right\n        #if(c_loc[2]+1 &gt; ncol(r_map)) return(list(r_map, overflow = TRUE))\n        if (r_map[c_loc[1]+1, c_loc[2]+1] == \".\") {\n          return(falling_sand2(c(c_loc[1]+1, c_loc[2]+1), r_map))\n        } else {\n          r_map[c_loc[1], c_loc[2]] &lt;- \"o\"\n        }\n      }\n    }\n  }\n  return(list(r_map, overflow = FALSE))\n}\n\nAs before, we loop through the sand units until the overflow condition is met.\n\nloop_count &lt;- 0\ntemp &lt;- list(rock_map, overflow = FALSE)\n\nwhile (temp[[\"overflow\"]] == FALSE & loop_count &lt; 50000) {\n  loop_count &lt;- loop_count + 1\n  temp &lt;- falling_sand2(c(1, 500-num_cols+3), temp[[1]])\n}\n\nprint(paste(\"Sand bags dropped = \", loop_count-1))\n\n[1] \"Sand bags dropped =  25402\""
  },
  {
    "objectID": "posts/2023-08-09_aoc_Day14_FallingSand/2023-08-09_aoc_Day14_FallingSand.html#summary",
    "href": "posts/2023-08-09_aoc_Day14_FallingSand/2023-08-09_aoc_Day14_FallingSand.html#summary",
    "title": "Advent of Code Day 14: Falling Sand",
    "section": "Summary",
    "text": "Summary\nOverall, the Day 14 puzzle required a lot of work to read the input file, figure out how to create the map and then figure out the number of falling sand units. The recursive function worked well for determining the resting location for each sand unit. One source of frustration was keeping track of the x coordinate as column and y coordinate as row since the order of parameters is reversed in the matrix location.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-08-28\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.353 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P dplyr       * 1.1.2   2023-04-20 [?] CRAN (R 4.2.3)\n P ggplot2     * 3.4.2   2023-04-03 [?] CRAN (R 4.2.3)\n P purrr       * 1.0.1   2023-01-10 [?] CRAN (R 4.2.3)\n P readr       * 2.1.4   2023-02-10 [?] CRAN (R 4.2.3)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.5.0   2022-12-02 [?] CRAN (R 4.2.2)\n P tidyr       * 1.3.0   2023-01-24 [?] CRAN (R 4.2.3)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpuAnuOC/renv-use-libpath-934370a1a3b\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/R/cache/R/renv/sandbox/R-4.2/x86_64-w64-mingw32/30182023\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-09-01_aoc_Day15_DistressBeacon/2023-09-01_aoc_Day15_DistressBeacon.html",
    "href": "posts/2023-09-01_aoc_Day15_DistressBeacon/2023-09-01_aoc_Day15_DistressBeacon.html",
    "title": "Advent of Code Day 15: Distress Beacon",
    "section": "",
    "text": "Locating the position of a distress beacon using Manhattan distance from sensors"
  },
  {
    "objectID": "posts/2023-09-01_aoc_Day15_DistressBeacon/2023-09-01_aoc_Day15_DistressBeacon.html#introduction",
    "href": "posts/2023-09-01_aoc_Day15_DistressBeacon/2023-09-01_aoc_Day15_DistressBeacon.html#introduction",
    "title": "Advent of Code Day 15: Distress Beacon",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 15. The locations for sensors and the closest beacon to each sensor is provided. None of the beacons identified are in distress. The goal is to use this data to find to location of the beacon sending the distress signal."
  },
  {
    "objectID": "posts/2023-09-01_aoc_Day15_DistressBeacon/2023-09-01_aoc_Day15_DistressBeacon.html#loading-the-input-file",
    "href": "posts/2023-09-01_aoc_Day15_DistressBeacon/2023-09-01_aoc_Day15_DistressBeacon.html#loading-the-input-file",
    "title": "Advent of Code Day 15: Distress Beacon",
    "section": "Loading the input file",
    "text": "Loading the input file\nThe data is read into strings using read_lines from the readr package which has the handy option to ignore the blank rows. In addition, any white space before or after the packet is removed using str_trim.\n\nlibrary(readr)\nlibrary(stringr)\nlibrary(purrr)\n\nfilepath &lt;- here::here(\"./posts/data/aoc/day15_input.txt\")\n\nbeacons &lt;- read_lines(filepath, skip_empty_rows = TRUE)\nbeacons &lt;- str_trim(beacons)\nhead(beacons)\n\n[1] \"Sensor at x=3289936, y=2240812: closest beacon is at x=3232809, y=2000000\"\n[2] \"Sensor at x=30408, y=622853: closest beacon is at x=-669401, y=844810\"    \n[3] \"Sensor at x=3983196, y=3966332: closest beacon is at x=3232807, y=4625568\"\n[4] \"Sensor at x=929672, y=476353: closest beacon is at x=-669401, y=844810\"   \n[5] \"Sensor at x=1485689, y=3597734: closest beacon is at x=1951675, y=3073734\"\n[6] \"Sensor at x=69493, y=1886070: closest beacon is at x=-669401, y=844810\"   \n\n\nThe only important information are the numbers indicating the sensor and closest beacon positions. We strip the other characters using some stringr functions and then convert each line to a numeric vector.\n\nlocs &lt;- str_remove(beacons, \"Sensor at \") |&gt;\n  str_replace(\": closest beacon is at \", \" \") |&gt;\n  str_replace_all(\"x=\", \"\") |&gt;\n  str_replace_all(\", y=\", \" \") |&gt;\n  str_split(\" \")\n\nlocs &lt;- map(locs, \\(x) as.numeric(x))\nhead(locs)\n\n[[1]]\n[1] 3289936 2240812 3232809 2000000\n\n[[2]]\n[1]   30408  622853 -669401  844810\n\n[[3]]\n[1] 3983196 3966332 3232807 4625568\n\n[[4]]\n[1]  929672  476353 -669401  844810\n\n[[5]]\n[1] 1485689 3597734 1951675 3073734\n\n[[6]]\n[1]   69493 1886070 -669401  844810"
  },
  {
    "objectID": "posts/2023-09-01_aoc_Day15_DistressBeacon/2023-09-01_aoc_Day15_DistressBeacon.html#part-one-searching-one-y-location",
    "href": "posts/2023-09-01_aoc_Day15_DistressBeacon/2023-09-01_aoc_Day15_DistressBeacon.html#part-one-searching-one-y-location",
    "title": "Advent of Code Day 15: Distress Beacon",
    "section": "Part One: Searching one y location",
    "text": "Part One: Searching one y location\nThe goal of part one is to focus on one y location (y = 2000000) and determine all of the x locations where the distress beacon cannot be located. These locations can be determined from the input data considering that the sensors are identifying the close beacon and, therefore, the locations closer cannnot contain a beacon. Two helper functions are set up first to measure the Manhattan distance between a sensor and a beacon and to measure the distance between a sensor and the line forming the y location.\n\nfind_beacon_distance &lt;- function(b_vctr) {\n  dist &lt;- abs(b_vctr[1:2] - b_vctr[3:4])\n  dist &lt;- sum(dist)\n  return(dist)\n}\nfind_dist_from_y &lt;- function(b_vctr, y = 10) {\n  dist &lt;- abs(b_vctr[2] - y)\n  return(dist)\n}\n\nNext, we loop through the sensor-beacon locations to find the where the distress beacon cannot be located. Initially, I saved all of the locations in a list but it becomes a very large amount of locations quickly. The “not” locations were in a consecutive sequence so it is only needed to save the starting and ending locations.\n\nnot_beacon &lt;- list()\ny_loc &lt;- 2000000\n\nfor (i in 1:length(locs)){\n  b_dist &lt;- find_beacon_distance(locs[[i]])\n  y_dist &lt;- find_dist_from_y(locs[[i]], y_loc)\n  \n  if (b_dist &gt; y_dist) {\n    start_loc &lt;- locs[[i]][1] - (b_dist - y_dist)\n    end_loc &lt;- locs[[i]][1] + (b_dist - y_dist)\n    not_beacon &lt;- c(not_beacon, list(c(start_loc, end_loc)))\n    #this took too long\n    # for (j in start_loc:end_loc) {\n    #   if (!(list(c(j, y_loc)) %in% not_beacon)) not_beacon &lt;- c(not_beacon, list(c(j, y_loc)))\n    # }\n  }\n}\nhead(not_beacon)\n\n[[1]]\n[1] 3232809 3347063\n\n[[2]]\n[1]  485789 1373555\n\n[[3]]\n[1] -1596731  1735717\n\n[[4]]\n[1] 2909111 3153777\n\n[[5]]\n[1]  811351 1603969\n\n[[6]]\n[1]  768183 2801607\n\n\nIt is easier to count the number of locations if the start/end locations are listed in order. The reason is that the location spans can overlap so we don’t want to count a specific location more than once.\n\nnot_beacon &lt;- not_beacon[order(map_vec(not_beacon, ~ .[[1]]))]\nhead(not_beacon)\n\n[[1]]\n[1] -1596731  1735717\n\n[[2]]\n[1]  485789 1373555\n\n[[3]]\n[1]  768183 2801607\n\n[[4]]\n[1]  811351 1603969\n\n[[5]]\n[1] 1811141 3232809\n\n[[6]]\n[1] 2053947 2468477\n\n\nThe “not beacon” locations are then processed to find the total number. Anticipating part two, the interval for these positions is saved rather than counting the total number of positions.\n\ncur_loc &lt;- not_beacon[[1]][1]-1\nintervals &lt;- list()\nq &lt;- 0\n\nfor (i in 1:length(not_beacon)) {\n  if (cur_loc &gt; not_beacon[[i]][2]) {\n    #skip\n  } else if (cur_loc &lt; not_beacon[[i]][1]) {\n    cur_loc &lt;- not_beacon[[i]][2]\n    q &lt;- q + 1\n    intervals[[q]] &lt;- c(not_beacon[[i]][1], not_beacon[[i]][2])\n  } else if (cur_loc == not_beacon[[i]][1]) {\n    cur_loc &lt;- not_beacon[[i]][2]\n    intervals[[q]] &lt;- c(intervals[[q]][1], not_beacon[[i]][2])\n  } else {\n    #current point is between start and end point\n    cur_loc &lt;- not_beacon[[i]][2]\n    intervals[[q]] &lt;- c(intervals[[q]][1], not_beacon[[i]][2])\n  }\n}\nintervals\n\n[[1]]\n[1] -1596731  4679191\n\n\nThe total number of positions is found by the difference between the start and end of the interval. We also need to subtract any beacons located on the line (which will also be within the interval).\n\nbeacon_yloc &lt;- locs[map_lgl(locs, ~ .[[4]] == y_loc)]\npaste(\"The total number of positions where the beacon cannot be located:\", \n      intervals[[1]][2] - intervals[[1]][1] - length(unique(map_vec(beacon_yloc, ~ .[[3]]))))\n\n[1] \"The total number of positions where the beacon cannot be located: 6275921\""
  },
  {
    "objectID": "posts/2023-09-01_aoc_Day15_DistressBeacon/2023-09-01_aoc_Day15_DistressBeacon.html#part-two-find-the-distress-beacon-location",
    "href": "posts/2023-09-01_aoc_Day15_DistressBeacon/2023-09-01_aoc_Day15_DistressBeacon.html#part-two-find-the-distress-beacon-location",
    "title": "Advent of Code Day 15: Distress Beacon",
    "section": "Part Two: Find the distress beacon location",
    "text": "Part Two: Find the distress beacon location\nAs anticipated, we now need to find the location of the distress location. Our device has determined the location is between 0 and 4000000 in both the x and y directions. We can use mostly the same code developed in Part One but now we need to loop through many y “lines”. I did this part in one script which is not the best way but it worked after a bit of troubleshooting. It is important to not aggregate data that is not needed to answer the question. The critical piece of information are the intervals. The y location with 2 intervals separated by one position identifies the location of the distress beacon.\n\ny_lo &lt;- 0\ny_hi &lt;- 4000000\nall_intervals &lt;- list()\n\n\nfor (j in y_lo:y_hi) {\n  not_beacon &lt;- list()\n  for (i in 1:length(locs)){\n    b_dist &lt;- find_beacon_distance(locs[[i]])\n    y_dist &lt;- find_dist_from_y(locs[[i]], j)\n    \n    if (b_dist &gt; y_dist) {\n      #print(i)\n      start_loc &lt;- locs[[i]][1] - (b_dist - y_dist)\n      end_loc &lt;- locs[[i]][1] + (b_dist - y_dist)\n      not_beacon &lt;- c(not_beacon, list(c(start_loc, end_loc)))\n    }\n  }\n  not_beacon &lt;- not_beacon[order(sapply(not_beacon, '[[', 1))]\n  \n  cur_loc &lt;- not_beacon[[1]][1]-1\n  intervals &lt;- list()\n  q &lt;- 0\n  \n  for (k in 1:length(not_beacon)) {\n    if (cur_loc &gt; not_beacon[[k]][2]) {\n      #skip\n    } else if (cur_loc &lt; not_beacon[[k]][1]) {\n      cur_loc &lt;- not_beacon[[k]][2]\n      q &lt;- q + 1\n      intervals[[q]] &lt;- c(not_beacon[[k]][1], not_beacon[[k]][2])\n    } else if (cur_loc == not_beacon[[k]][1]) {\n      cur_loc &lt;- not_beacon[[k]][2]\n      intervals[[q]] &lt;- c(intervals[[q]][1], not_beacon[[k]][2])\n    } else {\n      #current point is between start and end point\n      cur_loc &lt;- not_beacon[[k]][2]\n      intervals[[q]] &lt;- c(intervals[[q]][1], not_beacon[[k]][2])\n    }\n  }\n  if (length(intervals) &gt; 1) {\n    all_intervals &lt;- list(intervals, j)\n  }\n}\n\nThe solution to Part Two asks for the x location multiplied by 4000000 plus the y location.\n\nprint(paste(\"The location of the distress beacon is: x =\", all_intervals[[1]][[1]][2] + 1, \"y =\", all_intervals[[2]]))\n\n[1] \"The location of the distress beacon is: x = 2936793 y = 3442119\"\n\nprint(paste(\"The Part Two solution is:\",\n            format((all_intervals[[1]][[1]][2] + 1) * 4000000 + all_intervals[[2]], scientific = FALSE)))\n\n[1] \"The Part Two solution is: 11747175442119\""
  },
  {
    "objectID": "posts/2023-09-01_aoc_Day15_DistressBeacon/2023-09-01_aoc_Day15_DistressBeacon.html#summary",
    "href": "posts/2023-09-01_aoc_Day15_DistressBeacon/2023-09-01_aoc_Day15_DistressBeacon.html#summary",
    "title": "Advent of Code Day 15: Distress Beacon",
    "section": "Summary",
    "text": "Summary\nFiguring out the logic to find the distress beacon from the input data was the main challenge for the Day 15 challenge. Using lists and some of the purrr map functions helped streamline the code. Breaking out some of the last script into helper functions would have helped with readability and troubleshooting. I needed to avoid aggregating and storing intermediate information in order to loop through all 4000000 lines in Part Two.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-09-01\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.353 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P purrr       * 1.0.1   2023-01-10 [?] CRAN (R 4.2.3)\n P readr       * 2.1.4   2023-02-10 [?] CRAN (R 4.2.3)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.5.0   2022-12-02 [?] CRAN (R 4.2.2)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/Rtmpk9CTa6/renv-use-libpath-2418356e68d2\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/R/cache/R/renv/sandbox/R-4.2/x86_64-w64-mingw32/30182023\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-10-17_aoc_Day16_BitOps/2023-10-17_aoc_Day16_BitOps.html",
    "href": "posts/2023-10-17_aoc_Day16_BitOps/2023-10-17_aoc_Day16_BitOps.html",
    "title": "Advent of Code Day 16: BitOps and Hashtables",
    "section": "",
    "text": "Finding the sequence of valves to open to maximize the pressure relief"
  },
  {
    "objectID": "posts/2023-10-17_aoc_Day16_BitOps/2023-10-17_aoc_Day16_BitOps.html#introduction",
    "href": "posts/2023-10-17_aoc_Day16_BitOps/2023-10-17_aoc_Day16_BitOps.html#introduction",
    "title": "Advent of Code Day 16: BitOps and Hashtables",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 16. At the location of the distress beacon from Day 15, a device exists to scan the cave (and with elephants). The scan reveals a series of pipes and pressure relief valves. Also, each pipe has a specific flow rate and network of tunnels provides access to all of the valves. The goal is to determine the sequence of valves to open to maximize the pressure release in a given amount of time. It takes 1 min to move through a single tunnel to a new valve location and another minute to open the valve."
  },
  {
    "objectID": "posts/2023-10-17_aoc_Day16_BitOps/2023-10-17_aoc_Day16_BitOps.html#loading-the-input-file",
    "href": "posts/2023-10-17_aoc_Day16_BitOps/2023-10-17_aoc_Day16_BitOps.html#loading-the-input-file",
    "title": "Advent of Code Day 16: BitOps and Hashtables",
    "section": "Loading the input file",
    "text": "Loading the input file\nThe input file contains one text line for each valve with its flow rate and which valves are connected by taking different tunnels from this valve. The file is read using the read_lines from the readr package. Here we just load the example data because the analysis using this code for Part Two takes about 2 hours to run.\n\nlibrary(readr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(bitops)\n\n#filepath &lt;- here::here(\"./posts/data/aoc/day16_input.txt\")\nfilepath &lt;- here::here(\"./posts/data/aoc/day16_test.txt\")\n\nvalves &lt;- read_lines(filepath, skip_empty_rows = TRUE)\nvalves &lt;- str_trim(valves)\nhead(valves)\n\n[1] \"Valve AA has flow rate=0; tunnels lead to valves DD, II, BB\" \n[2] \"Valve BB has flow rate=13; tunnels lead to valves CC, AA\"    \n[3] \"Valve CC has flow rate=2; tunnels lead to valves DD, BB\"     \n[4] \"Valve DD has flow rate=20; tunnels lead to valves CC, AA, EE\"\n[5] \"Valve EE has flow rate=3; tunnels lead to valves FF, DD\"     \n[6] \"Valve FF has flow rate=0; tunnels lead to valves EE, GG\"     \n\n\nA parse_valves function is defined to parse the required values from each input file line into a list.\n\nparse_valves &lt;- function(valve) {\n  temp &lt;- str_split(valve, \";\")\n  label &lt;- str_split(temp[[1]][1], \" \")[[1]][2]\n  flow &lt;- str_split(temp[[1]][1], \" \")[[1]][5]\n  flow &lt;- as.numeric(str_split(flow, \"=\")[[1]][2])\n  tunnels &lt;- str_split(temp[[1]][2], \"valve[s]?\")\n  tunnels &lt;- str_split(str_trim(tunnels[[1]][2]),\", \")\n  return(c(label = label, flow = flow, tunnels = tunnels))\n}\n\nThe map function from purrr is used to apply the parse_valves function to each of the input file lines. The result is a nested list structure.\n\nparsed &lt;- map(valves, parse_valves)\nhead(parsed, 3)\n\n[[1]]\n[[1]]$label\n[1] \"AA\"\n\n[[1]]$flow\n[1] 0\n\n[[1]]$tunnels\n[1] \"DD\" \"II\" \"BB\"\n\n\n[[2]]\n[[2]]$label\n[1] \"BB\"\n\n[[2]]$flow\n[1] 13\n\n[[2]]$tunnels\n[1] \"CC\" \"AA\"\n\n\n[[3]]\n[[3]]$label\n[1] \"CC\"\n\n[[3]]$flow\n[1] 2\n\n[[3]]$tunnels\n[1] \"DD\" \"BB\""
  },
  {
    "objectID": "posts/2023-10-17_aoc_Day16_BitOps/2023-10-17_aoc_Day16_BitOps.html#part-one-one-person",
    "href": "posts/2023-10-17_aoc_Day16_BitOps/2023-10-17_aoc_Day16_BitOps.html#part-one-one-person",
    "title": "Advent of Code Day 16: BitOps and Hashtables",
    "section": "Part One: One person",
    "text": "Part One: One person\nIn part one, you need to relieve as much pressure as possible in 30 min. You start at valve AA and all of the valves are closed. It takes 1 min to open a valve and 1 min to take a tunnel to another valve. Two helper functions are created to check the flow for a specific valve and check the valves accessible through connecting tunnels from the current valve.\n\ncheck_flow &lt;- function(parsed_list, valve_label) {\n  parsed_list[map_lgl(parsed_list, ~.x$label == valve_label)][[1]]$flow\n}\n\ncheck_tunnels &lt;- function(parsed_list, valve_label) {\n  parsed_list[map_lgl(parsed_list, ~.x$label == valve_label)][[1]]$tunnels\n}\n\nMany of the valves have zero flow and, therefore, there is no benefit to opening that valve. We calculate the distances between valves with non-zero flow only and store in list called dists. Each list element contains a character vector with the starting valve, ending valve and distance in min.\n\ndists &lt;- list()\nnonempty &lt;- list()\nvisited &lt;- list()\n\nfor (i in 1:length(parsed)) {\n  if ((parsed[[i]]$label != \"AA\") & (parsed[[i]]$flow == 0)) {\n    next\n  }\n  if ((parsed[[i]]$label != \"AA\")){\n    nonempty &lt;- c(nonempty, parsed[[i]]$label)\n  }\n  \n  dists &lt;- c(dists, list(c(parsed[[i]]$label, 0)))\n  visited &lt;- parsed[[i]]$label\n  \n  queue &lt;- list(list(parsed[[i]]$label, 0))\n  \n  while (length(queue) &gt; 0) {\n    position &lt;- queue[[1]][[1]]\n    distance &lt;- queue[[1]][[2]]\n    queue &lt;- queue[-1]\n    tunnels &lt;- check_tunnels(parsed, position)\n    for (neighbor in tunnels) {\n      if (neighbor %in% visited) {\n        next\n      }\n      visited &lt;- c(visited, neighbor)\n      flow &lt;- check_flow(parsed, neighbor)\n      if (flow &gt; 0) {\n        dists &lt;- c(dists, list(c(parsed[[i]]$label, neighbor, distance + 1)))\n      }\n      queue &lt;- c(queue, list(list(neighbor, distance + 1)))\n    }\n  }\n}\n\ndists &lt;- dists[map_lgl(dists, function(x) length(x) == 3)]\nhead(dists)\n\n[[1]]\n[1] \"AA\" \"DD\" \"1\" \n\n[[2]]\n[1] \"AA\" \"BB\" \"1\" \n\n[[3]]\n[1] \"AA\" \"CC\" \"2\" \n\n[[4]]\n[1] \"AA\" \"EE\" \"2\" \n\n[[5]]\n[1] \"AA\" \"JJ\" \"2\" \n\n[[6]]\n[1] \"AA\" \"HH\" \"5\" \n\n\nA depth-first search is applied to find the optimal sequence for opening the valves. A bitmask is used to track the status of the valves (with non-zero flows) with 0 for closed and 1 for open.\n\ndfs &lt;- function(time, valve, bitmask) {\n  \n  #check cache/hash enviroment\n  cache_index &lt;- cache2[[paste(time, valve, bitmask)]]\n  if (!is.null(cache_index)) return(cache_index)\n  \n  maxval &lt;- 0\n  dist_valve &lt;- dists[map_lgl(dists, function(x) x[[1]] == valve)]\n  \n  for (i in seq_along(dist_valve)) {\n    \n    bit &lt;- bitShiftL(1, which(nonempty == dist_valve[[i]][2])-1)\n\n    if (bitAnd(bitmask, bit)) {\n      next\n    }\n\n    remtime &lt;- time - as.numeric(dist_valve[[i]][3]) - 1\n\n    if (remtime &lt;= 0) {\n      next\n    }\n    \n    maxval &lt;- max(maxval, dfs(remtime, dist_valve[[i]][2], bitOr(bitmask, bit)) + check_flow(parsed, dist_valve[[i]][2]) * remtime)\n  }\n  \n  cache2[[paste(time, valve, bitmask)]] &lt;&lt;- maxval\n  return(maxval)\n}\n\nA hash environment is created to cache the pressure release for a specific time, valve and bitmask. This hash environment significantly accelerates the calculations especially when you get to Part Two.\n\n#hash table environment\ncache2 &lt;- new.env(hash = TRUE)\n\nstart &lt;- Sys.time()\ndfs(30, \"AA\", 0)\n\n[1] 1651\n\nend &lt;- Sys.time()\n\nend-start\n\nTime difference of 0.1562989 secs"
  },
  {
    "objectID": "posts/2023-10-17_aoc_Day16_BitOps/2023-10-17_aoc_Day16_BitOps.html#part-two-one-person-and-an-elephant",
    "href": "posts/2023-10-17_aoc_Day16_BitOps/2023-10-17_aoc_Day16_BitOps.html#part-two-one-person-and-an-elephant",
    "title": "Advent of Code Day 16: BitOps and Hashtables",
    "section": "Part Two: One person and an elephant!",
    "text": "Part Two: One person and an elephant!\nYou can teach an elephant how to open the valves in 4 min! Can you release more pressure in the remaining 26 min than working alone for 30 min? Here we use the same depth-first search function but for both the person and the elephant. The bitXor function is used to provide the opposite valve configuration for the elephant.\n\nb &lt;- bitShiftL(1, length(nonempty)) - 1\nm &lt;- 0\n\ncache2 &lt;- new.env(hash = TRUE)\n\nstart &lt;- Sys.time()\nfor (i in 1:(floor((b + 1) / 2))) {\n  m &lt;- max(m, dfs(26, \"AA\", i-1) + dfs(26, \"AA\", bitXor(b, (i-1))))\n  #if (!(i %% 10)) print(paste(i, Sys.time()))\n}\nend &lt;- Sys.time()\nend-start\n\nTime difference of 0.5186789 secs\n\nm\n\n[1] 1707"
  },
  {
    "objectID": "posts/2023-10-17_aoc_Day16_BitOps/2023-10-17_aoc_Day16_BitOps.html#summary",
    "href": "posts/2023-10-17_aoc_Day16_BitOps/2023-10-17_aoc_Day16_BitOps.html#summary",
    "title": "Advent of Code Day 16: BitOps and Hashtables",
    "section": "Summary",
    "text": "Summary\nI found Part Two was very challenging to execute in a reasonable time with the full input data set. The hash environment was essential to be able to find the answer in a somewhat reasonable time of about 2 hours. Interestingly, similar Python code executes in a matter of a couple minutes!\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-10-27\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.433 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P bitops      * 1.0-7   2021-04-24 [?] CRAN (R 4.2.0)\n P purrr       * 1.0.1   2023-01-10 [?] CRAN (R 4.2.3)\n P readr       * 2.1.4   2023-02-10 [?] CRAN (R 4.2.3)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.5.0   2022-12-02 [?] CRAN (R 4.2.2)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpmUjaWp/renv-use-libpath-337473dc6026\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/R/cache/R/renv/sandbox/R-4.2/x86_64-w64-mingw32/30182023\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2024-02-29_aoc_Day17_Tetris/2024-02-29_aoc_Day17_Tetris.html",
    "href": "posts/2024-02-29_aoc_Day17_Tetris/2024-02-29_aoc_Day17_Tetris.html",
    "title": "Advent of Code Day 17: Coordinates as Complex Numbers",
    "section": "",
    "text": "Simulating falling rocks with different shapes (similar to the game Tetris)"
  },
  {
    "objectID": "posts/2024-02-29_aoc_Day17_Tetris/2024-02-29_aoc_Day17_Tetris.html#introduction",
    "href": "posts/2024-02-29_aoc_Day17_Tetris/2024-02-29_aoc_Day17_Tetris.html#introduction",
    "title": "Advent of Code Day 17: Coordinates as Complex Numbers",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 17. You arrive in a tall, narrow cavern with different shape rocks falling down. If you can’t figure out which rocks will drop next, you will be crushed. The rocks have the following shapes (with ‘#’ meaning rock and a ‘.’ meaning a space:\n####\n\n.#.\n###\n.#.\n\n..#\n..#\n###\n\n#\n#\n#\n#\n\n##\n##\nThe rocks fall in the order as shown from top to bottom. After the fifth rock has fallen, the sequence starts again in the same order. The rocks are pushed left or right by jets of gas coming from the cavern walls. The jets follow a sequence as shown below.\n&gt;&gt;&gt;&lt;&lt;&gt;&lt;&gt;&gt;&lt;&lt;&lt;&gt;&gt;&lt;&gt;&gt;&gt;&lt;&lt;&lt;&gt;&gt;&gt;&lt;&lt;&lt;&gt;&lt;&lt;&lt;&gt;&gt;&lt;&gt;&gt;&lt;&lt;&gt;&gt;\nA ‘&gt;’ means the gas jet pushes the rock to the right and a ‘&lt;’ means the gas jet pushes the rock to the right. Once the end is reached, the sequence starts over again starting at the beginning. The cavern is exactly 7 units wide. A rock appears so that its left edge is two units away from the left cavern wall and 3 units from the highest rock (or the floor if no rocks have dropped). After the rock appears, it alternates between being pushed by a gas jet and then falling one unit down. If the rock is blocked from moving left or right due to another rock or the cavern wall, the rock stays at its current location. If the rock is blocked from moving down due to another rock or the cavern floor, the rock becomes fixed at its current location and part of a solid rock structure in the cavern. Once the rock becomes fixed, a new rock appears at the top of the cavern.\nThe goal is to determine the height of the solid rock structure in the cavern after a specific number of rocks have fallen. In Part One, you need to determine the height after 2022 rocks have fallen. In Part Two, you need to determine the height after one trillion rocks have fallen."
  },
  {
    "objectID": "posts/2024-02-29_aoc_Day17_Tetris/2024-02-29_aoc_Day17_Tetris.html#loading-the-input-file",
    "href": "posts/2024-02-29_aoc_Day17_Tetris/2024-02-29_aoc_Day17_Tetris.html#loading-the-input-file",
    "title": "Advent of Code Day 17: Coordinates as Complex Numbers",
    "section": "Loading the input file",
    "text": "Loading the input file\nThe input file contains only the jet sequence which is a single string made up of ‘&gt;’ and ‘&lt;’ characters. The input file is much longer than in the example shown above. The string is split into an array of single characters using the str_split_1 function from the stringr package.\n\nlibrary(readr)\nlibrary(stringr)\nlibrary(purrr)\n\nfilepath &lt;- here::here(\"./posts/data/aoc/day17_input.txt\")\n#filepath &lt;- here::here(\"./posts/data/aoc/day17_test.txt\")\n\ninput_jet &lt;- read_lines(filepath, skip_empty_rows = TRUE)\ninput_jet &lt;- str_trim(input_jet) |&gt; str_split_1(\"\")\n\nlength(input_jet)\n\n[1] 10091\n\n\nAt first, I tried representing the rocks using the characters as shown however this became overly complex programmatically. The better way to represent the rocks is numerically according to the x and y coordinates. A further optimization can be realized if you use complex numbers where the real part represents the x coordinate and the imaginary part represents the y coordinate. I learned this from the Day 17 solution posted by hyper neutrino. In fact, much of the solution presented below is based upon his Python implementation. The five types of rocks represented as complex numbers are shown below.\n\nrocks &lt;- list(\n  c(0, 1, 2, 3),\n  c(1, 1i, 1+1i, 2+1i, 1+2i),\n  c(0, 1, 2, 2+1i, 2+2i),\n  c(0, 1i, 2i, 3i),\n  c(0, 1, 1i, 1+1i)\n)\n\nThe jet sequence is also best represented numerically which can be accomplished with a single line of code. A ‘&gt;’ or shift to the right is changed to a 1 and a ‘&lt;’ or shift to the left is change to a -1. The map_int function from the purrr package is used to transform the character array into a numerical list.\n\njets &lt;- map_int(input_jet, \\(x) ifelse(x == \"&gt;\", 1, -1))"
  },
  {
    "objectID": "posts/2024-02-29_aoc_Day17_Tetris/2024-02-29_aoc_Day17_Tetris.html#part-one",
    "href": "posts/2024-02-29_aoc_Day17_Tetris/2024-02-29_aoc_Day17_Tetris.html#part-one",
    "title": "Advent of Code Day 17: Coordinates as Complex Numbers",
    "section": "Part One",
    "text": "Part One\nNow, we need to determine the height of the solid rock structure after 2022 rocks have dropped into the cavern. The cavern floor and first rock are initialized. The rock position is then first updated based on the gas jet and then updated based on dropping one unit down. The set of map functions from the purrr package are used extensively to update the rock coordinates in a concise way. The list containing the solid structure is occasionally updated to just the top 200 coordinates in order to optimize the speed of execution.\n\n# initialize the solid as the cavern floor\nsolid &lt;- map(0:6, \\(x) x-1i)\n\nheight &lt;- 0\nrc &lt;- 0 #rock count\nri &lt;- 0 #rock index\nlim &lt;- 200 #limit the amount of data stored for the solid\n\n# update position of the first rock\nrock &lt;- map(rocks[[ri+1]], \\(x) x + 2 + (height + 3) * 1i)\n\nwhile (rc &lt; 2022) {\n  for (jet in jets) {\n\n    # update the rock position due to the gas jet\n    moved &lt;- map(rock, \\(x) x + jet)\n    blocked &lt;- any(moved %in% solid)\n    if (all(map_lgl(moved, \\(x) (Re(x) &lt; 7) & (Re(x) &gt;= 0))) & !blocked) {\n      rock &lt;- moved\n    }\n    # update the rock position to drop one unit\n    moved &lt;- map(rock, \\(x) x - 1i)\n    # if the rock is blocked from moving down then it becomes part of the solid\n    if (any(moved %in% solid)) {\n      solid &lt;- c(solid, rock[!(rock %in% solid)])\n      rc &lt;- rc + 1\n      height &lt;- max(map_vec(solid, \\(x) Im(x))) + 1\n      if (rc &gt;= 2022) break\n      ri &lt;- (ri + 1) %% 5\n      rock &lt;- map(rocks[[ri+1]], \\(x) x + 2 + (height + 3) * 1i)\n    } else {\n      # if rock is not blocked then update the position\n      rock &lt;- moved\n    }\n    # keep the solid list from taking up too much memory\n    if (length(solid) &gt; lim) solid &lt;- solid[(length(solid)-(lim+1)):length(solid)]\n  }\n}\n\nheight\n\n[1] 3117\n\n\nAfter 2022 rocks have fallen, the height of the solid rock structure is 3117 units."
  },
  {
    "objectID": "posts/2024-02-29_aoc_Day17_Tetris/2024-02-29_aoc_Day17_Tetris.html#part-two",
    "href": "posts/2024-02-29_aoc_Day17_Tetris/2024-02-29_aoc_Day17_Tetris.html#part-two",
    "title": "Advent of Code Day 17: Coordinates as Complex Numbers",
    "section": "Part Two",
    "text": "Part Two\nApparently, the elephants are not impressed by the simulation of 2022 rocks and want to see the results after dropping a trillion rocks. Of course, executing the same loop for a trillion rocks would take much too long so we need to find a shortcut. The shortcut is to look for a repeating sequence as the rocks are dropped. To find the start of the repeating sequence, we are looking for three conditions to be met: the jet index, the rock index and the top of the solid rock structure are the same values.\n\nsolid &lt;- map(0:6, \\(x) x-1i)\nheight &lt;- 0\nnumblock &lt;- 1e12\n\n# store the rock count and height at each jet index, rock index and top of solid\nseen &lt;- list()\n\n# summarize the top of the solid rock structure\nsummarize_solid &lt;- function() {\n  o &lt;- rep(-20, 7)\n  for (x in solid) {\n    r &lt;- Re(x)\n    i &lt;- Im(x)\n    o[r+1] &lt;- max(o[r+1], i)\n  }\n  top &lt;- max(o)\n  return(map_int(o, \\(x) x - top))\n}\n\nrc &lt;- 0\nri &lt;- 0\nlim &lt;- 200\nrock &lt;- map(rocks[[ri+1]], \\(x) x + 2 + (height + 3) * 1i)\n\nwhile (rc &lt; numblock) {\n  for (ji in seq_along(jets)) {\n    jet &lt;- jets[ji]\n    moved &lt;- map(rock, \\(x) x + jet)\n    blocked &lt;- any(moved %in% solid)\n    if (all(map_lgl(moved, \\(x) (Re(x) &lt; 7) & (Re(x) &gt;= 0))) & !blocked) {\n      rock &lt;- moved\n    }\n    moved &lt;- map(rock, \\(x) x - 1i)\n    if (any(moved %in% solid)) {\n      solid &lt;- c(solid, rock[!(rock %in% solid)])\n      rc &lt;- rc + 1\n      height &lt;- max(map_vec(solid, \\(x) Im(x))) + 1\n      if (rc &gt;= numblock) break\n      ri &lt;- (ri + 1) %% 5\n      rock &lt;- map(rocks[[ri+1]], \\(x) x + 2 + (height + 3) * 1i)\n      # create a key with the jet index, rock index and top of the solid\n      key &lt;- paste0(ji, ri, paste(summarize_solid(), collapse = ''))\n      # if the key has been seen before, calculate the offset\n      if (key %in% names(seen)) {\n        lrc &lt;- seen[[key]][1] #last rock count\n        lh &lt;- seen[[key]][2]  #last height\n        rem &lt;- numblock - rc  #remainder of rocks left\n        rep &lt;- floor(rem / (rc - lrc)) # number of repetitions of the same sequence\n        offset &lt;- rep * (height - lh)  # amount to add to the height due to repeats\n        rc &lt;- rc + (rep * (rc - lrc))  # update the rock count\n        seen &lt;- list()                 # clear the seen list\n      }\n      seen[[key]] &lt;- c(rc, height)\n    } else {\n      rock &lt;- moved\n    }\n    if (length(solid) &gt; lim) solid &lt;- solid[(length(solid)-(lim+1)):length(solid)]\n  }\n}\n\nformat(height + offset, scientific = FALSE)\n\n[1] \"1553314121019\"\n\n\nUsing the shortcut of finding the repeating sequence enables finding the height of 1553314121019 units for a trillion blocks in a little over a minute! The rocks are iterated until the repeat is found, the rock count is jumped ahead for the number of repeats that can occur without exceeding one trillion rocks and then the rocks are iterated until one trillion rocks are dropped."
  },
  {
    "objectID": "posts/2024-02-29_aoc_Day17_Tetris/2024-02-29_aoc_Day17_Tetris.html#summary",
    "href": "posts/2024-02-29_aoc_Day17_Tetris/2024-02-29_aoc_Day17_Tetris.html#summary",
    "title": "Advent of Code Day 17: Coordinates as Complex Numbers",
    "section": "Summary",
    "text": "Summary\nThe simulation of dropping a sequence of rocks into a cavern was successfully executed in R for one trillion rocks. Some nice optimizations were implemented to have concise code which executes relatively quickly. It helped to represent the rock coordinates as complex numbers. The map functions from the purrr package enabled concise code. The shortcut to find a repeating sequence using a named list in a manner analogous to a Python dictionary was the key to completing Part Two.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2024-03-01\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P purrr       * 1.0.2   2023-08-10 [?] CRAN (R 4.3.2)\n P readr       * 2.1.5   2024-01-10 [?] CRAN (R 4.3.2)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.3.2)\n P stringr     * 1.5.1   2023-11-14 [?] CRAN (R 4.3.2)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpwB7kPZ/renv-use-libpath-1e2c4fb81c94\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.3/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/R/cache/R/renv/sandbox/R-4.3/x86_64-w64-mingw32/7f66d5b3\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2024-03-11_aoc_Day18_SurfaceArea/2024-03-11_aoc_Day18_SurfaceArea.html",
    "href": "posts/2024-03-11_aoc_Day18_SurfaceArea/2024-03-11_aoc_Day18_SurfaceArea.html",
    "title": "Advent of Code Day 18: Surface Area of Lava Droplet",
    "section": "",
    "text": "Calculate surface area of a lava droplet using a breadth-first search approach"
  },
  {
    "objectID": "posts/2024-03-11_aoc_Day18_SurfaceArea/2024-03-11_aoc_Day18_SurfaceArea.html#introduction",
    "href": "posts/2024-03-11_aoc_Day18_SurfaceArea/2024-03-11_aoc_Day18_SurfaceArea.html#introduction",
    "title": "Advent of Code Day 18: Surface Area of Lava Droplet",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 18. Bits of lava are being ejected from a volcano and you are able to get a scan of a droplet. The scan contains a 3D grid of 1x1x1 cubes to approximate the shape of the lava droplet. The speed at which the droplet cools could determine whether it creates obsidian. The cooling rate will depend upon the surface area of the droplet. Surface area is determined by the number of exposed cube faces."
  },
  {
    "objectID": "posts/2024-03-11_aoc_Day18_SurfaceArea/2024-03-11_aoc_Day18_SurfaceArea.html#loading-the-input-file",
    "href": "posts/2024-03-11_aoc_Day18_SurfaceArea/2024-03-11_aoc_Day18_SurfaceArea.html#loading-the-input-file",
    "title": "Advent of Code Day 18: Surface Area of Lava Droplet",
    "section": "Loading the input file",
    "text": "Loading the input file\nThe input file contains a file where each line is a series of 3 numbers separated by a comma (e.g. “13, 6, 3”). The read_lines function from the readr package is used to read the input file.\n\nlibrary(readr)\nlibrary(stringr)\nlibrary(collections)\n\nfilepath &lt;- here::here(\"./posts/data/aoc/day18_input.txt\") #\n#filepath &lt;- here::here(\"./posts/data/aoc/day18_test.txt\") #\n\ncubes &lt;- read_lines(filepath, skip_empty_rows = TRUE)"
  },
  {
    "objectID": "posts/2024-03-11_aoc_Day18_SurfaceArea/2024-03-11_aoc_Day18_SurfaceArea.html#part-one",
    "href": "posts/2024-03-11_aoc_Day18_SurfaceArea/2024-03-11_aoc_Day18_SurfaceArea.html#part-one",
    "title": "Advent of Code Day 18: Surface Area of Lava Droplet",
    "section": "Part One",
    "text": "Part One\nWe are first asked to determine the number of cube faces which are not attached to other cubes. This number can be calculated by first determining the total possible number of faces which is the total number of cubes multiplied by 6 since a cube has 6 faces. For each cube, the coordinates for each face are determined. If the face is adjacent to another cube, the face is not part of the surface area and subtracted from the total possible number of faces.\n\n#coordinate system can be viewed as lower left corner of cube\nsides &lt;- list(c(-1,  0,  0),\n              c( 1,  0,  0),\n              c( 0, -1,  0),\n              c( 0,  1,  0),\n              c( 0,  0, -1),\n              c( 0,  0,  1))\n\n#total possible side (face) count\nsc &lt;- length(cubes) * 6\n\nfor (cube in cubes) {\n  temp &lt;- as.integer(unlist(str_split(cube, \",\")))\n\n  for (side in sides) {\n    ss &lt;- str_c(temp + side, collapse = \",\")\n    # subtract from total possible if occupied by an adjacent cube\n    if (ss %in% cubes) sc &lt;- sc - 1\n  }\n}\n\nsc\n\n[1] 4536\n\n\nThe total surface area calculated in Part One is 4536 units."
  },
  {
    "objectID": "posts/2024-03-11_aoc_Day18_SurfaceArea/2024-03-11_aoc_Day18_SurfaceArea.html#part-two",
    "href": "posts/2024-03-11_aoc_Day18_SurfaceArea/2024-03-11_aoc_Day18_SurfaceArea.html#part-two",
    "title": "Advent of Code Day 18: Surface Area of Lava Droplet",
    "section": "Part Two",
    "text": "Part Two\nThe calculation of surface area in Part One is not quite correct. This calculation does not take into account the case where the cube face is in an inner pore within the lava droplet. In this case, this face will not be accessible from outside the droplet and should not be included in the surface area calculation.\nI will use a slightly different coordinate system for the cube faces to help with the calculations. In this case, the coordinate indicates the center of the cube and each face will be 0.5 units away from the center.\n\noffsets &lt;- list(c(-0.5,    0,    0),\n                c( 0.5,    0,    0),\n                c(   0, -0.5,    0),\n                c(   0,  0.5,    0),\n                c(   0,    0, -0.5),\n                c(   0,    0,  0.5)) \n\nThe faces are stored in a list and the coordinates are stored in the list name. The minimum (mx, my, mz) and maximum (Mx, My, Mz) coordinates for the droplet are saved to understand the outer limits around the droplet.\n\nmx &lt;- my &lt;- mz &lt;- Inf\nMx &lt;- My &lt;- Mz &lt;- -Inf\n\ndroplet &lt;- list()\nfaces &lt;- list()\nn &lt;- 0\n\nfor (cube in cubes) {\n   n &lt;- n + 1\n   cell &lt;- as.numeric(unlist(str_split(cube, \",\")))\n   if (sum(cell == c(4, 5 ,6)) == 3) print(n)\n   droplet &lt;- c(droplet, list(cell))\n   \n   mx &lt;- min(mx, cell[1])\n   my &lt;- min(my, cell[2])\n   mz &lt;- min(mz, cell[3])\n   \n   Mx &lt;- max(Mx, cell[1])\n   My &lt;- max(My, cell[2])\n   Mz &lt;- max(Mz, cell[3])\n   \n   for (offset in offsets) {\n     k &lt;-paste(cell[1] + offset[1], cell[2] + offset[2], cell[3] + offset[3], collapse = \"\")\n     if (!(k %in% names(faces))) {\n       faces[[k]] &lt;- 0\n     }\n     faces[[k]] &lt;- faces[[k]] + 1\n   }\n}\n\n[1] 1717\n\n\nA breadth-first search is used to determine which coordinates are part of the air around the droplet (not one of the droplet faces). To begin the search, 1 is subtracted to the minimum coordinates and 1 is added to the maximum coordinates.\n\nmx &lt;- mx - 1\nmy &lt;- my - 1\nmz &lt;- mz - 1\n\nMx &lt;- Mx + 1\nMy &lt;- My + 1\nMz &lt;- Mz + 1\n\nThe collections package is used to create the queue (deque) for storing the coordinates to be explored by the breadth-first search. If the search coordinates are outside the min/max limits then it skips to the next coordinate. If the search coordinates are part of the droplet then it skips to the next coordinate. If the search coordinates are already in air then it skips to the next coordinate otherwise it is appended to the air.\n\nstart &lt;- Sys.time()\nq &lt;- deque(list(c(mx, my, mz)))\nair &lt;- list(c(mx, my, mz))\n\nwhile (q$size() &gt; 0) {\n  coords &lt;- unlist(q$popleft())\n\n  for (d in offsets) {\n    k &lt;- c(coords[1] + d[1] * 2, coords[2] + d[2] * 2, coords[3] + d[3] * 2)\n    \n    if (!((mx &lt;= k[1]) & (k[1] &lt;= Mx) & (my &lt;= k[2]) & (k[2] &lt;= My) & (mz &lt;= k[3]) & (k[3] &lt;= Mz))) {\n      next\n    }\n    \n    if (sum(droplet %in% list(k)) &gt; 0) next\n    if (sum(air %in% list(k)) &gt; 0) next\n  \n    air &lt;- c(air, list(k))\n    q$push(list(k))\n  }\n}\nend &lt;- Sys.time()\n\nI tried the deque function to see whether it would bring any speed advantage over a manual implementation but did not observe any significant improvement. This breadth-first search took 44 min to run in this implementation.\nA free list is used to contain all of the faces of the air cubes in the same format as for the faces of the droplet cubes. This code is basically a reformatting of the air list.\n\nfree &lt;- list()\n\nfor (a in air) {\n  for (d in offsets) {\n    k &lt;-paste(a[1] + d[1], a[2] + d[2], a[3] + d[3], collapse = \"\")\n     if (!(k %in% names(free))) {\n       free[[k]] &lt;- 0\n     }\n     free[[k]] &lt;- free[[k]] + 1\n  }\n}\n\nThe surface area of the droplet is then calculated as the cube face coordinates which are in both the droplet(faces) and in the air (free).\n\nsurface_area &lt;- 0\n\nfor (face in names(faces)) {\n  if (face %in% names(free)) {\n    surface_area &lt;- surface_area + 1\n  }\n}\n\nsurface_area\n\n[1] 2606\n\n\nThe total surface area as corrected in Part Two is 2606 units. This is a significant correction from 4536 units calculated in Part One."
  },
  {
    "objectID": "posts/2024-03-11_aoc_Day18_SurfaceArea/2024-03-11_aoc_Day18_SurfaceArea.html#summary",
    "href": "posts/2024-03-11_aoc_Day18_SurfaceArea/2024-03-11_aoc_Day18_SurfaceArea.html#summary",
    "title": "Advent of Code Day 18: Surface Area of Lava Droplet",
    "section": "Summary",
    "text": "Summary\nBreadth-first search is used to find the air around a complex droplet surface as a means of calculating surface area. The deque function from the collections package was used to manage the queue which made the code a bit cleaner but didn’t improve the speed at all.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2024-04-19\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P collections * 0.3.7   2023-01-05 [?] CRAN (R 4.3.3)\n P readr       * 2.1.5   2024-01-10 [?] CRAN (R 4.3.2)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.3.2)\n P stringr     * 1.5.1   2023-11-14 [?] CRAN (R 4.3.2)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpmWTBJB/renv-use-libpath-1f5854ca799\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.3/x86_64-w64-mingw32\n [3] C:/Users/David Zoller/AppData/Local/R/cache/R/renv/sandbox/R-4.3/x86_64-w64-mingw32/7f66d5b3\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html",
    "href": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html",
    "title": "Advent of Code Day 19: Depth-First Search",
    "section": "",
    "text": "Determine the optimal blueprint for building robots using a depth-first search algorithm"
  },
  {
    "objectID": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html#introduction",
    "href": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html#introduction",
    "title": "Advent of Code Day 19: Depth-First Search",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 19. The goal is to collect geodes using geode cracking robots. To build a geode cracking robot, you need to have obsidian which requires obsidian collecting robots. In order to build the obsidian collecting robot, you will need clay which requires clay collecting robots. Building any of the robots also requires ore which is collected using ore robots with big drills. You have multiple blueprints with different amounts of raw materials required for building the robots. You need to find which blueprint maximizes the number of geodes collected.\nBlueprint examples:\nBlueprint 1:\n  Each ore robot costs 4 ore.\n  Each clay robot costs 2 ore.\n  Each obsidian robot costs 3 ore and 14 clay.\n  Each geode robot costs 2 ore and 7 obsidian.\n\nBlueprint 2:\n  Each ore robot costs 2 ore.\n  Each clay robot costs 3 ore.\n  Each obsidian robot costs 3 ore and 8 clay.\n  Each geode robot costs 3 ore and 12 obsidian."
  },
  {
    "objectID": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html#loading-the-input-file",
    "href": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html#loading-the-input-file",
    "title": "Advent of Code Day 19: Depth-First Search",
    "section": "Loading the input file",
    "text": "Loading the input file\nEach blueprint is captured in a single line in the input file with each instruction separated by a space. The read_lines function from the readr package is used to read the input file.\n\nlibrary(readr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(tidyverse)\n\nfilepath &lt;- here::here(\"./posts/data/aoc/day19_input.txt\")\n#filepath &lt;- here::here(\"./posts/data/aoc/day19_test.txt\")\n\nrobots &lt;- read_lines(filepath, skip_empty_rows = TRUE)"
  },
  {
    "objectID": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html#data-cleaning",
    "href": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html#data-cleaning",
    "title": "Advent of Code Day 19: Depth-First Search",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nEach blueprint (line) needs to be parsed. The first step splits the string at the colon to separate the blueprint number from the rest of the blueprint instructions. The next step is to split each of the instructions using the period. The third step is to extract the all number followed by a word in each instruction. For example, “Each obsidian robot costs 3 ore and 8 clay” is extracted into “3 ore” and “8 clay”. The last step is to replace the resource type by a number specific to each type.\n\n#split blueprint number from instructions\ntemp &lt;- str_split(robots, \": \")\n#split each instruction step\nrobot_types &lt;- map(temp, \\(x) unlist(str_split(x[2], \"\\\\. \")))\n#extract the resource type and amount\nbp &lt;- map(robot_types, \\(y) map(y, \\(x) unlist(str_extract_all(x, \"\\\\d+ \\\\w+\"))))\nbp &lt;- map(bp, \\(y) map(y,  \\(x) str_replace_all(x, c(ore = \"1\", clay = \"2\", obsidian = \"3\"))))"
  },
  {
    "objectID": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html#depth-first-search",
    "href": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html#depth-first-search",
    "title": "Advent of Code Day 19: Depth-First Search",
    "section": "Depth-First Search",
    "text": "Depth-First Search\nA depth-first search is performed to find the strategy to crack the most geodes from a blueprint. A recursive function is created to perform the depth-first search. Several optimizations are implemented to improve the speed of the search. A cache is used to store the results when a prior state which has already been calculated is available. A second optimization is to limit the number of robots of a given type by the maximum spend in a turn. This maxspend is calculated in a separate function. The last optimization is to limit the recorded amount to the maximum that can be used in the remaining time. This last optimization helps with the caching since the result will be the same regardless of the amount of unusable resources available.\n\ndfs &lt;- function(bp, maxspend, cache, time, bots, amt) {\n  #return number of geodes if time has expired\n  if (time == 0) return(amt[4])\n  \n  #if previously seen then return value\n  key &lt;- paste(c(time, bots, amt), collapse = \" \")\n  cache_index &lt;- cache[[key]]\n  if (!is.null(cache_index)) return(cache_index)\n  \n  maxval &lt;- amt[4] + bots[4] * time\n  \n  for (blue in bp) {\n    for (btype in 1:length(blue)) {\n      #optimization to not exceed maxspend for a given bot type\n      if ((btype !=  4) & (bots[btype] &gt;= maxspend[btype])) next\n\n      wait &lt;- 0\n      breakFlag &lt;- FALSE\n    \n      for (i in 1:length(blue[[btype]])) {\n        temp &lt;- as.numeric(unlist(str_split(blue[[btype]][i], \" \")))\n        ramt &lt;- temp[1]\n        rtype &lt;- temp[2]\n      \n        #avoid divide by zero error\n        if (bots[rtype] == 0){\n          breakFlag &lt;- TRUE\n          break\n        }\n        wait &lt;- max(wait, ceiling((ramt - amt[rtype]) / bots[rtype]))\n        #print(c(\"wait=\", wait, breakFlag))\n      }\n      if (!breakFlag) {\n        remtime &lt;- time - wait - 1\n        if (remtime &lt;= 0) next\n        \n        bots_tmp &lt;- bots\n        amt_tmp &lt;- amt + bots * (wait + 1)\n        \n        for (i in 1:length(blue[[btype]])) {\n          temp &lt;- as.numeric(unlist(str_split(blue[[btype]][i], \" \")))\n          ramt &lt;- temp[1]\n          rtype &lt;- temp[2]\n          amt_tmp[rtype] &lt;- amt_tmp[rtype] - ramt \n        }\n        bots_tmp[btype] &lt;- bots_tmp[btype] + 1\n        #third optimization to limit amount to maximum which can be used in the remaining time\n        for (j in (1:3)) {\n          amt_tmp[j] &lt;- min(amt_tmp[j], maxspend[j] * remtime)\n        }\n        #print(paste(\"remtime= \", remtime, \"bots_tmp= \", paste(bots_tmp, collapse = \" \"), \"amt_tmp=\", paste(amt_tmp, collapse = \" \")))\n        maxval &lt;- max(maxval, dfs(bp, maxspend, cache, remtime, bots_tmp, amt_tmp))\n      }\n    }\n  }\n  cache[[key]] &lt;&lt;- maxval\n  return(maxval)\n}\n\nThe function to find the maximum spend takes the blueprint instructions for a given blueprint and calculates the maximum resource amount which can be utilized in a single turn. This result is then used for two search optimizations as described above.\n\nfind_maxspend &lt;- function(bp_lst, bp_num){\n  maxspend &lt;- c(0, 0, 0)\n  for (blue in bp_lst[[bp_num]]) {\n    for (i in 1:length(blue)) {\n      temp &lt;- as.numeric(unlist(str_split(blue[[i]], \" \")))\n      if (temp[2] == 1) {\n        maxspend[1] &lt;- max(maxspend[1], temp[1])\n      } else if (temp[2] == 2) {\n        maxspend[2] &lt;- max(maxspend[2], temp[1])\n      } else if (temp[2] == 3) {\n        maxspend[3] &lt;- max(maxspend[3], temp[1])\n      } else {\n        maxspend &lt;- maxspend\n      }\n    }\n  }\n  return(maxspend)\n}"
  },
  {
    "objectID": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html#part-1-calculation",
    "href": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html#part-1-calculation",
    "title": "Advent of Code Day 19: Depth First Search",
    "section": "Part 1 calculation",
    "text": "Part 1 calculation\nIn Part 1, you have 24 minutes to crack the most number of geodes from each blueprint. There are a total of 30 blueprints in the input file. An R environment is set up as a hash table to serve as the cache for already calculated states. The robot is initialized with one ore robot and no available resources. The total value for the puzzle solution is calculated as summation of the blueprint number multiplied by the number of geodes cracked.\n\ntotal &lt;- 0\n\nfor (i in 1:length(bp)) {\n  print(paste(i, Sys.time()))\n  cache &lt;- new.env(hash = TRUE)\n  total_time &lt;- 24\n  bots &lt;- c(1, 0, 0, 0)\n  amt &lt;- c(0, 0, 0, 0)\n  maxspend &lt;- find_maxspend(bp, i)\n  v &lt;- dfs(bp[i], maxspend, cache, total_time, bots, amt)\n  total &lt;- total + (i * v)\n}\n\n[1] \"1 2025-01-29 09:47:07.899964\"\n[1] \"2 2025-01-29 09:47:10.535221\"\n[1] \"3 2025-01-29 09:47:14.096207\"\n[1] \"4 2025-01-29 09:47:14.844179\"\n[1] \"5 2025-01-29 09:47:16.86325\"\n[1] \"6 2025-01-29 09:47:18.712507\"\n[1] \"7 2025-01-29 09:47:22.478412\"\n[1] \"8 2025-01-29 09:47:24.193512\"\n[1] \"9 2025-01-29 09:47:29.834521\"\n[1] \"10 2025-01-29 09:47:42.341058\"\n[1] \"11 2025-01-29 09:47:44.73841\"\n[1] \"12 2025-01-29 09:48:00.108386\"\n[1] \"13 2025-01-29 09:48:02.766838\"\n[1] \"14 2025-01-29 09:48:09.898301\"\n[1] \"15 2025-01-29 09:48:11.519536\"\n[1] \"16 2025-01-29 09:48:13.198434\"\n[1] \"17 2025-01-29 09:48:17.21546\"\n[1] \"18 2025-01-29 09:48:18.807573\"\n[1] \"19 2025-01-29 09:48:42.129725\"\n[1] \"20 2025-01-29 09:48:45.237107\"\n[1] \"21 2025-01-29 09:48:50.416549\"\n[1] \"22 2025-01-29 09:48:55.33532\"\n[1] \"23 2025-01-29 09:48:57.008854\"\n[1] \"24 2025-01-29 09:49:00.12405\"\n[1] \"25 2025-01-29 09:49:01.468057\"\n[1] \"26 2025-01-29 09:49:11.581361\"\n[1] \"27 2025-01-29 09:49:15.136352\"\n[1] \"28 2025-01-29 09:49:19.626489\"\n[1] \"29 2025-01-29 09:49:29.921426\"\n[1] \"30 2025-01-29 09:49:31.021614\""
  },
  {
    "objectID": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html#part-1-solution",
    "href": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html#part-1-solution",
    "title": "Advent of Code Day 19: Depth-First Search",
    "section": "Part 1 Solution",
    "text": "Part 1 Solution\nIn Part 1, you have 24 minutes to crack the most number of geodes from each blueprint. There are a total of 30 blueprints in the input file. An R environment is set up as a hash table to serve as the cache for already calculated states. The robot is initialized with one ore robot and no available resources. The total value for the puzzle solution is calculated as summation of the blueprint number multiplied by the number of geodes cracked.\n\ntotal &lt;- 0\n\nfor (i in 1:length(bp)) {\n  #print(paste(i, Sys.time()))\n  cache &lt;- new.env(hash = TRUE)\n  total_time &lt;- 24\n  bots &lt;- c(1, 0, 0, 0)\n  amt &lt;- c(0, 0, 0, 0)\n  maxspend &lt;- find_maxspend(bp, i)\n  v &lt;- dfs(bp[i], maxspend, cache, total_time, bots, amt)\n  total &lt;- total + (i * v)\n}\n\ntotal\n\n[1] 1199\n\n\nThe puzzle solution was calculated to be 1199."
  },
  {
    "objectID": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html#part-2-solution",
    "href": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html#part-2-solution",
    "title": "Advent of Code Day 19: Depth-First Search",
    "section": "Part 2 Solution",
    "text": "Part 2 Solution\nIn Part 2, the time available to crack the geodes has been increase to 32 minutes however only the first 3 blueprints are now available. The puzzle solution is calculated differently as well for Part 2. In this case, we just multiply the number of cracked geodes for each of the three blueprints together to get the total value. Increasing the total time available significantly increases the amount of time required to solve each blueprint.\n\ntotal &lt;- 1\n\nfor (i in 1:3) {\n  #print(paste(i, Sys.time()))\n  cache &lt;- new.env(hash = TRUE)\n  total_time &lt;- 32\n  bots &lt;- c(1, 0, 0, 0)\n  amt &lt;- c(0, 0, 0, 0)\n  maxspend &lt;- find_maxspend(bp, i)\n  v &lt;- dfs(bp[i], maxspend, cache, total_time, bots, amt)\n  total &lt;- total * v\n  print(Sys.time())\n}\n\n[1] \"2025-01-29 10:33:35 CST\"\n[1] \"2025-01-29 10:42:04 CST\"\n[1] \"2025-01-29 10:45:21 CST\"\n\ntotal\n\n[1] 3510\n\n\nThe puzzle solution for Part 2 was calculated as 3510."
  },
  {
    "objectID": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html#summary",
    "href": "posts/2024-08-13_aoc_Day19_DepthFirstSearch/2024-08-13_aoc_Day19_DepthFirstSearch.html#summary",
    "title": "Advent of Code Day 19: Depth-First Search",
    "section": "Summary",
    "text": "Summary\nA depth-first search algorithm was implemented to reach the optimum solution for each set of blueprint instructions. A hash environment was effectively used as a cache to reduce the search time. A couple of additional time optimizations were implemented based on this specific problem in order to not calculate scenarios which were clearly not going to result in the best solution.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2025-01-29\n pandoc   3.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.5.57 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n dplyr       * 1.1.4   2023-11-17 [1] CRAN (R 4.4.2)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.4.2)\n ggplot2     * 3.5.1   2024-04-23 [1] CRAN (R 4.4.2)\n lubridate   * 1.9.4   2024-12-08 [1] CRAN (R 4.4.2)\n purrr       * 1.0.2   2023-08-10 [1] CRAN (R 4.4.2)\n readr       * 2.1.5   2024-01-10 [1] CRAN (R 4.4.2)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.4.2)\n stringr     * 1.5.1   2023-11-14 [1] CRAN (R 4.4.2)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.4.2)\n tidyr       * 1.3.1   2024-01-24 [1] CRAN (R 4.4.2)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.4.2)\n\n [1] C:/Users/David Zoller/AppData/Local/R/win-library/4.4\n [2] C:/Program Files/R/R-4.4.2/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2025-01-31_aoc_Day20_DoublyLinkedList/2025-01-31_aoc_Day20_DoublyLinkedList.html",
    "href": "posts/2025-01-31_aoc_Day20_DoublyLinkedList/2025-01-31_aoc_Day20_DoublyLinkedList.html",
    "title": "Advent of Code Day 20: Doubly-Linked List",
    "section": "",
    "text": "Track an encryption algorithm by implementing a doubly-linked list"
  },
  {
    "objectID": "posts/2025-01-31_aoc_Day20_DoublyLinkedList/2025-01-31_aoc_Day20_DoublyLinkedList.html#introduction",
    "href": "posts/2025-01-31_aoc_Day20_DoublyLinkedList/2025-01-31_aoc_Day20_DoublyLinkedList.html#introduction",
    "title": "Advent of Code Day 20: Doubly-Linked List",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 20. The goal is to find coordinates to a grove by applying an encryption algorithm to a sequence of numbers. The coordinates are then found on the 1000th, 2000th and 3000th number away from the 0 value in the number sequence. The encryption algorithm is to move each number in the sequence forward or backward in the list according to its value. A negative value moves the number backwards and a positive value moves the number forwards. Each number in the sequence is moved in the order of the original sequence. The list is circular meaning moving past the last number in the sequence results in starting back at the beginning of the sequence and vice versa. An example is provided as such:\nInitial arrangement:\n1, 2, -3, 3, -2, 0, 4\n\n1 moves between 2 and -3:\n2, 1, -3, 3, -2, 0, 4\n\n2 moves between -3 and 3:\n1, -3, 2, 3, -2, 0, 4\n\n-3 moves between -2 and 0:\n1, 2, 3, -2, -3, 0, 4\n\n3 moves between 0 and 4:\n1, 2, -2, -3, 0, 3, 4\n\n-2 moves between 4 and 1:\n1, 2, -3, 0, 3, 4, -2\n\n0 does not move:\n1, 2, -3, 0, 3, 4, -2\n\n4 moves between -3 and 0:\n1, 2, -3, 4, 0, 3, -2\nAfter each number in the initial sequence has been mixed one time, the coordinates are found by cycling through the list starting at the 0 value. The puzzle solution is then found from the summation of the 3 coordinates corresponding to the 1000th, 2000th and 3000th number away from 0 in the mixed sequence."
  },
  {
    "objectID": "posts/2025-01-31_aoc_Day20_DoublyLinkedList/2025-01-31_aoc_Day20_DoublyLinkedList.html#loading-the-input-file",
    "href": "posts/2025-01-31_aoc_Day20_DoublyLinkedList/2025-01-31_aoc_Day20_DoublyLinkedList.html#loading-the-input-file",
    "title": "Advent of Code Day 20: Doubly-Linked List",
    "section": "Loading the input file",
    "text": "Loading the input file\nEach number in the sequence is captured in a single line of the input file. The read_lines function from the readr package is used to read the input file. The input file sequence contained 5000 numbers.\n\nlibrary(readr)\n\nfilepath &lt;- here::here(\"./posts/data/aoc/day20_input.txt\")\n#filepath &lt;- here::here(\"./posts/data/aoc/day20_test.txt\")\n\nfileseq &lt;- read_lines(filepath, skip_empty_rows = TRUE) |&gt; as.numeric()"
  },
  {
    "objectID": "posts/2025-01-31_aoc_Day20_DoublyLinkedList/2025-01-31_aoc_Day20_DoublyLinkedList.html#data-cleaning-structure",
    "href": "posts/2025-01-31_aoc_Day20_DoublyLinkedList/2025-01-31_aoc_Day20_DoublyLinkedList.html#data-cleaning-structure",
    "title": "Advent of Code Day 20: Doubly-Linked List",
    "section": "Data Cleaning / Structure",
    "text": "Data Cleaning / Structure\nA doubly-linked list structure is used to keep track of the mixed sequence. In this case, the number before and after each sequence element is recorded and updated at each step. This structure is programmatically easier to maintain rather than slicing and reforming the list each time. The initial structure is set up from the input file using the create_linked_list function shown below. Each list element contains the value (v), left or preceding element (l) and right or succeeding element (r). The modulo calculation is used to connect the beginning and end of the list to make it circular.\n\ncreate_linked_list &lt;- function(file_seq, crypt_key) {\n  lnk_lst &lt;- list()\n  for (i in 1:length(file_seq)) {\n    node &lt;- list(v = file_seq[i] * crypt_key, \n                l = ifelse(((i - 1) %% length(file_seq)) == 0, as.numeric(length(file_seq)), ((i - 1) %% length(file_seq))),\n                r = ifelse(((i + 1) %% length(file_seq)) == 0, length(file_seq), ((i + 1) %% length(file_seq))))\n    lnk_lst &lt;- c(lnk_lst, list(node))\n  }\n  return(lnk_lst)\n}"
  },
  {
    "objectID": "posts/2025-01-31_aoc_Day20_DoublyLinkedList/2025-01-31_aoc_Day20_DoublyLinkedList.html#mixing-the-sequence",
    "href": "posts/2025-01-31_aoc_Day20_DoublyLinkedList/2025-01-31_aoc_Day20_DoublyLinkedList.html#mixing-the-sequence",
    "title": "Advent of Code Day 20: Doubly-Linked List",
    "section": "Mixing the Sequence",
    "text": "Mixing the Sequence\nAs mentioned previously, each element is moved forwards or backwards in the sequence according to its value. This mixing is performed in two steps: removing the element (node) from its current position and inserting the element (node) in its new position. In each case, the links for the adjacent elements also need to be updated. The function for the first step takes the current sequence and the location of the element to be moved. If the element doesn’t move as its value is 0 or the movement would result in the same position, no update is performed and the sequence is returned without change. Otherwise, the links for the adjacent elements are changed to point to each other.\n\nremove_node &lt;- function(lnk_lst, lst_loc) {\n  value &lt;- lnk_lst[[lst_loc]]$v\n  movement &lt;- ifelse(value &gt;= 0, value %% (length(lnk_lst)-1), -value %% (length(lnk_lst)-1))\n  if (value == 0 | movement == 0){  \n  #if (value == 0){\n    return(lnk_lst)\n  }\n  l_loc &lt;- lnk_lst[[lst_loc]]$l\n  r_loc &lt;- lnk_lst[[lst_loc]]$r\n  lnk_lst[[l_loc]]$r &lt;- r_loc\n  lnk_lst[[r_loc]]$l &lt;- l_loc\n  return(lnk_lst)\n}\n\nThe function for the second step of inserting the element (node) in its new location is shown below. Again, if the element value is 0 or the movement results in the same position, the sequence is returned unchanged. The cases for moving forwards or backwards in the sequence are handled separately as it is more clear programmatically. In both cases, the links for the inserted element and the adjacent elements are updated.\n\ninsert_node &lt;- function(lnk_lst, lst_loc) {\n  value &lt;- lnk_lst[[lst_loc]]$v\n  movement &lt;- ifelse(value &gt;= 0, value %% (length(lnk_lst)-1), -value %% (length(lnk_lst)-1))\n  if (value == 0 | movement == 0){\n    return(lnk_lst)\n  }\n  #move right\n  if (value &gt; 0) {\n    new_loc &lt;- lst_loc\n    for (i in 1:movement) {\n    #for (i in 1:(value %% length(lnk_lst))) {\n      new_loc &lt;- lnk_lst[[new_loc]]$r\n      #print(new_loc)\n    }\n    #update inserted node left\n    lnk_lst[[lst_loc]]$l &lt;- new_loc\n    #update inserted node right\n    lnk_lst[[lst_loc]]$r &lt;- lnk_lst[[new_loc]]$r\n    #update right to insert to new left\n    lnk_lst[[lnk_lst[[new_loc]]$r]]$l &lt;- lst_loc\n    #update left to insert to new right\n    lnk_lst[[new_loc]]$r &lt;- lst_loc\n\n  # move left\n  } else {\n    new_loc &lt;- lst_loc\n    for (i in 1:movement) {    \n    #for (i in 1:(-value %% length(lnk_lst))) {\n      new_loc &lt;- lnk_lst[[new_loc]]$l\n    }\n    #update inserted node right\n    lnk_lst[[lst_loc]]$r &lt;- new_loc\n    #update inserted node left\n    lnk_lst[[lst_loc]]$l &lt;- lnk_lst[[new_loc]]$l\n    #update left to insert to new right\n    lnk_lst[[lnk_lst[[new_loc]]$l]]$r &lt;- lst_loc    \n    #update right to insert to new left\n    lnk_lst[[new_loc]]$l &lt;- lst_loc\n\n  }\n  return(lnk_lst)\n}\n\nA simple function is created to return the position of the element with the zero value in the sequence. This function is needed for the calculation of the grove coordinates.\n\nfind_zero &lt;- function(lnk_lst) {\n  for (i in 1:length(lnk_lst)) {\n    if (lnk_lst[[i]]$v == 0) return(i)\n  }\n  return(0)\n}"
  },
  {
    "objectID": "posts/2025-01-31_aoc_Day20_DoublyLinkedList/2025-01-31_aoc_Day20_DoublyLinkedList.html#part-1-solution",
    "href": "posts/2025-01-31_aoc_Day20_DoublyLinkedList/2025-01-31_aoc_Day20_DoublyLinkedList.html#part-1-solution",
    "title": "Advent of Code Day 20: Doubly-Linked List",
    "section": "Part 1 Solution",
    "text": "Part 1 Solution\nThe linked list is first created from the input sequence and then the sequence is mixed according to the encryption algorithm.\n\nlinked_list &lt;- create_linked_list(fileseq, 1)\n\nfor (i in 1:length(linked_list)) {\n  linked_list &lt;- remove_node(linked_list, i)\n  linked_list &lt;- insert_node(linked_list, i)\n}\n\nThe solution is then found by summing the values for the 1000th, 2000th and 3000th elements from the element containing the zero value. For this puzzle input, the total of the grove coordinates was 13883.\n\n#puzzle solution initialized at 0\ntotal &lt;- 0\n#start from zero\nnew_loc &lt;- find_zero(linked_list)\n\nfor (i in 1:3){\n  for (i in 1:1000) {\n    new_loc &lt;- linked_list[[new_loc]]$r\n  }\ntotal &lt;- total + linked_list[[new_loc]]$v\n}\n\ntotal\n\n[1] 13883"
  },
  {
    "objectID": "posts/2025-01-31_aoc_Day20_DoublyLinkedList/2025-01-31_aoc_Day20_DoublyLinkedList.html#summary",
    "href": "posts/2025-01-31_aoc_Day20_DoublyLinkedList/2025-01-31_aoc_Day20_DoublyLinkedList.html#summary",
    "title": "Advent of Code Day 20: Doubly-Linked List",
    "section": "Summary",
    "text": "Summary\nAn encryption algorithm was tracked using a doubly-linked list. This list structure enabled a programmatically clear approach to solving the puzzle. The use of the modulo operator helped with the overall computation time and hardly any change was required to move from Part 1 with modest values to Part 2 with significantly higher values.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2025-01-31\n pandoc   3.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.5.57 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P readr       * 2.1.5   2024-01-10 [?] CRAN (R 4.4.2)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.4.2)\n\n [1] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/windows/R-4.4/x86_64-w64-mingw32\n [2] C:/Users/David Zoller/AppData/Local/R/cache/R/renv/sandbox/windows/R-4.4/x86_64-w64-mingw32/6698a5f3\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2025-01-31_aoc_Day20_DoublyLinkedList/2025-01-31_aoc_Day20_DoublyLinkedList.html#part-2-solution",
    "href": "posts/2025-01-31_aoc_Day20_DoublyLinkedList/2025-01-31_aoc_Day20_DoublyLinkedList.html#part-2-solution",
    "title": "Advent of Code Day 20: Doubly-Linked List",
    "section": "Part 2 Solution",
    "text": "Part 2 Solution\nIn part 2, a decryption key, 811589153, is included which is multiplied by each number in the initial sequence. Also, the sequence is now mixed ten times rather than just once. With these two minor adjustments, the same functions can be used as in Part 1. Increasing the values does not increase the computation time much with the use of the modulo transformation. Of course, the mixing takes longer since the sequence is mixed 10 times now.\n\nlinked_list2 &lt;- create_linked_list(fileseq, 811589153)\n\nfor (j in 1:10) {\n  for (i in 1:length(linked_list2)) {\n    linked_list2 &lt;- remove_node(linked_list2, i)\n    linked_list2 &lt;- insert_node(linked_list2, i)\n  }\n}\n\nThe grove coordinates are found as in Part 1 and the summation is 19185967576920.\n\ntotal &lt;- 0\nnew_loc &lt;- find_zero(linked_list2)\n\nfor (i in 1:3){\n  for (i in 1:1000) {\n    new_loc &lt;- linked_list2[[new_loc]]$r\n  }\ntotal &lt;- total + linked_list2[[new_loc]]$v\n}\n\noptions(scipen = 999)\ntotal\n\n[1] 19185967576920"
  },
  {
    "objectID": "posts/2025-02-04_aoc_Day21_NamedList/2025-02-04_aoc_Day21_NamedList.html",
    "href": "posts/2025-02-04_aoc_Day21_NamedList/2025-02-04_aoc_Day21_NamedList.html",
    "title": "Advent of Code Day 21: Named List",
    "section": "",
    "text": "Find solution by looping through a series of equations stored in a named list"
  },
  {
    "objectID": "posts/2025-02-04_aoc_Day21_NamedList/2025-02-04_aoc_Day21_NamedList.html#introduction",
    "href": "posts/2025-02-04_aoc_Day21_NamedList/2025-02-04_aoc_Day21_NamedList.html#introduction",
    "title": "Advent of Code Day 21: Named List",
    "section": "Introduction",
    "text": "Introduction\nThis post explains my solution to the Advent of Code problem from Day 21. Monkeys are yelling either a specific number or the result of a math operation. The input file consists of the monkey’s name and a colon followed by their yell as shown below.\nroot: pppw + sjmn\ndbpl: 5\ncczh: sllz + lgvd\nzczc: 2\nptdq: humn - dvpt\ndvpt: 3\nlfqf: 4\nhumn: 5\nljgn: 2\nsjmn: drzm * dbpl\nsllz: 4\npppw: cczh / lfqf\nlgvd: ljgn * ptdq\ndrzm: hmdt - zczc\nhmdt: 32\nIn this case, the monkey named “dbpl” will yell a specific number, 5. The monkey named “cczh” will yell the summation of the number yelled by the monkeys named “sllZ” and “lgvd”. The monkeys don’t yell until they have a number or all the numbers hav been called which are required for their math operation. Your goal is to figure out what the monkey named “root” will yell before he yells it."
  },
  {
    "objectID": "posts/2025-02-04_aoc_Day21_NamedList/2025-02-04_aoc_Day21_NamedList.html#loading-the-input-file",
    "href": "posts/2025-02-04_aoc_Day21_NamedList/2025-02-04_aoc_Day21_NamedList.html#loading-the-input-file",
    "title": "Advent of Code Day 21: Named List",
    "section": "Loading the input file",
    "text": "Loading the input file\nEach monkey is captured in a single line of the input file and the actual file contains over 2000 monkeys. The read_lines function from the readr package is used to read the input file.\n\nlibrary(readr)\nlibrary(stringr)\noptions(scipen = 999)\n\nfilepath &lt;- here::here(\"./posts/data/aoc/day21_input.txt\")\n#filepath &lt;- here::here(\"./posts/data/aoc/day21_test.txt\")\n\nyells &lt;- read_lines(filepath, skip_empty_rows = TRUE)"
  },
  {
    "objectID": "posts/2025-02-04_aoc_Day21_NamedList/2025-02-04_aoc_Day21_NamedList.html#part-1-solution",
    "href": "posts/2025-02-04_aoc_Day21_NamedList/2025-02-04_aoc_Day21_NamedList.html#part-1-solution",
    "title": "Advent of Code Day 21: Named List",
    "section": "Part 1 Solution",
    "text": "Part 1 Solution\nA while loop is used to cycle through the yelling monkeys. The monkeys with specific numbers are stored in a named list called monkeys and removed from the iterated list of monkeys yet to yell (“x”). If the result of the math operation is available, the monkey is added to the monkeys list otherwise it is added to the end of the list of monkeys yet to yell. The loop continues until the monkey named “root” yells. The math operation is performed by parsing the text of the math operation and performing the calculation using the eval function. A couple of functions from the stringr package are used to manipulate and evaluate the strings.\n\nmonkeys &lt;- list()\nx &lt;- yells\n\nwhile(length(x) &gt; 0) {\n a &lt;- x[1]\n x &lt;- tail(x, -1)\n response &lt;- str_split(a, \": \")\n name &lt;- response[[1]][1]\n expr &lt;- response[[1]][2]\n if (str_detect(expr, \"^[[:digit:]]+$\")) {\n   monkeys[name] &lt;- as.numeric(expr)\n } else {\n   expression &lt;- str_split(expr, \" \")\n   left &lt;- expression[[1]][1]\n   op &lt;- expression[[1]][2]\n   right &lt;- expression[[1]][3]\n   #test is both numbers for math operation are available\n   test_left &lt;- left %in% names(monkeys)\n   test_right &lt;- right %in% names(monkeys)\n   if (test_left & test_right) {\n     left_val &lt;- monkeys[[left]]\n     right_val &lt;- monkeys[[right]]\n     result &lt;- eval(parse(text = paste0(left_val, op, right_val)))\n     monkeys[name] &lt;- result\n     if (name == \"root\") break\n   } else {\n     #add unsolved math operation to end of the list\n     x &lt;- c(x, a)\n   }\n }\n}\n\nmonkeys[[\"root\"]]\n\n[1] 364367103397416\n\n\nThe loop performed above using the named list runs quite quickly which will be important for Part 2. Also, notice that the number yelled by “root” is quite large: 364367103397416."
  },
  {
    "objectID": "posts/2025-02-04_aoc_Day21_NamedList/2025-02-04_aoc_Day21_NamedList.html#part-2-solution",
    "href": "posts/2025-02-04_aoc_Day21_NamedList/2025-02-04_aoc_Day21_NamedList.html#part-2-solution",
    "title": "Advent of Code Day 21: Named List",
    "section": "Part 2 Solution",
    "text": "Part 2 Solution\nSeveral changes to the puzzle are introduced in Part 2. First, the monkey named “humn” is not a monkey after all and it actually stands for human and that human is you. Also, the math operation for the monkey named root is not an addition but rather an equal sign meaning the numbers must match. For the example above, the monkey “pppw” and the monkey “sjmn” must yell the same number. So, you need to figure out the number to yell as “humn” in order for the numbers yelled by the two monkeys associated with root are the same.\nI saw online a couple of Python-based solution which used sympy to solve for the correct number for “humn”. I couldn’t find a similar R-based solver in my initial search and so I tried an iterative search. As the Part 1 solution was quite large, I also realized that the number for “humn” could be quite large as well. I also assumed the solution would be an integer.\nThe first function, find_root was created to find the names of the two monkeys required for “root” to yell. I created this function because those monkeys were different between the test data and the input file. This function enables me to switch between those two datasets without manually resetting the associated strings.\n\nfind_root &lt;- function(yells_vec) {\n  for (a in yells_vec) {\n   response &lt;- str_split(a, \": \")\n   name &lt;- response[[1]][1]\n   expr &lt;- response[[1]][2]\n   if (name == \"root\") {\n     expression &lt;- str_split(expr, \" \")\n     left &lt;- expression[[1]][1]\n     right &lt;- expression[[1]][3]\n     #if found, return in a string vector\n     return(c(left, right))\n   }\n  }\n  #if can't find root, return zeros\n  return(c(0,0))\n}\n\nNext, the loop created in Part 1 is converted into a function with some small changes. The number for “humn” is replaced with a guess which is an argument to the function. The two monkeys associated with “root” are passed as the root_lbls argument and determined from the find_root function. If the monkey name is “root”, the difference is calculated between the monkeys as diff when both of those monkeys have yelled their numbers.\n\nhumn_search &lt;- function(yells_vec, guess, root_lbls) {\n  monkeys &lt;- list()\n  x &lt;- yells_vec\n  \n  while(length(x) &gt; 0) {\n   a &lt;- x[1]\n   x &lt;- tail(x, -1)\n   response &lt;- str_split(a, \": \")\n   name &lt;- response[[1]][1]\n   expr &lt;- response[[1]][2]\n   if (str_detect(expr, \"^[[:digit:]]+$\")) {\n     #replace humn with the guess instead of what was in the file\n     monkeys[name] &lt;- ifelse(name == \"humn\", guess, as.numeric(expr))\n   } else {\n     expression &lt;- str_split(expr, \" \")\n     left &lt;- expression[[1]][1]\n     op &lt;- expression[[1]][2]\n     right &lt;- expression[[1]][3]\n     test_left &lt;- left %in% names(monkeys)\n     test_right &lt;- right %in% names(monkeys)\n     if (test_left & test_right) {\n       left_val &lt;- monkeys[[left]]\n       right_val &lt;- monkeys[[right]]\n       result &lt;- eval(parse(text = paste0(left_val, op, right_val)))\n       monkeys[name] &lt;- result\n       if (name == \"root\") {\n         #calculate difference between the monkeys related to root\n         diff &lt;- monkeys[[root_lbls[1]]] - monkeys[[root_lbls[2]]]\n         break\n       }\n     } else {\n       x &lt;- c(x, a)\n     }\n   }\n  }\n  return(diff)\n}\n\nThe first step to find the solution to Part 2 was to identify the number of digits. I did this by iterating by powers of 10 from 1 up to 20 and stopping once a sign change in the difference from humn_search is detected. For the input dataset, the solution was determined to be a 13-digit number.\n\nval_srch &lt;- 1:20\nroot_labels &lt;- find_root(yells)\ncurr_sign &lt;- sign(humn_search(yells, 1, root_labels))\n\nfor (i in val_srch) {\n  new_sign &lt;- sign(humn_search(yells, 1*10^i, root_labels))\n  if (new_sign != curr_sign) break\n}\n\nThe next step was to determine each digit for the solution. Two for loops were used to cycle through all 13 digits starting at the highest place and, again, checking for the sign change. The sign function will return -1 for a negative number, 1 for a positive number and 0 for zero. If the difference is 0, the solution has been found and the number is returned.\n\nval2_srch &lt;- 1:10\nsolution &lt;- 0\n\nfor (k in 1:i) {\n  for (j in val2_srch) {\n    new_sign &lt;- sign(humn_search(yells, solution + j*10^(i-k), root_labels))\n    if (new_sign != curr_sign) break\n  }\n  #if sign == 0 then the solution has been found\n  if (new_sign == 0) {\n    solution &lt;- solution + j*10^(i-k)\n  } else {\n    solution &lt;- solution + (j-1)*10^(i-k)\n  }\n  if (new_sign == 0) break\n}\n\nsolution\n\n[1] 3782852515583\n\n\nThe number to be yelled by me, the “humn”, was found to be 3782852515583. This was a bit of a hacky way to arrive at the solution and made certain assumptions that the solution would be positive, an integer and would have one sign change transition across this range."
  },
  {
    "objectID": "posts/2025-02-04_aoc_Day21_NamedList/2025-02-04_aoc_Day21_NamedList.html#summary",
    "href": "posts/2025-02-04_aoc_Day21_NamedList/2025-02-04_aoc_Day21_NamedList.html#summary",
    "title": "Advent of Code Day 21: Named List",
    "section": "Summary",
    "text": "Summary\nA while loop with a named list structure was used to efficiently arrive at the solution to Part 1. A very similar loop was implemented as a function to search for the number to solve Part 2. Further optimizations are certainly possible however this less optimum solution was able to arrive at the solution in under a minute.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2025-02-04\n pandoc   3.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.5.57 @ C:\\\\PROGRA~1\\\\RStudio\\\\RESOUR~1\\\\app\\\\bin\\\\quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P readr       * 2.1.5   2024-01-10 [?] CRAN (R 4.4.2)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.4.2)\n P stringr     * 1.5.1   2023-11-14 [?] CRAN (R 4.4.2)\n\n [1] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/windows/R-4.4/x86_64-w64-mingw32\n [2] C:/Users/David Zoller/AppData/Local/R/cache/R/renv/sandbox/windows/R-4.4/x86_64-w64-mingw32/6698a5f3\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  }
]