[
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Oct 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "blog",
    "section": "",
    "text": "tidymodels\n\n\nMLOps\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsci-kit\n\n\nreticulate\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreticulate\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 26, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2022\n\n\ndatadavidz\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2022-08-19_ConcreteIntro/2022-08-19_ConcreteIntro.html",
    "href": "posts/2022-08-19_ConcreteIntro/2022-08-19_ConcreteIntro.html",
    "title": "Introduction to the Concrete Dataset",
    "section": "",
    "text": "I have a particular interest in the ability of machine learning algorithms to predict formulations and I am always searching for these types of datasets.\nOne such dataset is the Concrete Compressive Strength Dataset found on the UCI Machine Learning Repository. Many thanks to the original owner, Prof. I-Cheng Yeh for making this dataset available to the public!\nConventional concrete contains cement, fine and coarse aggregates and water. High performance concrete incorporates additional ingredients such as fly ash, blast furnace slag and chemical additives like superplasticizer. The compressive strength of concrete has been empirically found to have an inverse relationship to the water-to-cement ratio also known as the Abrams’ rule. High performance concrete is a more complex material and experimental data does not always support this general rule. This dataset contains over 1000 high performance concrete formulations containing the ingredients described above along with the compressive strength of each formulation. In addition, the age of the concrete before testing is also recorded.\n\nBasic Analysis of the Concrete Dataset\nThe concrete dataset was downloaded from the UCI repository as an Excel file and imported into R using the readxl package without further modification. Once the dataset was loaded into R, the column names were cleaned up into a format more amenable to data analysis. The complete dataset contains 1030 rows of concrete formulations of which the first 5 are shown below.\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(knitr)\n\n\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))\n\n\nknitr::kable(head(concrete_tbl, 5), caption = \"First 5 Rows of Concrete Dataset\")\n\n\nFirst 5 Rows of Concrete Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\ncement\nblast_furnace_slag\nfly_ash\nwater\nsuperplasticizer\ncoarse_aggregate\nfine_aggregate\nage\ncompressive_strength\n\n\n\n\n540.0\n0.0\n0\n162\n2.5\n1040.0\n676.0\n28\n79.98611\n\n\n540.0\n0.0\n0\n162\n2.5\n1055.0\n676.0\n28\n61.88737\n\n\n332.5\n142.5\n0\n228\n0.0\n932.0\n594.0\n270\n40.26954\n\n\n332.5\n142.5\n0\n228\n0.0\n932.0\n594.0\n365\n41.05278\n\n\n198.6\n132.4\n0\n192\n0.0\n978.4\n825.5\n360\n44.29608"
  },
  {
    "objectID": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html",
    "href": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html",
    "title": "Exploratory Analysis of the Concrete Dataset",
    "section": "",
    "text": "Several exploratory data analysis (EDA) packages are used to evaluate the concrete dataset.\nIn the previous post, the concrete dataset was introduced. In this post, we further explore topics such as data completeness, distributions and correlations both with the target variable (compressive strength) and between predictor variables (ingredients). I use Several R packages which I have found to make this analysis quite simple and efficient: skimr, GGally and correlationFunnel."
  },
  {
    "objectID": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#load-libraries",
    "href": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#load-libraries",
    "title": "Exploratory Analysis of the Concrete Dataset",
    "section": "Load libraries",
    "text": "Load libraries\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n#EDA\nlibrary(skimr)\nlibrary(GGally)\nlibrary(correlationfunnel)\n\nA good first analysis once the dataset is loaded is to use the skimr package to provide an overview of the data columns.\n\nskimr::skim(concrete_tbl)\n\n\nData summary\n\n\nName\nconcrete_tbl\n\n\nNumber of rows\n1030\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncement\n0\n1\n281.17\n104.51\n102.00\n192.38\n272.90\n350.00\n540.0\n▆▇▇▃▂\n\n\nblast_furnace_slag\n0\n1\n73.90\n86.28\n0.00\n0.00\n22.00\n142.95\n359.4\n▇▂▃▁▁\n\n\nfly_ash\n0\n1\n54.19\n64.00\n0.00\n0.00\n0.00\n118.27\n200.1\n▇▁▂▂▁\n\n\nwater\n0\n1\n181.57\n21.36\n121.75\n164.90\n185.00\n192.00\n247.0\n▁▅▇▂▁\n\n\nsuperplasticizer\n0\n1\n6.20\n5.97\n0.00\n0.00\n6.35\n10.16\n32.2\n▇▆▁▁▁\n\n\ncoarse_aggregate\n0\n1\n972.92\n77.75\n801.00\n932.00\n968.00\n1029.40\n1145.0\n▃▅▇▅▂\n\n\nfine_aggregate\n0\n1\n773.58\n80.18\n594.00\n730.95\n779.51\n824.00\n992.6\n▂▃▇▃▁\n\n\nage\n0\n1\n45.66\n63.17\n1.00\n7.00\n28.00\n56.00\n365.0\n▇▁▁▁▁\n\n\ncompressive_strength\n0\n1\n35.82\n16.71\n2.33\n23.71\n34.44\n46.14\n82.6\n▅▇▇▃▁\n\n\n\n\n\nThe good news is that there were no missing data points in the concrete dataset. There are concrete compositions with no blast furnace slag, fly ash or superplasticizer. The age before testing is skewed to lower age before testing. These observations are further supported looking at the histograms shown below.\n\nconcrete_tbl %>%\n  pivot_longer(cement:age, names_to=\"ingredient\", values_to = \"amount\") %>%\n  ggplot(aes(x=amount)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(~ingredient, scales = \"free\")"
  },
  {
    "objectID": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#variable-correlations",
    "href": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#variable-correlations",
    "title": "Exploratory Analysis of the Concrete Dataset",
    "section": "Variable correlations",
    "text": "Variable correlations\nNext, we analyze the correlations between variables. When the amount of one ingredient is increased, we expect one or more of the other ingredients in the concrete mixture to decrease. So, some correlation between the concrete ingredients is expected.\n\nGGally::ggcorr(concrete_tbl)\n\n\n\n\nThe correlation analysis showed a strong, positive correlation with cement content and compressive strength and less strong correlations with age with compressive strength and superplasticizer with compressive strength. An inverse correlation between water and superplasticizer was detected perhaps due to the water content of the superplasticizer requiring less additional water in the formulation.\nAnother way of visualizing the correlation of variables with the property you wish to predict is the called a “correlation funnel”.\n\nconcrete_tbl %>%\n  binarize(n_bins = 3) %>%\n  correlate(`compressive_strength__41.36856_Inf`) %>%\n  plot_correlation_funnel(interactive = FALSE)\n\n\n\n\nThe correlation funnel shows some degree of correlation between the cement, water, superplasticizer and age with compressive strength. The fly ash, coarse and fine aggregate and blast furnace slag showed very little correlation with compressive strength."
  },
  {
    "objectID": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#summary",
    "href": "posts/2022-08-26_ConcreteEDA/2022-08-26_ConcreteEDA.html#summary",
    "title": "Exploratory Analysis of the Concrete Dataset",
    "section": "Summary",
    "text": "Summary\nThis post has shown several techniques for exploring the concrete dataset. The next post will use a generalized linear modeling approach to predict concrete compressive strength and compare the results with the conventional material modeling approach. Subsequent articles will use machine learning techniques such as artificial neural networks and extreme gradient boosting.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\nWarning: package 'sessioninfo' was built under R version 4.2.1\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package           * version date (UTC) lib source\n P correlationfunnel * 0.2.0   2020-06-09 [?] CRAN (R 4.2.1)\n P dplyr             * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats           * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P GGally            * 2.1.2   2021-06-21 [?] CRAN (R 4.2.1)\n P ggplot2           * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P purrr             * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P readr             * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl            * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P sessioninfo       * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P skimr             * 2.1.4   2022-04-15 [?] CRAN (R 4.2.1)\n P stringr           * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble            * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidyr             * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse         * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpolCg0M/renv-library-cb838f17a38\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2022-09-03_ConcreteGLM/2022-09-03_ConcreteGLM.html",
    "href": "posts/2022-09-03_ConcreteGLM/2022-09-03_ConcreteGLM.html",
    "title": "GLM model for Concrete Strength",
    "section": "",
    "text": "A generalized linear model (GLM) was built to predict the compressive strength of high-performance concrete formulations.\nAn elastic net regularization has been employed to develop the generalized linear model using the glmnet engine within the tidymodels framework. These results will be compared with a conventional materials modeling approach in the next post.\n##Load libraries and data"
  },
  {
    "objectID": "posts/2022-09-03_ConcreteGLM/2022-09-03_ConcreteGLM.html#stage-machine-learning-approach",
    "href": "posts/2022-09-03_ConcreteGLM/2022-09-03_ConcreteGLM.html#stage-machine-learning-approach",
    "title": "GLM model for Concrete Strength",
    "section": "3-Stage Machine Learning Approach",
    "text": "3-Stage Machine Learning Approach\nWe will utilize the 3-stage machine learning approach promoted by Matt Dancho at Business Science. He posted an excellent tutorial “Product Price Prediction: A Tidy Hyperparameter Tuning and Cross Validation Tutorial”. I haven’t found a better example of applying the tidymodels framework to develop a predictive model.\nThe 3-stage hyperparameter tuning process:\n1. Find Parameters: Use hyperparameter tuning on a “training dataset” that sections your training data into cross validation folds. The output of stage 1 is the parameter set.\n2. Compare and Select the Best Model: Evaluate the performance on a hidden “test dataset”. The output at Stage 2 is what we determined as the best model.\n3. Train Final Model: Once we have selected the best model, we train the full dataset. This model goes into production.\n\nStage 1: Find Parameters\nHere we want to make different machine learning models and try them out by performing the following steps: - Initial Splitting: Separate into random training and test datasets - Preprocessing: Make a pipeline to transform raw data into a dataset ready for machine learning - Cross Validation Specification: Sample the training data into splits - Model Specification: Select model algorithms and identify key tuning parameters - Grid Specification: Set up a grid using wise parameter choices - Hyperparameter Tuning: Implement the tuning process\nInitial splitting of the dataset into Training and Test Dataset Here we use the rsample package to create an 80/20 split. The concrete dataset contains 1030 formulations of which 825 are randomly assigned to training and 205 are randomly assigned to testing.\n\nset.seed(123)\nconcrete_split <- initial_split(concrete_tbl, prop = 0.80)\nconcrete_train <- training(concrete_split)\nconcrete_test <- testing(concrete_split)\n\nPreprocessing is accomplished by using the recipe package. The recipe provides the steps required to transform our raw data into a dataset suitable for machine learning. The Concrete dataset actually doesn’t require much reformatting. The major issue was the lengthy column names which was addressed immediately after the dataset was imported. The dataset contained all numerical values and no missing data. Initially we will just center and scale the predictors before sending to the glmnet model.\n\nconcrete_rec <- recipe(compressive_strength ~ ., data = concrete_train) %>%\n  step_center(all_predictors()) %>%\n  step_scale(all_predictors())\n\nconcrete_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          8\n\nOperations:\n\nCentering for all_predictors()\nScaling for all_predictors()\n\n\nCross validation folds are created in order to assess the performance of the model parameters. Here we use 5-fold cross validation to create splits from our training dataset and also using the preprocessing pipeline specified above.\n\nset.seed(234)\nconcrete_folds <- vfold_cv(concrete_train, v = 5)\n\nconcrete_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [659/165]> Fold1\n2 <split [659/165]> Fold2\n3 <split [659/165]> Fold3\n4 <split [659/165]> Fold4\n5 <split [660/164]> Fold5\n\n\nModel specifications are created using the parsnip package. Here we specify a linear regression model using the glmnet engine. glmnet uses an Elastic Net which combines LASSO and Ridge Regression techniques. This is a linear algorithm which may have difficulty with the skewed numeric data which is present in the Concrete dataset. Notice that the penalty and mixture parameters have been specified to be tuned.\n\nglmnet_spec <- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"regression\")\n\nglmnet_spec\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = tune()\n\nComputational engine: glmnet \n\n\nGrid specifications sets up a variety of parameter values used with our model to find which combination yields the lowest prediction error (or best accuracy). Here we specify the parameter ranges and grid function using the dials package.\nSpecify the grid function (max entropy, hypercube etc.). Here we make a grid of 20 values using the grid_max_entropy() function in the dials package. Since there are just 2 tuning parameters in this case, we can visualize the grid selections. Note the penalty parameter is on the log base 10 scale by default. The dials package helps us make smarter choices for the critical tuning parameters.\n\nset.seed(345)\nglmnet_grid <- grid_max_entropy(penalty(), mixture(), size = 20)\n\nglmnet_grid %>%\n  ggplot(aes(penalty, mixture)) +\n  geom_point(color = \"steelblue\", size = 3) +\n  scale_x_log10() +\n  theme_light() +\n  labs(title = \"Max Entropy Grid\", x = \"Penalty (log scale)\", y = \"Mixture\")\n\n\n\n\n\nconcrete_wf <- workflow() %>%\n  add_recipe(concrete_rec) %>%\n  add_model(glmnet_spec)\n\nHyperparameter tuning is now performed using the tune_grid() function from the tune package. Here we specific the formula, model, resamples, grid and metrics. The metrics come from the yardstick package. For regression problems, we can specify multiple metrics such as mae, mape, rmse and rsq into a metric_set().\n\ndoParallel::registerDoParallel()\n\nglmnet_res <- tune_grid(\n  concrete_wf,\n  resamples = concrete_folds,\n  grid = glmnet_grid,\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\nIdentify the best hyperparameter values using the show_best() function.\n\nglmnet_res %>% show_best(\"mae\", n = 5)\n\n# A tibble: 5 × 8\n   penalty mixture .metric .estimator  mean     n std_err .config              \n     <dbl>   <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1 1.09e- 9   0.155 mae     standard    8.10     5   0.216 Preprocessor1_Model06\n2 5.89e- 4   0.208 mae     standard    8.10     5   0.216 Preprocessor1_Model07\n3 4.05e-10   0.392 mae     standard    8.10     5   0.216 Preprocessor1_Model10\n4 1.16e- 6   0.340 mae     standard    8.10     5   0.216 Preprocessor1_Model08\n5 1.41e- 8   0.520 mae     standard    8.10     5   0.216 Preprocessor1_Model12\n\n\nVisualize the tuning results\n\n\n\n\n\n\n\nStage 2: Compare and Select the Best Model\nSelect the best parameters based on the lowest mean absolute error.\n\nparams_glmnet_best <- glmnet_res %>% select_best(\"mae\")\nparams_glmnet_best\n\n# A tibble: 1 × 3\n        penalty mixture .config              \n          <dbl>   <dbl> <chr>                \n1 0.00000000109   0.155 Preprocessor1_Model06\n\n\nFinalize the model with the best parameters.\n\nfinal_glmnet <- finalize_workflow(concrete_wf, params_glmnet_best)\n\nfinal_glmnet\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_center()\n• step_scale()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1.09262294094878e-09\n  mixture = 0.155459027038887\n\nComputational engine: glmnet \n\n\nWhich Features are most important?\n\nfinal_glmnet %>%\n  fit(data = concrete_train) %>%\n  pull_workflow_fit() %>%\n  vip(aesthetics = list(fill = \"steelblue\")) +\n  labs(title = \"GLMNET Model Importance - Compressive Strength (MPa) Prediction\")\n\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nPlease use `extract_fit_parsnip()` instead.\n\n\n\n\n\n\n\nStage 3: Train Final Model\nFit model on train and evaluate on test.\n\nfinal_res <- last_fit(final_glmnet, concrete_split, metrics = metric_set(rmse, rsq, mae))\n\nAssess final model performance metrics.\n\ncollect_metrics(final_res)\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      11.4   Preprocessor1_Model1\n2 rsq     standard       0.615 Preprocessor1_Model1\n3 mae     standard       9.09  Preprocessor1_Model1\n\n\nVisualize actual vs. predicted compressive strength for final model."
  },
  {
    "objectID": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html",
    "href": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html",
    "title": "Conventional Material Models for Concrete Dataset",
    "section": "",
    "text": "Fitting the concrete dataset to a pre-determined equation using a non-linear, least squares approximation.\nAbrams’ law states that the strength of a concrete mix is inversely related to the mass ratio of water to cement. In other words, as the water content increases, the strength of the concrete decreases. Experimental data however shows that this law does not provide the complete picture and concrete formulations with the same water:cement can have significantly different performance."
  },
  {
    "objectID": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#load-libraries-and-data",
    "href": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#load-libraries-and-data",
    "title": "Conventional Material Models for Concrete Dataset",
    "section": "Load libraries and data",
    "text": "Load libraries and data\n\nlibrary(readxl)\nlibrary(tidyverse)\n\ntheme_set(theme_light())\n\n\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"../data/\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))"
  },
  {
    "objectID": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#plot-compressive-strength-as-a-function-of-watercement",
    "href": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#plot-compressive-strength-as-a-function-of-watercement",
    "title": "Conventional Material Models for Concrete Dataset",
    "section": "Plot compressive strength as a function of water:cement",
    "text": "Plot compressive strength as a function of water:cement\n\nconcrete_tbl <- concrete_tbl %>%\n  mutate(water_cement = water / cement,\n         water_binder = water / (cement + blast_furnace_slag + fly_ash))\n\nconcrete_tbl %>%\n  ggplot(aes(water_cement, compressive_strength)) +\n  geom_point(alpha = 0.15) +\n  geom_smooth(formula = y ~ x, method = \"lm\") +\n  theme_light() +\n  labs(title = \"Concrete Compressive Strength vs. Water:Cement\",\n       x = \"Water:Cement\", y = \"Compressive Strength (MPa)\")\n\n\n\n\nIt is apparent from the plot above that water:cement is not the only factor important for determining the compressive strength of concrete. For example, there are multiple formulations with a water:cement of ~1 with a range of compressive strengths from less than 10 MPa to greater than 50 MPa. The age of the concrete at the time of testing is also recognized as an important factor in determining the concrete strength for a sample.\n\\[ f^\\prime_c(t) = a X^b \\cdot [c \\ln(t)+(d)] \\]\nwhere t = age at test, X = w/c or water-to-binder ratio and a, b, c, d are regression coefficients\nThe above equation also includes the age at test variable (t) to predict the compressive strength. This equation uses four parameters reminding me of the famous quote by mathematician John von Neumann, “with four parameters I can fit an elephant, with five I can make him wiggle his trunk.”\nThis equation is fit to the experimental dataset using non-linear least squares approximation. The nls function in base R has been used as shown below."
  },
  {
    "objectID": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#nls-fit-using-watercement",
    "href": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#nls-fit-using-watercement",
    "title": "Conventional Material Models for Concrete Dataset",
    "section": "NLS Fit using water:cement",
    "text": "NLS Fit using water:cement\n\nwc <- concrete_tbl$water_cement\nwb <- concrete_tbl$water_binder\nage <- concrete_tbl$age\ncs <- concrete_tbl$compressive_strength\n\ncsFunc <- function(wc, age, a, b, c, d) { (a * wc^b) + (c * log(age) + d)}\n\nFit with water:cement\n\ncsFit <- nls(cs ~ csFunc(wc, age, a, b, c, d), start=list(a=30, b=-0.6, c=0.3, d=0.1))\n\nsummary(csFit)\n\n\nFormula: cs ~ csFunc(wc, age, a, b, c, d)\n\nParameters:\n  Estimate Std. Error t value Pr(>|t|)    \na  14.0112     2.9035   4.826 1.61e-06 ***\nb  -1.0536     0.1357  -7.763 2.00e-14 ***\nc   8.1770     0.2587  31.608  < 2e-16 ***\nd -12.8289     3.1703  -4.047 5.59e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.869 on 1026 degrees of freedom\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 3.529e-06\n\n\nVisualize actual vs. predicted compressive strength for water:cement model.\n\n\n\n\n\n\nNLS Fit using Water:Binder\nFit with water:binder\n\ncsFit_wb <- nls(cs ~ csFunc(wb, age, a, b, c, d), start=list(a=10, b=-0.5, c=10, d=10))\n\nsummary(csFit_wb)\n\n\nFormula: cs ~ csFunc(wb, age, a, b, c, d)\n\nParameters:\n  Estimate Std. Error t value Pr(>|t|)    \na  23.5011     6.9342   3.389 0.000728 ***\nb  -0.8614     0.1440  -5.980 3.07e-09 ***\nc   8.5739     0.2043  41.971  < 2e-16 ***\nd -39.2343     8.2396  -4.762 2.20e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.774 on 1026 degrees of freedom\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 2.226e-06\n\n\nVisualize actual vs. predicted compressive strength for water:binder model."
  },
  {
    "objectID": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html",
    "href": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html",
    "title": "Neural Network for Concrete Dataset",
    "section": "",
    "text": "A single-layer neural network is fit to predict concrete compressive strength.\nIn this post, we will begin to use machine learning techniques for predicting compressive strength of formulations using the concrete dataset. In a previous post, we created a model using a conventional material modeling approach which resulted in an R2 of 0.78. Here we will use a single-layer neural network to predict compressive strength and compare the results with the conventional material model."
  },
  {
    "objectID": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#load-libraries-and-data",
    "href": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#load-libraries-and-data",
    "title": "Neural Network for Concrete Dataset",
    "section": "Load libraries and data",
    "text": "Load libraries and data\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n#Tidymodels\nlibrary(tidymodels)\nlibrary(vip)\n\n\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"../data/\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))"
  },
  {
    "objectID": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#stage-1-model-tuning",
    "href": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#stage-1-model-tuning",
    "title": "Neural Network for Concrete Dataset",
    "section": "Stage 1: Model Tuning",
    "text": "Stage 1: Model Tuning\nInitial splitting of the dataset into Training and Test Dataset Here we use the rsample package to create an 80/20 split. The concrete dataset contains 1030 formulations of which 825 are randomly assigned to training and 205 are randomly assigned to testing.\n\nset.seed(123)\nconcrete_split <- initial_split(concrete_tbl, prop = 0.80)\nconcrete_train <- training(concrete_split)\nconcrete_test <- testing(concrete_split)\n\nPreprocessing is accomplished by using the recipe package. The recipe provides the steps required to transform our raw data into a dataset suitable for machine learning. The Concrete dataset actually doesn’t require much reformatting. The major issue was the lengthy column names which was addressed immediately after the dataset was imported. The dataset contained all numerical values and no missing data. Initially we will just center and scale the predictors before sending to the nnet model.\n\nconcrete_rec <- recipe(compressive_strength ~ ., data = concrete_train) %>%\n  step_center(all_predictors()) %>%\n  step_scale(all_predictors())\n\nconcrete_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          8\n\nOperations:\n\nCentering for all_predictors()\nScaling for all_predictors()\n\n\nCross validation folds are created in order to assess the performance of the model parameters. Here we use 5-fold cross validation to create splits from our training dataset and also using the preprocessing pipeline specified above.\n\nset.seed(234)\nconcrete_folds <- vfold_cv(concrete_train, v = 5)\n\nconcrete_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [659/165]> Fold1\n2 <split [659/165]> Fold2\n3 <split [659/165]> Fold3\n4 <split [659/165]> Fold4\n5 <split [660/164]> Fold5\n\n\nModel specifications are created using the parsnip package. Here we specify a multi-layer perceptron (mlp) using the nnet engine. The multi-layer perceptron is a single-layer neural network. Notice that the penalty and number of hidden units parameters have been specified to be tuned.\n\nmlp_spec = mlp(\n  hidden_units = tune(),\n  penalty = tune(),\n  epochs = 3000,\n  #activation = \"linear\"\n) %>%\n  set_engine(\"nnet\") %>%\n  #set_engine(\"nnet\", objective = \"reg:squarederror\") %>%\n  set_mode(\"regression\")\n\nmlp_spec\n\nSingle Layer Neural Network Model Specification (regression)\n\nMain Arguments:\n  hidden_units = tune()\n  penalty = tune()\n  epochs = 3000\n\nComputational engine: nnet \n\n\nGrid specifications sets up a variety of parameter values used with our model to find which combination yields the lowest prediction error (or best accuracy). Here we specify the parameter ranges and grid function using the dials package.\nSpecify the grid function (max entropy, hypercube etc.). Here we make a grid of 20 values using the grid_max_entropy() function in the dials package. Since there are just 2 tuning parameters in this case, we can visualize the grid selections. Note the penalty parameter is on the log base 10 scale by default. The dials package helps us make smarter choices for the critical tuning parameters.\n\nset.seed(345)\nnnet_grid <- grid_max_entropy(penalty(), hidden_units(), size = 20)\n\nnnet_grid %>%\n  ggplot(aes(penalty, hidden_units)) +\n  geom_point(color = \"steelblue\", size = 3) +\n  scale_x_log10() +\n  theme_light() +\n  labs(title = \"Max Entropy Grid\", x = \"Penalty (log scale)\", y = \"Hidden Units\")\n\n\n\n\nDefine a workflow for the tuning process\n\nconcrete_wf <- workflow() %>%\n  add_recipe(concrete_rec) %>%\n  add_model(mlp_spec)\n\nHyperparameter tuning is now performed using the tune_grid() function from the tune package. Here we specific the formula, model, resamples, grid and metrics. The metrics come from the yardstick package. For regression problems, we can specify multiple metrics such as mae, mape, rmse and rsq into a metric_set().\n\ndoParallel::registerDoParallel()\n\nbegin <- Sys.time()\n\nset.seed(456)\n\nnnet_res <- tune_grid(\n  concrete_wf,\n  resamples = concrete_folds,\n  grid = nnet_grid,\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\nend1 <- Sys.time() - begin"
  },
  {
    "objectID": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#stage-2-compare-and-select-the-best-model",
    "href": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#stage-2-compare-and-select-the-best-model",
    "title": "Neural Network for Concrete Dataset",
    "section": "Stage 2: Compare and Select the Best Model",
    "text": "Stage 2: Compare and Select the Best Model\nIdentify the best hyperparameter values using the show_best() function.\n\nnnet_res %>% show_best(\"mae\", n = 5)\n\n# A tibble: 5 × 8\n  hidden_units   penalty .metric .estimator  mean     n std_err .config         \n         <int>     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>           \n1           10 0.0928    mae     standard    4.14     5  0.234  Preprocessor1_M…\n2           10 0.00165   mae     standard    4.32     5  0.189  Preprocessor1_M…\n3            7 0.0000358 mae     standard    4.36     5  0.322  Preprocessor1_M…\n4            7 0.316     mae     standard    4.40     5  0.0475 Preprocessor1_M…\n5            6 0.00285   mae     standard    4.61     5  0.315  Preprocessor1_M…\n\n\nVisualize the tuning results\n\n\n\n\n\nSelect the best parameters based on the lowest mean absolute error.\n\nparams_nnet_best <- nnet_res %>% select_best(\"mae\")\nparams_nnet_best\n\n# A tibble: 1 × 3\n  hidden_units penalty .config              \n         <int>   <dbl> <chr>                \n1           10  0.0928 Preprocessor1_Model10\n\n\n\nfinal_nnet <- finalize_workflow(concrete_wf, params_nnet_best)\n\nfinal_nnet\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: mlp()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_center()\n• step_scale()\n\n── Model ───────────────────────────────────────────────────────────────────────\nSingle Layer Neural Network Model Specification (regression)\n\nMain Arguments:\n  hidden_units = 10\n  penalty = 0.0927943905608392\n  epochs = 3000\n\nComputational engine: nnet \n\n\n\nset.seed(567)\n\nfinal_nnet %>%\n  fit(data = concrete_train) %>%\n  extract_fit_parsnip() %>%\n  vip(aesthetics = list(fill = \"steelblue\")) +\n  labs(title = \"NNET Model Importance - Compressive Strength (MPa) Prediction\")"
  },
  {
    "objectID": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#stage-3-train-final-model",
    "href": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#stage-3-train-final-model",
    "title": "Neural Network for Concrete Dataset",
    "section": "Stage 3: Train Final Model",
    "text": "Stage 3: Train Final Model\nFit model on train and evaluate on test.\n\nset.seed(678)\n\nfinal_res <- last_fit(final_nnet, concrete_split, metrics = metric_set(rmse, rsq, mae))\n\nAssess final model performance metrics.\n\ncollect_metrics(final_res)\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       6.53  Preprocessor1_Model1\n2 rsq     standard       0.875 Preprocessor1_Model1\n3 mae     standard       4.94  Preprocessor1_Model1\n\n\nVisualize actual vs. predicted compressive strength for final model."
  },
  {
    "objectID": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#summary",
    "href": "posts/2022-09-17_ConcreteMLP/2022-09-17_ConcreteMLP.html#summary",
    "title": "Neural Network for Concrete Dataset",
    "section": "Summary",
    "text": "Summary\nThe single-layer, feed-forward neural network had an R2 of 0.87 and RMSE of 6.5 MPa.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version date (UTC) lib source\n P broom        * 1.0.1   2022-08-29 [?] CRAN (R 4.2.1)\n P dials        * 1.0.0   2022-06-14 [?] CRAN (R 4.2.1)\n P dplyr        * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats      * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2      * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P infer        * 1.0.3   2022-08-22 [?] CRAN (R 4.2.1)\n P modeldata    * 1.0.0   2022-07-01 [?] CRAN (R 4.2.1)\n P parsnip      * 1.0.0   2022-06-16 [?] CRAN (R 4.2.1)\n P purrr        * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P readr        * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl       * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P recipes      * 1.0.1   2022-07-07 [?] CRAN (R 4.2.1)\n P rsample      * 1.0.0   2022-06-24 [?] CRAN (R 4.2.1)\n P scales       * 1.2.1   2022-08-20 [?] CRAN (R 4.2.1)\n P sessioninfo  * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr      * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble       * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidymodels   * 1.0.0   2022-07-13 [?] CRAN (R 4.2.1)\n P tidyr        * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse    * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n P tune         * 1.0.0   2022-07-07 [?] CRAN (R 4.2.1)\n P vip          * 0.3.2   2020-12-17 [?] CRAN (R 4.0.5)\n P workflows    * 1.0.0   2022-07-05 [?] CRAN (R 4.2.1)\n P workflowsets * 1.0.0   2022-07-12 [?] CRAN (R 4.2.1)\n P yardstick    * 1.0.0   2022-06-06 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpE9tHQu/renv-library-30081bca49\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html",
    "href": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html",
    "title": "Prediction of Concrete Strength using XGBoost",
    "section": "",
    "text": "A gradient boosting model to predict the compressive strength of concrete was built using a tidymodels approach.\nIn this post, we will begin to use machine learning techniques for predicting compressive strength of formulations using the concrete dataset. In a previous post, we created a model using a conventional material modeling approach which resulted in an R2 of 0.78. Here we will use an XGBoost model to predict compressive strength and compare the results with a conventional material model."
  },
  {
    "objectID": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html#load-libraries-and-data",
    "href": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html#load-libraries-and-data",
    "title": "Prediction of Concrete Strength using XGBoost",
    "section": "Load libraries and data",
    "text": "Load libraries and data\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(ragg)\n\n#Tidymodels\nlibrary(tidymodels)\nlibrary(xgboost)\nlibrary(vip)\n\n\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"../data/\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))"
  },
  {
    "objectID": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html#stage-1-model-tuning",
    "href": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html#stage-1-model-tuning",
    "title": "Prediction of Concrete Strength using XGBoost",
    "section": "Stage 1: Model Tuning",
    "text": "Stage 1: Model Tuning\nInitial splitting of the dataset into Training and Test Dataset Here we use the rsample package to create an 80/20 split. The concrete dataset contains 1030 formulations of which 825 are randomly assigned to training and 205 are randomly assigned to testing.\n\nset.seed(123)\nconcrete_split <- initial_split(concrete_tbl, prop = 0.80)\nconcrete_train <- training(concrete_split)\nconcrete_test <- testing(concrete_split)\n\nPreprocessing is accomplished by using the recipe package. The recipe provides the steps required to transform our raw data into a dataset suitable for machine learning. The Concrete dataset actually doesn’t require much reformatting. The major issue was the lengthy column names which was addressed immediately after the dataset was imported. The dataset contained all numerical values and no missing data. Initially we will just center and scale the predictors before sending to the nnet model.\n\nconcrete_rec <- recipe(compressive_strength ~ ., data = concrete_train) %>%\n  step_center(all_predictors()) %>%\n  step_scale(all_predictors())\n\nconcrete_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          8\n\nOperations:\n\nCentering for all_predictors()\nScaling for all_predictors()\n\n\nCross validation folds are created in order to assess the performance of the model parameters. Here we use 5-fold cross validation to create splits from our training dataset and also using the preprocessing pipeline specified above.\n\nset.seed(234)\nconcrete_folds <- vfold_cv(concrete_train, v = 5)\n\nconcrete_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [659/165]> Fold1\n2 <split [659/165]> Fold2\n3 <split [659/165]> Fold3\n4 <split [659/165]> Fold4\n5 <split [660/164]> Fold5\n\n\nModel specifications are created using the parsnip package. Here we specify a boosted tree model using the XGBoost engine. Notice that the min n, tree depth and learn rate parameters have been specified to be tuned.\n\nxgboost_spec = boost_tree(\n  mode = \"regression\",\n  trees = 1000,\n  min_n = tune(),\n  tree_depth = tune(),\n  learn_rate = tune()\n) %>%\n  set_engine(\"xgboost\", objective = \"reg:squarederror\") %>%\n  set_mode(\"regression\")\n\nxgboost_spec\n\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n\nEngine-Specific Arguments:\n  objective = reg:squarederror\n\nComputational engine: xgboost \n\n\nGrid specifications sets up a variety of parameter values used with our model to find which combination yields the lowest prediction error (or best accuracy). Here we specify the parameter ranges and grid function using the dials package.\n\nset.seed(345)\nxgboost_grid <- grid_max_entropy(min_n(), tree_depth(), learn_rate(), size = 30)\n\nxgboost_grid\n\n# A tibble: 30 × 3\n   min_n tree_depth learn_rate\n   <int>      <int>      <dbl>\n 1     6          1   8.78e- 7\n 2    38          3   9.89e- 8\n 3    30          1   1.18e- 2\n 4    23          5   3.77e- 7\n 5    37         15   1.07e-10\n 6    16          6   3.19e- 4\n 7    12          7   1.34e-10\n 8    40         10   9.93e- 8\n 9     2         15   2.08e- 8\n10    36         10   2.64e- 2\n# … with 20 more rows\n\n\nDefine a workflow for the tuning process\n\nconcrete_wf <- workflow() %>%\n  add_recipe(concrete_rec) %>%\n  add_model(xgboost_spec)\n\nHyperparameter tuning is now performed using the tune_grid() function from the tune package. Here we specific the formula, model, resamples, grid and metrics. The metrics come from the yardstick package. For regression problems, we can specify multiple metrics such as mae, mape, rmse and rsq into a metric_set().\n\ndoParallel::registerDoParallel()\n\nset.seed(456)\n\nbegin <- Sys.time()\n\nxgboost_res <- tune_grid(\n  concrete_wf,\n  resamples = concrete_folds,\n  grid = xgboost_grid,\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\nend1 <- Sys.time() - begin"
  },
  {
    "objectID": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html#stage-2-compare-and-select-the-best-model",
    "href": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html#stage-2-compare-and-select-the-best-model",
    "title": "Prediction of Concrete Strength using XGBoost",
    "section": "Stage 2: Compare and Select the Best Model",
    "text": "Stage 2: Compare and Select the Best Model\nIdentify the best hyperparameter values using the show_best() function.\n\nxgboost_res %>% show_best(\"mae\", n = 5)\n\n# A tibble: 5 × 9\n  min_n tree_depth learn_rate .metric .estimator  mean     n std_err .config    \n  <int>      <int>      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>      \n1    18         10    0.0265  mae     standard    2.98     5  0.103  Preprocess…\n2    28          7    0.0302  mae     standard    3.10     5  0.103  Preprocess…\n3    36         10    0.0264  mae     standard    3.14     5  0.0829 Preprocess…\n4    19          2    0.0904  mae     standard    3.37     5  0.110  Preprocess…\n5    24         14    0.00824 mae     standard    3.46     5  0.0668 Preprocess…\n\n\nVisualize the tuning results\n\n\n\n\n\nSelect the best parameters based on the lowest mean absolute error.\n\nparams_xgboost_best <- xgboost_res %>% select_best(\"mae\")\nparams_xgboost_best\n\n# A tibble: 1 × 4\n  min_n tree_depth learn_rate .config              \n  <int>      <int>      <dbl> <chr>                \n1    18         10     0.0265 Preprocessor1_Model14\n\n\nFinalize workflow with the best model parameters\n\nfinal_xgboost <- finalize_workflow(concrete_wf, params_xgboost_best)\n\nfinal_xgboost\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_center()\n• step_scale()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 18\n  tree_depth = 10\n  learn_rate = 0.0264752492619167\n\nEngine-Specific Arguments:\n  objective = reg:squarederror\n\nComputational engine: xgboost \n\n\nWhich Features are most important?\n\nfinal_xgboost %>%\n  fit(data = concrete_train) %>%\n  extract_fit_parsnip() %>%\n  vip(aesthetics = list(fill = \"steelblue\")) +\n  labs(title = \"XGBoost Model Importance - Compressive Strength (MPa) Prediction\")\n\n\n\n\n\nStage 3: Train Final Model\nFit model on train and evaluate on test.\n\nfinal_res <- last_fit(final_xgboost, concrete_split, metrics = metric_set(rmse, rsq, mae))\n\nAssess final model performance metrics.\n\ncollect_metrics(final_res)\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       4.33  Preprocessor1_Model1\n2 rsq     standard       0.945 Preprocessor1_Model1\n3 mae     standard       2.69  Preprocessor1_Model1\n\n\nVisualize actual vs. predicted compressive strength for final model."
  },
  {
    "objectID": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html#summary",
    "href": "posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html#summary",
    "title": "Prediction of Concrete Strength using XGBoost",
    "section": "Summary",
    "text": "Summary\nThe XGBoost model to predict the compressive strength of concrete performed better (RMSE = 4.3 MPa, R2 = 0.945) than a conventional materials model(RMSE = 7.9 MPa, R2 = 0.78).\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version date (UTC) lib source\n P broom        * 1.0.1   2022-08-29 [?] CRAN (R 4.2.1)\n P dials        * 1.0.0   2022-06-14 [?] CRAN (R 4.2.1)\n P dplyr        * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats      * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2      * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P infer        * 1.0.3   2022-08-22 [?] CRAN (R 4.2.1)\n P modeldata    * 1.0.0   2022-07-01 [?] CRAN (R 4.2.1)\n P parsnip      * 1.0.0   2022-06-16 [?] CRAN (R 4.2.1)\n P purrr        * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P ragg         * 1.2.2   2022-02-21 [?] CRAN (R 4.2.1)\n P readr        * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl       * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P recipes      * 1.0.1   2022-07-07 [?] CRAN (R 4.2.1)\n P rsample      * 1.0.0   2022-06-24 [?] CRAN (R 4.2.1)\n P scales       * 1.2.1   2022-08-20 [?] CRAN (R 4.2.1)\n P sessioninfo  * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr      * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble       * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidymodels   * 1.0.0   2022-07-13 [?] CRAN (R 4.2.1)\n P tidyr        * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse    * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n P tune         * 1.0.0   2022-07-07 [?] CRAN (R 4.2.1)\n P vip          * 0.3.2   2020-12-17 [?] CRAN (R 4.0.5)\n P workflows    * 1.0.0   2022-07-05 [?] CRAN (R 4.2.1)\n P workflowsets * 1.0.0   2022-07-12 [?] CRAN (R 4.2.1)\n P xgboost      * 1.6.0.1 2022-04-16 [?] CRAN (R 4.2.1)\n P yardstick    * 1.0.0   2022-06-06 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpquYv5l/renv-library-21f850a61f6a\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html",
    "href": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html",
    "title": "Random Forest Model for Concrete Dataset",
    "section": "",
    "text": "A predictive model for compressive strength of concrete is built using a random forest algorithm.\nIn this post, we will begin to use machine learning techniques for predicting compressive strength of formulations using the concrete dataset. In a previous post, we created a model using a conventional material modeling approach which resulted in an R2 of 0.78. Here we will use a random forest model to predict compressive strength and compare the results with the conventional material model."
  },
  {
    "objectID": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html#load-libraries-and-data",
    "href": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html#load-libraries-and-data",
    "title": "Random Forest Model for Concrete Dataset",
    "section": "Load libraries and data",
    "text": "Load libraries and data\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n#Tidymodels\nlibrary(tidymodels)\nlibrary(ranger)\nlibrary(vip)\n\n\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"../data/\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))"
  },
  {
    "objectID": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html#stage-1-model-tuning",
    "href": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html#stage-1-model-tuning",
    "title": "Random Forest Model for Concrete Dataset",
    "section": "Stage 1: Model Tuning",
    "text": "Stage 1: Model Tuning\nInitial splitting of the dataset into Training and Test Dataset Here we use the rsample package to create an 80/20 split. The concrete dataset contains 1030 formulations of which 825 are randomly assigned to training and 205 are randomly assigned to testing.\n\nset.seed(123)\nconcrete_split <- initial_split(concrete_tbl, prop = 0.80)\nconcrete_train <- training(concrete_split)\nconcrete_test <- testing(concrete_split)\n\nPreprocessing is accomplished by using the recipe package. The recipe provides the steps required to transform our raw data into a dataset suitable for machine learning. The Concrete dataset actually doesn’t require much reformatting. The major issue was the lengthy column names which was addressed immediately after the dataset was imported. The dataset contained all numerical values and no missing data. Initially we will just center and scale the predictors before sending to the nnet model.\n\nconcrete_rec <- recipe(compressive_strength ~ ., data = concrete_train) %>%\n  step_center(all_predictors()) %>%\n  step_scale(all_predictors())\n\nconcrete_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          8\n\nOperations:\n\nCentering for all_predictors()\nScaling for all_predictors()\n\n\nCross validation folds are created in order to assess the performance of the model parameters. Here we use 5-fold cross validation to create splits from our training dataset and also using the preprocessing pipeline specified above.\n\nset.seed(234)\nconcrete_folds <- vfold_cv(concrete_train, v = 5)\n\nconcrete_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [659/165]> Fold1\n2 <split [659/165]> Fold2\n3 <split [659/165]> Fold3\n4 <split [659/165]> Fold4\n5 <split [660/164]> Fold5\n\n\nModel specifications are created using the parsnip package. Here we specify a random forest model using the ranger engine. Notice that the min n and mtry parameters have been specified to be tuned.\n\nrf_spec = rand_forest(\n  trees = 1000,\n  min_n = tune(),\n  mtry = tune()\n) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\nrf_spec\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n\nComputational engine: ranger \n\n\nGrid specifications sets up a variety of parameter values used with our model to find which combination yields the lowest prediction error (or best accuracy). Here we specify the parameter ranges and grid function using the dials package.\nSpecify the grid function (max entropy, hypercube etc.). Here we make a grid of 20 values using the grid_max_entropy() function in the dials package. Since there are just 2 tuning parameters in this case, we can visualize the grid selections. Note the penalty parameter is on the log base 10 scale by default. The dials package helps us make smarter choices for the critical tuning parameters.\n\nset.seed(345)\nrf_grid <- grid_max_entropy(min_n(), mtry(c(1L, 10L)), size = 20)\n\nrf_grid %>%\n  ggplot(aes(min_n, mtry)) +\n  geom_point(color = \"steelblue\", size = 3) +\n  #scale_x_log10() +\n  theme_light() +\n  labs(title = \"Max Entropy Grid\", x = \"min n\", y = \"mtry\")\n\n\n\n\nDefine a workflow for the tuning process\n\nconcrete_wf <- workflow() %>%\n  add_recipe(concrete_rec) %>%\n  add_model(rf_spec)\n\nHyperparameter tuning is now performed using the tune_grid() function from the tune package. Here we specific the formula, model, resamples, grid and metrics. The metrics come from the yardstick package. For regression problems, we can specify multiple metrics such as mae, mape, rmse and rsq into a metric_set().\n\ndoParallel::registerDoParallel()\n\nset.seed(456)\n\nbegin <- Sys.time()\n\nrf_res <- tune_grid(\n  concrete_wf,\n  resamples = concrete_folds,\n  grid = rf_grid,\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\nend1 <- Sys.time() - begin"
  },
  {
    "objectID": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html#stage-2-compare-and-select-the-best-model",
    "href": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html#stage-2-compare-and-select-the-best-model",
    "title": "Random Forest Model for Concrete Dataset",
    "section": "Stage 2: Compare and Select the Best Model",
    "text": "Stage 2: Compare and Select the Best Model\nIdentify the best hyperparameter values using the show_best() function.\n\nrf_res %>% show_best(\"mae\", n = 5)\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     7     2 mae     standard    3.67     5  0.0647 Preprocessor1_Model01\n2     5     4 mae     standard    3.73     5  0.0648 Preprocessor1_Model14\n3     9     6 mae     standard    3.79     5  0.0695 Preprocessor1_Model15\n4     6    10 mae     standard    3.96     5  0.0734 Preprocessor1_Model09\n5     2     6 mae     standard    4.36     5  0.105  Preprocessor1_Model16\n\n\nVisualize the tuning results\n\nautoplot(rf_res)\n\n\n\n\nSelect the best parameters based on the lowest mean absolute error.\n\nparams_rf_best <- rf_res %>% select_best(\"mae\")\nparams_rf_best\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1     7     2 Preprocessor1_Model01\n\n\nFinalize workflow with the best model parameters\n\nfinal_rf <- finalize_workflow(concrete_wf, params_rf_best)\n\nfinal_rf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_center()\n• step_scale()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 7\n  trees = 1000\n  min_n = 2\n\nComputational engine: ranger \n\n\nWhich Features are most important? For random forest, we are defining the importance measure as permutation which requires a new specification since including this calculation in the initial specification would slow down the tuning process.\n\nimp_spec <- rf_spec %>%\n  finalize_model(params_rf_best) %>%\n  set_engine(\"ranger\", importance = \"permutation\")\n\nworkflow() %>%\n  add_recipe(concrete_rec) %>%\n  add_model(imp_spec) %>%\n  fit(data = concrete_train) %>%\n  extract_fit_parsnip() %>%\n  vip(aesthetics = list(fill = \"steelblue\")) +\n  labs(title = \"Random Forest Model Importance - Compressive Strength (MPa) Prediction\")\n\n\n\n\n\nStage 3: Train Final Model\nFit model on train and evaluate on test.\n\nfinal_res <- last_fit(final_rf, concrete_split, metrics = metric_set(rmse, rsq, mae))\n\nAssess final model performance metrics.\n\ncollect_metrics(final_res)\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       5.07  Preprocessor1_Model1\n2 rsq     standard       0.929 Preprocessor1_Model1\n3 mae     standard       3.31  Preprocessor1_Model1\n\n\nVisualize actual vs. predicted compressive strength for final model.\n\ncollect_predictions(final_res) %>%\n  ggplot(aes(compressive_strength, .pred)) +\n  geom_abline(slope = 1, lty = 2, color = \"gray50\", alpha = 0.5) +\n  geom_point(alpha = 0.6, color = \"midnightblue\") +\n  ylim(0, NA) +\n  labs(title = \"Random Forest Model Performance for Concrete Dataset\", \n       x = \"Actual Compressive Strength (MPa)\", \n       y = \"Predicted Compressive Strength (MPa)\")"
  },
  {
    "objectID": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html#summary",
    "href": "posts/2022-10-01_ConcreteRF/2022-10-01_ConcreteRF.html#summary",
    "title": "Random Forest Model for Concrete Dataset",
    "section": "Summary",
    "text": "Summary\nThe random forest model to predict the compressive strength of concrete performed better (RMSE = 5.0 MPa, R2 = 0.93) than a conventional materials model(RMSE = 7.9 MPa, R2 = 0.78) but not quite as good as the XGBoost model (RMSE = 4.3 MPa, R2 = 0.945).\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version date (UTC) lib source\n P broom        * 1.0.1   2022-08-29 [?] CRAN (R 4.2.1)\n P dials        * 1.0.0   2022-06-14 [?] CRAN (R 4.2.1)\n P dplyr        * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats      * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2      * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P infer        * 1.0.3   2022-08-22 [?] CRAN (R 4.2.1)\n P modeldata    * 1.0.0   2022-07-01 [?] CRAN (R 4.2.1)\n P parsnip      * 1.0.0   2022-06-16 [?] CRAN (R 4.2.1)\n P purrr        * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P ranger       * 0.14.1  2022-06-18 [?] CRAN (R 4.2.1)\n P readr        * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl       * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P recipes      * 1.0.1   2022-07-07 [?] CRAN (R 4.2.1)\n P rsample      * 1.0.0   2022-06-24 [?] CRAN (R 4.2.1)\n P scales       * 1.2.1   2022-08-20 [?] CRAN (R 4.2.1)\n P sessioninfo  * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr      * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble       * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidymodels   * 1.0.0   2022-07-13 [?] CRAN (R 4.2.1)\n P tidyr        * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse    * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n P tune         * 1.0.0   2022-07-07 [?] CRAN (R 4.2.1)\n P vip          * 0.3.2   2020-12-17 [?] CRAN (R 4.0.5)\n P workflows    * 1.0.0   2022-07-05 [?] CRAN (R 4.2.1)\n P workflowsets * 1.0.0   2022-07-12 [?] CRAN (R 4.2.1)\n P yardstick    * 1.0.0   2022-06-06 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpIPfqiD/renv-library-11c44d35eb2\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html",
    "href": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html",
    "title": "Summary of Concrete Models",
    "section": "",
    "text": "A comparison of the predictive performance and speed for the different modeling approaches.\nSeveral models have been created to predict the compressive strength of high performance concrete based on the I-Cheng Yeh dataset. A conventional material model using a pre-determined transfer function which was fit to the data using a non-linear least squares approach. Four different models were created using machine learning algorithms, elastic net (glmnet), single-layer neural net (nnet), random forest (ranger) and boosted tree (xgboost), applied to the dataset."
  },
  {
    "objectID": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html#load-libraries-and-data",
    "href": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html#load-libraries-and-data",
    "title": "Summary of Concrete Models",
    "section": "Load libraries and data",
    "text": "Load libraries and data\n\n\nCode\nlibrary(knitr)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n\n\nCode\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"../data/\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))"
  },
  {
    "objectID": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html#predictive-accuracy",
    "href": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html#predictive-accuracy",
    "title": "Summary of Concrete Models",
    "section": "Predictive Accuracy",
    "text": "Predictive Accuracy\nEach model performance was assessed by several metrics: R-squared (R2), Root Mean Square Error (RMSE) and Mean Absolute Error (MAE).\n\n\n\n\n\n\n\n\n\n\nmetric\nnls\nglm\nmlp\nrf\nxgboost\n\n\n\n\nrmse\n7.76\n11.37\n6.53\n5.08\n4.33\n\n\nrsq\n0.78\n0.62\n0.88\n0.93\n0.95\n\n\nmae\n5.95\n9.09\n4.94\n3.32\n2.69\n\n\n\n\n\nAs shown in the figure and table above, the random forest (rf) and boosted tree (xgboost) models showed a significant improvement in predictive capability as compared with the conventional modeling approach (nls). The xgboost model had an R2 of 0.95 compared to 0.78 for the nls model with similar improvements in root mean square error and mean absolute error. The glmnet model gave worse performance than the non-linear models."
  },
  {
    "objectID": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html#benchmark-performance",
    "href": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html#benchmark-performance",
    "title": "Summary of Concrete Models",
    "section": "Benchmark performance",
    "text": "Benchmark performance\nHere we load the final models for the different approaches for comparison of prediction time. In this case, the time to make predictions for 10,300 (10 times the original dataset) was determined as a benchmark.\n\n\nCode\n#load all the models\nconcrete_nls <- readRDS(\"../results/concrete_nls_model.rds\")\nconcrete_glm <- readRDS(\"../results/concrete_glm_model.rds\")\nconcrete_mlp <- readRDS(\"../results/concrete_mlp_model.rds\")\nconcrete_rf <- readRDS(\"../results/concrete_rf_model.rds\")\nconcrete_xgb <- readRDS(\"../results/concrete_xgb_model.rds\")\n\n\nCreate the prediction dataset using 10 times the original dataset for purpose of comparing very fast prediction times.\n\n\nCode\ntemp <- concrete_tbl %>% slice(rep(row_number(), 10))\n\n\nBenchmarking was performed in the following manner using Sys.time to capture the time before and after each set of model predictions.\n\n\nCode\nbegin <- Sys.time()\na_temp <- predict(concrete_nls, new_data = temp)\nend1 <- Sys.time()\n\nb_temp <- predict(concrete_glm, new_data = temp)\nend2 <- Sys.time()\n\nc_temp <- predict(concrete_mlp, new_data = temp)\nend3 <- Sys.time()\n\nd_temp <- predict(concrete_rf, new_data = temp)\nend4 <- Sys.time()\n\ne_temp <- predict(concrete_xgb, new_data = temp)\nend5 <- Sys.time()\n\n# print(end1 - begin)[[1]]\n# print(end2 - end1)[[1]]\n# print(end3 - end2)[[1]]\n# print(end4 - end3)[[1]]\n\n#rm(temp)\n\n\nAs shown in the figure and table below, the xgboost model was the slowest taking about 1 second to perform 10,300 predictions. For the example of making a prediction of compressive strength of concrete for a particular formulation, however, this amount of time is trivial and the increased accuracy would be preferred over a faster and less accurate model.\n\n\n\n\n\n\n\n\n\n\nModel\nTime (ms)\n\n\n\n\nnls\n2\n\n\nglm\n132\n\n\nmlp\n60\n\n\nrf\n488\n\n\nxgboost\n555"
  },
  {
    "objectID": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html#summary",
    "href": "posts/2022-10-07_ConcreteSummary/2022-10-07_ConcreteSummary.html#summary",
    "title": "Summary of Concrete Models",
    "section": "Summary",
    "text": "Summary\nPrediction with the conventional model (nls) is about two orders of magnitude faster than the boosted tree model (xgboost). It should be noted that the random forest model is about 40% faster than the xgboost model, in this case, with similar predictive accuracy.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version date (UTC) lib source\n P broom        * 1.0.1   2022-08-29 [?] CRAN (R 4.2.1)\n P dials        * 1.0.0   2022-06-14 [?] CRAN (R 4.2.1)\n P dplyr        * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats      * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2      * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P infer        * 1.0.3   2022-08-22 [?] CRAN (R 4.2.1)\n P knitr        * 1.40    2022-08-24 [?] CRAN (R 4.2.1)\n P modeldata    * 1.0.0   2022-07-01 [?] CRAN (R 4.2.1)\n P parsnip      * 1.0.0   2022-06-16 [?] CRAN (R 4.2.1)\n P purrr        * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P readr        * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl       * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P recipes      * 1.0.1   2022-07-07 [?] CRAN (R 4.2.1)\n P rsample      * 1.0.0   2022-06-24 [?] CRAN (R 4.2.1)\n P scales       * 1.2.1   2022-08-20 [?] CRAN (R 4.2.1)\n P sessioninfo  * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr      * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble       * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidymodels   * 1.0.0   2022-07-13 [?] CRAN (R 4.2.1)\n P tidyr        * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse    * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n P tune         * 1.0.0   2022-07-07 [?] CRAN (R 4.2.1)\n P workflows    * 1.0.0   2022-07-05 [?] CRAN (R 4.2.1)\n P workflowsets * 1.0.0   2022-07-12 [?] CRAN (R 4.2.1)\n P yardstick    * 1.0.0   2022-06-06 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpYhQbrp/renv-library-2ff468663bb8\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-09-03_ConcreteGLM/2022-09-03_ConcreteGLM.html#summary",
    "href": "posts/2022-09-03_ConcreteGLM/2022-09-03_ConcreteGLM.html#summary",
    "title": "GLM model for Concrete Strength",
    "section": "Summary",
    "text": "Summary\nThe regularized linear model had relatively poor predictive performance (RMSE = 11.4 MPa, R2 = 0.62). A non-linear, least squares model will be built with better model performance in the next post.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version date (UTC) lib source\n P broom        * 1.0.1   2022-08-29 [?] CRAN (R 4.2.1)\n P dials        * 1.0.0   2022-06-14 [?] CRAN (R 4.2.1)\n P dplyr        * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats      * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2      * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P glmnet       * 4.1-1   2021-02-21 [?] CRAN (R 4.0.5)\n P infer        * 1.0.3   2022-08-22 [?] CRAN (R 4.2.1)\n P Matrix       * 1.4-1   2022-03-23 [?] CRAN (R 4.2.0)\n P modeldata    * 1.0.0   2022-07-01 [?] CRAN (R 4.2.1)\n P parsnip      * 1.0.0   2022-06-16 [?] CRAN (R 4.2.1)\n P purrr        * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P readr        * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl       * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P recipes      * 1.0.1   2022-07-07 [?] CRAN (R 4.2.1)\n P rsample      * 1.0.0   2022-06-24 [?] CRAN (R 4.2.1)\n P scales       * 1.2.1   2022-08-20 [?] CRAN (R 4.2.1)\n P sessioninfo  * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr      * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble       * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidymodels   * 1.0.0   2022-07-13 [?] CRAN (R 4.2.1)\n P tidyr        * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse    * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n P tune         * 1.0.0   2022-07-07 [?] CRAN (R 4.2.1)\n P vip          * 0.3.2   2020-12-17 [?] CRAN (R 4.0.5)\n P workflows    * 1.0.0   2022-07-05 [?] CRAN (R 4.2.1)\n P workflowsets * 1.0.0   2022-07-12 [?] CRAN (R 4.2.1)\n P yardstick    * 1.0.0   2022-06-06 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/Rtmpuo7gRW/renv-library-30a47499110a\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#summary",
    "href": "posts/2022-09-10_ConcreteNLS/2022-09-10_ConcreteNLS.html#summary",
    "title": "Conventional Material Models for Concrete Dataset",
    "section": "Summary",
    "text": "Summary\nThe NLS model using water:binder was a better fit to the experimental data than the NLS model using water:cement. The R2 for the water:binder model was 0.78 compared to an R2 of 0.65 for the water:cement model.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P dplyr       * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats     * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2     * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P purrr       * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P readr       * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl      * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble      * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidyr       * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse   * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpIHRddC/renv-library-22e4303bbb\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-09-02_ReticulateSetup/2022-09-02_ReticulateSetup.html",
    "href": "posts/2022-09-02_ReticulateSetup/2022-09-02_ReticulateSetup.html",
    "title": "Python Setup in RStudio using reticulate",
    "section": "",
    "text": "My experience setting up Python using the reticulate package and the RStudio IDE.\nThis post is a summary of my initial exploration to set up Python to operate within RStudio. I was mostly interested in being able to apply machine learning algorithms from Sci-kit Learn but through the RStudio IDE. The possibility of switching between R and Python languages within an analysis was also intriguing to me. I was somewhat surprised that there doesn’t seem to be a consensus on how to set up Python with RStudio. I found many different recommendations as far as how to install Python and how to configure RStudio. I believe this lack of consensus is due to the feature being rather new and also there are many possible configurations depending on your usage and preferences.\nI started with a fresh installation of Python and I chose to use the Miniconda installer for Windows 64-bit. No problems here except the default installation directory contained my Windows user name which contained a space. The installer gave a warning and I instead chose to install in folder in root directory called miniconda. I then updated to the latest version of RStudio (2022.02.2) and installation of the reticulate package. The reticulate package is essential for using Python in the RStudio environment.\n\nLoad libraries\n\nlibrary(tidyverse)\nlibrary(reticulate)\n\nFrom here, I tried the setup recommended by Tiffany Timbers on her Github and further discussed in an R Ladies Baltimore video. This setup involved setting a system environment variable in the .Rprofile to specify which Python installation to use: Sys.setenv(RETICULATE_PYTHON = \"path_to_miniconda's_python\"). While this approach does in fact work, this system setting locks in the Python installation to use and you need to modify the .Rprofile in order to use a different conda python environment. The setup also mentioned to make configuration changes to Git Bash and RStudio terminal settings that I found were not necessary. The setup instructions were created in December 2020 so, perhaps, subsequent RStudio versions have made these terminal configurations obsolete.\nNext, I found the reticulate installation recommended by Matt Dancho on the Business Science website. This setup recommended setting up a conda environment using the following command: conda create -n py3.8 python=3.8 scikit-learn pandas numpy matplotlib. This command creates a new environment “py3.8”, installs Python 3.8 and installs the latest versions of scikit-learn, pandas, numpy and matplotlib.\n\n\nList the conda environments in RMarkdown\n\nconda_list()\n\n      name                                   python\n1     base                 C:\\\\miniconda/python.exe\n2 my-rdkit C:\\\\miniconda\\\\envs\\\\my-rdkit/python.exe\n3    py3.8    C:\\\\miniconda\\\\envs\\\\py3.8/python.exe\n\n\nYou can then set your conda environment using reticulate::use_condaenv.\n\nuse_condaenv(\"py3.8\", required = TRUE)\n\nThe conda environment used by reticulate can then be checked.\n\npy_config()\n\npython:         C:/miniconda/envs/py3.8/python.exe\nlibpython:      C:/miniconda/envs/py3.8/python38.dll\npythonhome:     C:/miniconda/envs/py3.8\nversion:        3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 05:59:45) [MSC v.1929 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          C:/miniconda/envs/py3.8/Lib/site-packages/numpy\nnumpy_version:  1.22.4\n\nNOTE: Python version was forced by RETICULATE_PYTHON\n\n\n\n\nTest 1: Is Python Working?\n\n1 + 1\n\n2\n\n\nNote that here we are using a Python code block in Rmarkdown.\n\n\nTest 2: Numpy and Pandas\n\nimport numpy as np\nimport pandas as pd\n\nUse numpy to create a sequence of numbers in an array\n\nnp.arange(1,10)\n\narray([1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\nUse pandas to create a dataframe\n\n# Make a sequence in a data frame using dict format\ndf = pd.DataFrame(data = {\"sequence\":np.arange(1,20,.01)})\n\n# Use assign (mutate) equivalent to calculate the np.sin() of the series\ndf = df.assign(value=np.sin(df[\"sequence\"]))\n\ndf\n\n      sequence     value\n0         1.00  0.841471\n1         1.01  0.846832\n2         1.02  0.852108\n3         1.03  0.857299\n4         1.04  0.862404\n...        ...       ...\n1895     19.95  0.891409\n1896     19.96  0.895896\n1897     19.97  0.900294\n1898     19.98  0.904602\n1899     19.99  0.908819\n\n[1900 rows x 2 columns]\n\n\n\n\nTest #3: Generate a plot using Matplotlib\n\nimport matplotlib as plt\n\ndf.plot(x=\"sequence\", y = \"value\", title = \"Matplotlib\")\n\n\n\n\n\n\nTest #4: Build a model using Sci-kit Learn\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state=0)\n\nX = [[ 1,  2,  3],  # 2 samples, 3 features\n     [11, 12, 13]]\n\ny = [0, 1]  # classes of each sample\n\nclf.fit(X, y)\n\nRandomForestClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=0)\n\n\n\nclf.predict(X)  # predict classes of the training data\n\narray([0, 1])\n\n\n\n\nTip from Business Science post - 4 Conda Terminal Commands\nAt some point you will need to create, modify, add more packages to your Conda Environment(s). Here are 4 useful commands:\n\nRun conda env list to list the available conda environments\nRun conda activate <env_name> to activate a conda environment\nRun conda update --all to update all python packages in a conda environment.\nRun conda install <package_name> to install a new package\n\n\n\nSummary\nI found the approach recommended in the post by Matt Dancho was more straightforward and I haven’t found any downside yet. I like the flexibilty to change the conda environment for each analysis (Rmarkdown file) rather than adjusting the .Rprofile setting and rebooting R each time for the change to take effect. I will continue to update this post as I learn more tips and tricks for mixing R and Python code in Rmarkdown using the RStudio IDE.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P dplyr       * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats     * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2     * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P purrr       * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P readr       * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P reticulate  * 1.26    2022-08-31 [?] CRAN (R 4.2.1)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble      * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidyr       * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse   * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/Rtmpm0ubzp/renv-library-3b9c661d2710\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n─ Python configuration ───────────────────────────────────────────────────────\n python:         C:/miniconda/envs/py3.8/python.exe\n libpython:      C:/miniconda/envs/py3.8/python38.dll\n pythonhome:     C:/miniconda/envs/py3.8\n version:        3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 05:59:45) [MSC v.1929 64 bit (AMD64)]\n Architecture:   64bit\n numpy:          C:/miniconda/envs/py3.8/Lib/site-packages/numpy\n numpy_version:  1.22.4\n \n NOTE: Python version was forced by RETICULATE_PYTHON\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-10-13_ConcreteGP/2022-10-13_ConcreteGP.html",
    "href": "posts/2022-10-13_ConcreteGP/2022-10-13_ConcreteGP.html",
    "title": "Gaussian Process Model for the Concrete Dataset",
    "section": "",
    "text": "A GP model to predict the compressive strength of concrete is built using R and Python.\nThis post shares my first analysis of the Concrete dataset using a Gaussian Process modeling approach. I was interested in Gaussian Process models due to the possibility of building a non-linear regression model which fits the dataset well and allows for predictions on new data along with the uncertainty in that prediction. I have previously analyzed this dataset using a variety of machine learning approaches which allows for a good comparison in prediction performance.\nThe analysis combines R and Python as I wanted to reuse some of the data cleaning from the previous analyses written in R while the Gaussian Process model was built using Python. The most relevant articles I could find on Gaussian Process modeling contained examples in Python so I decided to use a similar approach. In this post, I am using the GaussianProcessRegressor model in the Sci-Kit Learn package to build the model. An RStudio blog was written in 2019 in R using tfprobability package on the same dataset but, honestly, I found it difficult to follow and the modeling results (MSE) was higher than my sklearn model."
  },
  {
    "objectID": "posts/2022-10-13_ConcreteGP/2022-10-13_ConcreteGP.html#evaluating-the-gp-model-predictions-for-the-test-data",
    "href": "posts/2022-10-13_ConcreteGP/2022-10-13_ConcreteGP.html#evaluating-the-gp-model-predictions-for-the-test-data",
    "title": "Gaussian Process Model for the Concrete Dataset",
    "section": "Evaluating the GP model predictions for the test data",
    "text": "Evaluating the GP model predictions for the test data\nYou need to scale the test data before prediction using the same scaling used on the training dataset.\n\nX_test_scale = scaler.transform(X_test)\ny_pred_te_scale, y_pred_te_std_scale = gp_model.predict(X_test_scale, return_std=True)\ny_pred_te = target_scaler.inverse_transform(y_pred_te_scale.reshape(-1,1))\ny_pred_te_std = y_pred_te_std_scale * target_scaler.scale_\n\ntpred_gp = metrics.r2_score(y_test, y_pred_te)\n\ntest_metrics = [[\"Rsq\", metrics.r2_score(y_test, y_pred_te)], [\"Adjusted RSq\", 1 - (1-metrics.r2_score(y_test, y_pred_te))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)], [\"MAE\", metrics.mean_absolute_error(y_test, y_pred_te)], [\"MSE\", metrics.mean_squared_error(y_test, y_pred_te)], [\"RMSE\", np.sqrt(metrics.mean_squared_error(y_test, y_pred_te))]]\n\ntest_metrics_df = pd.DataFrame(test_metrics, columns = [\"metric\", \"value\"])\nprint(test_metrics_df)\n\n         metric      value\n0           Rsq   0.891769\n1  Adjusted RSq   0.889621\n2           MAE   4.019690\n3           MSE  29.627775\n4          RMSE   5.443140\n\n\nThe model performance was a bit worse for the testing data as compared to the training data. I believe one reason is that cross-validation was not used and the model is overfitting the training data. One of the advantages of the Gaussian Process model is the estimation of uncertainty in the prediction. In the figure below, the predicted vs. measured compressive strengths for the test dataset are displayed along with error bars for +/- 1 standard deviation.\n\npred_test <- tibble(y_test = py$y_test, y_pred_te = as.vector(py$y_pred_te), y_pred_te_std = py$y_pred_te_std)\n\nggplot(data = pred_test, aes(x = y_test, y = y_pred_te)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = y_pred_te - y_pred_te_std, ymax = y_pred_te + y_pred_te_std)) +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Gaussian Process Model: Predicted vs. Measured for Testing Data\",\n       x = \"Actual Compressive Strength (MPa)\",\n       y = \"Predicted Compressive Strength (MPa)\") +\n  theme_light()\n\n`geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "posts/2022-10-13_ConcreteGP/2022-10-13_ConcreteGP.html#summary",
    "href": "posts/2022-10-13_ConcreteGP/2022-10-13_ConcreteGP.html#summary",
    "title": "Gaussian Process Model for the Concrete Dataset",
    "section": "Summary",
    "text": "Summary\nA Gaussian process model has been built for the concrete dataset. The predictive performance of this model was lower than for random forest and xgboost models (GP R2 = 0.89 vs. RF R2 = 0.94). The main advantage of the Gaussian Process model is the calculation of prediction error which can be very helpful in assessing confidence in future predictions.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P dplyr       * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats     * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2     * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P purrr       * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P readr       * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl      * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P reticulate  * 1.26    2022-08-31 [?] CRAN (R 4.2.1)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble      * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidyr       * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse   * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpQ5SNYG/renv-library-35903dd013ae\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n─ Python configuration ───────────────────────────────────────────────────────\n python:         C:/miniconda/envs/py3.8/python.exe\n libpython:      C:/miniconda/envs/py3.8/python38.dll\n pythonhome:     C:/miniconda/envs/py3.8\n version:        3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 05:59:45) [MSC v.1929 64 bit (AMD64)]\n Architecture:   64bit\n numpy:          C:/miniconda/envs/py3.8/Lib/site-packages/numpy\n numpy_version:  1.22.4\n \n NOTE: Python version was forced by use_python function\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html",
    "href": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html",
    "title": "Pin a Vetiver Model to an AWS S3 Container",
    "section": "",
    "text": "An XGBoost model for predicting concrete strength is transformed into a deployable model object and store it in an AWS S3 container.\nIn this post, I will take the XGBoost model for predicting concrete compressive strength described in a previous post, convert the model into a deployable model object using vetiverand “pin” it to an S3 bucket. The purpose of this effort is to make the model accessible in the cloud to an API running in a different location. The development of the API will be discussed in the next post. S3 stands for the AWS Simple Storage Service which exists in the cloud. I chose AWS over other vetiver-compatible options simply because I already had an existing account."
  },
  {
    "objectID": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html#build-the-model-again",
    "href": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html#build-the-model-again",
    "title": "Pin a Vetiver Model to an AWS S3 Container",
    "section": "Build the model (again)",
    "text": "Build the model (again)\nThis section just performs the steps to build the XGBoost model described in detail in the previous post.\nLoad the libraries.\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n#Tidymodels\nlibrary(tidymodels)\nlibrary(xgboost)\n\n#MLOps\nlibrary(vetiver)\nlibrary(pins)\n\nLoad the dataset.\n\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"../data/\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))\n\nSplit the data into training and testing datasets.\n\nset.seed(123)\nconcrete_split <- initial_split(concrete_tbl, prop = 0.80)\nconcrete_train <- training(concrete_split)\nconcrete_test <- testing(concrete_split)\n\nCreate the model recipe.\n\nconcrete_rec <- recipe(compressive_strength ~ ., data = concrete_train) %>%\n  step_normalize(all_predictors())\n  \nconcrete_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          8\n\nOperations:\n\nCentering and scaling for all_predictors()\n\n\nCreate the model specification. Parameters were specified from tuning in previous post.\n\nxgboost_spec = boost_tree(\n  trees = 1000,\n  min_n = 18,\n  tree_depth = 10,\n  learn_rate = 0.02647525\n) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n\nxgboost_spec\n\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 18\n  tree_depth = 10\n  learn_rate = 0.02647525\n\nComputational engine: xgboost \n\n\nCreate the modeling workflow.\n\nconcrete_wf <- workflow() %>%\n  add_recipe(concrete_rec) %>%\n  add_model(xgboost_spec)\n\nFit model on train and evaluate on test.\n\nfinal_res <- last_fit(concrete_wf, concrete_split, metrics = metric_set(rmse, rsq, mae))\n\nAssess final model performance metrics.\n\ncollect_metrics(final_res)\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       4.33  Preprocessor1_Model1\n2 rsq     standard       0.945 Preprocessor1_Model1\n3 mae     standard       2.69  Preprocessor1_Model1"
  },
  {
    "objectID": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html#create-the-deployable-model-object",
    "href": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html#create-the-deployable-model-object",
    "title": "Pin a Vetiver Model to an AWS S3 Container",
    "section": "Create the Deployable Model Object",
    "text": "Create the Deployable Model Object\nThe deployable model object is created using the vetiver package. It is really as simple as extracting the workflow and passing it to the vetiver_model function.\n\nv <- final_res %>%\n  extract_workflow() %>%\n  vetiver_model(model_name = \"concrete-xgb\")\n\nv\n\n\n── concrete-xgb ─ <bundled_workflow> model for deployment \nA xgboost regression modeling workflow using 8 features"
  },
  {
    "objectID": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html#pins-and-aws-s3",
    "href": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html#pins-and-aws-s3",
    "title": "Pin a Vetiver Model to an AWS S3 Container",
    "section": "Pins and AWS s3",
    "text": "Pins and AWS s3\nThe pins package allows you to save data, models or R objects to the cloud such as an AWS S3 container. A new S3 container can be set up within AWS. In my case, I just used the default settings with the name pins-test-zoller. A security id and access key needs to be set up to enable saving of data from your local computer to the S3 container. In your AWS account options under Security Credentials, you can configure your security id and access key and save the file to your local computer. There are multiple options to tell R where to find this information but I preferred to create a shared AWS credentials file in a text editor as follows:\n[default]\naws_access_key_id=your AWS access key\naws_secret_access_key=your AWS secret key\nOn a Windows computer, the file needs to be saved with the name credentials without any extension. The file location needs to be C:\\Users\\[your username]\\.aws\\. You may need to create the .aws directory.\nYou can then connect to the board where you want to place the pin using board_s3 command. Here, we pin the vetiver model for the concrete data.\n\nboard <- board_s3(\"pins-test-zoller\", region = \"us-east-2\")\nboard %>% vetiver_pin_write(v)\n\nCreating new version '20221017T213232Z-5eb32'\nWriting to pin 'concrete-xgb'\n\nCreate a Model Card for your published model\n• Model Cards provide a framework for transparent, responsible reporting\n• Use the vetiver `.Rmd` template as a place to start\n\n\nIn the AWS S3 bucket with the name “pins-test-zoller”, a new folder is created with the same name as the model, concrete-xgb. Within this folder, there is a subfolder with the named according to the model version number and, within the subfolder, is the model object in rds form (concrete-xgb.rds) and a data.txt file with brief information about the model object."
  },
  {
    "objectID": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html#summary",
    "href": "posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html#summary",
    "title": "Pin a Vetiver Model to an AWS S3 Container",
    "section": "Summary",
    "text": "Summary\nAn XGBoost model for the concrete dataset has been converted to a deployable model object using the vetiver package and then uploaded (i.e. pinned) to an AWS S3 bucket. The model object can now be accessed in the cloud for different purposes including creating an API to provide model predictions. The API use case will be discussed further in the next post.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-10-17\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version date (UTC) lib source\n P broom        * 1.0.1   2022-08-29 [?] CRAN (R 4.2.1)\n P dials        * 1.0.0   2022-06-14 [?] CRAN (R 4.2.1)\n P dplyr        * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats      * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2      * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P infer        * 1.0.3   2022-08-22 [?] CRAN (R 4.2.1)\n P modeldata    * 1.0.0   2022-07-01 [?] CRAN (R 4.2.1)\n P parsnip      * 1.0.0   2022-06-16 [?] CRAN (R 4.2.1)\n P pins         * 1.0.3   2022-09-24 [?] CRAN (R 4.2.1)\n P purrr        * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P readr        * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl       * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P recipes      * 1.0.1   2022-07-07 [?] CRAN (R 4.2.1)\n P rsample      * 1.0.0   2022-06-24 [?] CRAN (R 4.2.1)\n P scales       * 1.2.1   2022-08-20 [?] CRAN (R 4.2.1)\n P sessioninfo  * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr      * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble       * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidymodels   * 1.0.0   2022-07-13 [?] CRAN (R 4.2.1)\n P tidyr        * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse    * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n P tune         * 1.0.0   2022-07-07 [?] CRAN (R 4.2.1)\n P vetiver      * 0.1.8   2022-09-29 [?] CRAN (R 4.2.1)\n P workflows    * 1.0.0   2022-07-05 [?] CRAN (R 4.2.1)\n P workflowsets * 1.0.0   2022-07-12 [?] CRAN (R 4.2.1)\n P xgboost      * 1.6.0.1 2022-04-16 [?] CRAN (R 4.2.1)\n P yardstick    * 1.0.0   2022-06-06 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/Rtmp6r699v/renv-library-198833416b74\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  }
]