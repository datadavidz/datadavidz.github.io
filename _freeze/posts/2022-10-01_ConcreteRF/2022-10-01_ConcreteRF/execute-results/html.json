{
  "hash": "583df447dc58a73eb2a13b1532f68f07",
  "result": {
    "markdown": "---\ntitle: \"Random Forest Model for Concrete Dataset\"\ndate: \"2022-10-01\"\ncategories: [tidymodels]\n---\n\n\nA predictive model for compressive strength of concrete is built using a random forest algorithm.\n\nIn this post, we will begin to use machine learning techniques for predicting compressive strength of formulations using the concrete dataset.  In a previous post, we created a model using a conventional material modeling approach which resulted in an R^2^ of 0.78.  Here we will use a random forest model to predict compressive strength and compare the results with the conventional material model.\n\n## Load libraries and data\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\nlibrary(tidyverse)\n\n#Tidymodels\nlibrary(tidymodels)\nlibrary(vip)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"../data/\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))\n```\n:::\n\n\n## Stage 1: Model Tuning\n\nInitial splitting of the dataset into Training and Test Dataset  Here we use the rsample package to create an 80/20 split.  The concrete dataset contains 1030 formulations of which 825 are randomly assigned to training and 205 are randomly assigned to testing.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nconcrete_split <- initial_split(concrete_tbl, prop = 0.80)\nconcrete_train <- training(concrete_split)\nconcrete_test <- testing(concrete_split)\n```\n:::\n\n\nPreprocessing is accomplished by using the recipe package.  The recipe provides the steps required to transform our raw data into a dataset suitable for machine learning.  The Concrete dataset actually doesn't require much reformatting.  The major issue was the lengthy column names which was addressed immediately after the dataset was imported.  The dataset contained all numerical values and no missing data.  Initially we will just center and scale the predictors before sending to the nnet model.\n\n::: {.cell}\n\n```{.r .cell-code}\nconcrete_rec <- recipe(compressive_strength ~ ., data = concrete_train) %>%\n  step_center(all_predictors()) %>%\n  step_scale(all_predictors())\n\nconcrete_rec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          8\n\nOperations:\n\nCentering for all_predictors()\nScaling for all_predictors()\n```\n:::\n:::\n\n\nCross validation folds are created in order to assess the performance of the model parameters.  Here we use 5-fold cross validation to create splits from our training dataset and also using the preprocessing pipeline specified above.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\nconcrete_folds <- vfold_cv(concrete_train, v = 5)\n\nconcrete_folds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [659/165]> Fold1\n2 <split [659/165]> Fold2\n3 <split [659/165]> Fold3\n4 <split [659/165]> Fold4\n5 <split [660/164]> Fold5\n```\n:::\n:::\n\n\nModel specifications are created using the parsnip package.  Here we specify a random forest model using the ranger engine.  Notice that the min n and mtry parameters have been specified to be tuned.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_spec = rand_forest(\n  trees = 1000,\n  min_n = tune(),\n  mtry = tune()\n) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\nrf_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n\nComputational engine: ranger \n```\n:::\n:::\n\n\nGrid specifications sets up a variety of parameter values used with our model to find which combination yields the lowest prediction error (or best accuracy).  Here we specify the parameter ranges and grid function using the dials package.\n\nSpecify the grid function (max entropy, hypercube etc.).  Here we make a grid of 20 values using the grid_max_entropy() function in the dials package.  Since there are just 2 tuning parameters in this case, we can visualize the grid selections.  Note the penalty parameter is on the log base 10 scale by default.  The dials package helps us make smarter choices for the critical tuning parameters.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(345)\nrf_grid <- grid_max_entropy(min_n(), mtry(c(1L, 10L)), size = 20)\n\nrf_grid %>%\n  ggplot(aes(min_n, mtry)) +\n  geom_point(color = \"steelblue\", size = 3) +\n  #scale_x_log10() +\n  theme_light() +\n  labs(title = \"Max Entropy Grid\", x = \"min n\", y = \"mtry\")\n```\n\n::: {.cell-output-display}\n![](2022-10-01_ConcreteRF_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nDefine a workflow for the tuning process\n\n::: {.cell}\n\n```{.r .cell-code}\nconcrete_wf <- workflow() %>%\n  add_recipe(concrete_rec) %>%\n  add_model(rf_spec)\n```\n:::\n\n\nHyperparameter tuning is now performed using the tune_grid() function from the tune package.  Here we specific the formula, model, resamples, grid and metrics.  The metrics come from the yardstick package. For regression problems, we can specify multiple metrics such as mae, mape, rmse and rsq into a metric_set().\n\n::: {.cell hash='2022-10-01_ConcreteRF_cache/html/unnamed-chunk-18_1537a80b585c3a0ad21e61793a0e9c31'}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\n\nset.seed(456)\n\nbegin <- Sys.time()\n\nrf_res <- tune_grid(\n  concrete_wf,\n  resamples = concrete_folds,\n  grid = rf_grid,\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\nend1 <- Sys.time() - begin\n```\n:::\n\n\n## Stage 2: Compare and Select the Best Model\n\nIdentify the best hyperparameter values using the show_best() function.\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_res %>% show_best(\"mae\", n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     7     2 mae     standard    3.67     5  0.0647 Preprocessor1_Model01\n2     5     4 mae     standard    3.73     5  0.0648 Preprocessor1_Model14\n3     9     6 mae     standard    3.79     5  0.0695 Preprocessor1_Model15\n4     6    10 mae     standard    3.96     5  0.0734 Preprocessor1_Model09\n5     2     6 mae     standard    4.36     5  0.105  Preprocessor1_Model16\n```\n:::\n:::\n\nVisualize the tuning results  \n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(rf_res)\n```\n\n::: {.cell-output-display}\n![](2022-10-01_ConcreteRF_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nSelect the best parameters based on the lowest mean absolute error.\n\n::: {.cell}\n\n```{.r .cell-code}\nparams_rf_best <- rf_res %>% select_best(\"mae\")\nparams_rf_best\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1     7     2 Preprocessor1_Model01\n```\n:::\n:::\n\n\nFinalize workflow with the best model parameters\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_rf <- finalize_workflow(concrete_wf, params_rf_best)\n\nfinal_rf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_center()\n• step_scale()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 7\n  trees = 1000\n  min_n = 2\n\nComputational engine: ranger \n```\n:::\n:::\n\n\nWhich Features are most important?  For random forest, we are defining the importance measure as permutation which requires a new specification since including this calculation in the initial specification would slow down the tuning process.\n\n::: {.cell}\n\n```{.r .cell-code}\nimp_spec <- rf_spec %>%\n  finalize_model(params_rf_best) %>%\n  set_engine(\"ranger\", importance = \"permutation\")\n\nworkflow() %>%\n  add_recipe(concrete_rec) %>%\n  add_model(imp_spec) %>%\n  fit(data = concrete_train) %>%\n  extract_fit_parsnip() %>%\n  vip(aesthetics = list(fill = \"steelblue\")) +\n  labs(title = \"Random Forest Model Importance - Compressive Strength (MPa) Prediction\")\n```\n\n::: {.cell-output-display}\n![](2022-10-01_ConcreteRF_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n### Stage 3: Train Final Model\n\nFit model on train and evaluate on test.\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_res <- last_fit(final_rf, concrete_split, metrics = metric_set(rmse, rsq, mae))\n```\n:::\n\n\nAssess final model performance metrics.\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(final_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       5.07  Preprocessor1_Model1\n2 rsq     standard       0.929 Preprocessor1_Model1\n3 mae     standard       3.31  Preprocessor1_Model1\n```\n:::\n:::\n\n\nVisualize actual vs. predicted compressive strength for final model.  \n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_predictions(final_res) %>%\n  ggplot(aes(compressive_strength, .pred)) +\n  geom_abline(slope = 1, lty = 2, color = \"gray50\", alpha = 0.5) +\n  geom_point(alpha = 0.6, color = \"midnightblue\") +\n  ylim(0, NA) +\n  labs(title = \"Random Forest Model Performance for Concrete Dataset\", \n       x = \"Actual Compressive Strength (MPa)\", \n       y = \"Predicted Compressive Strength (MPa)\")\n```\n\n::: {.cell-output-display}\n![](2022-10-01_ConcreteRF_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n## Summary\nThe random forest model to predict the compressive strength of concrete performed better (RMSE = 5.0 MPa, R^2^ = 0.93) than a conventional materials model(RMSE = 7.9 MPa, R^2^ = 0.78) but not quite as good as the XGBoost model (RMSE = 4.3 MPa, R^2^ = 0.945).\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}