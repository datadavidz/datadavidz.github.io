{
  "hash": "ead1d8b845d7befb095b42bc81715469",
  "result": {
    "markdown": "---\ntitle: \"Gaussian Process Model for the Concrete Dataset\"\ndate: \"2022-10-13\"\ncategories: [sci-kit, reticulate]\n---\n\n::: {.cell}\n\n:::\n\n\nA GP model to predict the compressive strength of concrete is built using R and Python.\n\nThis post shares my first analysis of the Concrete [dataset](http://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength) using a Gaussian Process modeling approach.  I was interested in Gaussian Process models due to the possibility of building a non-linear regression model which fits the dataset well and allows for predictions on new data along with the uncertainty in that prediction.  I have previously analyzed this dataset using a variety of machine learning approaches which allows for a good comparison in prediction performance.\n\nThe analysis combines R and Python as I wanted to reuse some of the data cleaning from the previous analyses written in R while the Gaussian Process model was built using Python.  The most relevant articles I could find on Gaussian Process modeling contained examples in Python so I decided to use a similar approach.  In this post, I am using the ```GaussianProcessRegressor``` model in the Sci-Kit Learn package to build the model.  An RStudio [blog](https://blogs.rstudio.com/ai/posts/2019-12-10-variational-gaussian-process/) was written in 2019 in R using ```tfprobability``` package on the same dataset but, honestly, I found it difficult to follow and the modeling results (MSE) was higher than my sklearn model.\n\n### Load the R libraries and data\n\nHere I am reusing the code from previous analyses on the Concrete dataset.  The column names needed to be renamed so that they are more manageable for further data manipulations.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"../data/\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))\n```\n:::\n\n\n### Initialize the Python environment\n\nThe ```reticulate``` library has been already loaded.  We want to use Python packages next so the following code activates a Python 3.8 environment set up through miniconda.\n\n::: {.cell}\n\n```{.r .cell-code}\nuse_condaenv(\"py3.8\", required = TRUE)\npy_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npython:         C:/miniconda/envs/py3.8/python.exe\nlibpython:      C:/miniconda/envs/py3.8/python38.dll\npythonhome:     C:/miniconda/envs/py3.8\nversion:        3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 05:59:45) [MSC v.1929 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          C:/miniconda/envs/py3.8/Lib/site-packages/numpy\nnumpy_version:  1.22.4\n\nNOTE: Python version was forced by use_python function\n```\n:::\n:::\n\n### Import Python libraries\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\n#Pre-processing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n#Gaussian process model\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel\n\nimport sklearn.metrics as metrics\n```\n:::\n\n\n##Building the Gaussian Process Model\n\nIn general, the Sci-kit learn models require the independent (a.k.a. predictor) variables and dependent (a.k.a. target) variables to be in separate dataframes.  By convention, the predictors are in X and the target is in y. **Using the ```StandardScaler``` requires a conversion from a dataframe to a numpy array using ```.values```.**  Next, we split into train and test datasets.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX = r.concrete_tbl.drop(['compressive_strength'], axis=1).values\ny = r.concrete_tbl['compressive_strength'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.4, random_state = 10)\n```\n:::\n\n\nHere, I add the centering and scaling of the predictors and target using ```StandardScaler```.  The target variable needs to be converted to a single column using ```.reshape(-1,1)```.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nscaler = StandardScaler()\ntarget_scaler = StandardScaler()\n\nX_train_scale = scaler.fit_transform(X_train)\ny_train_scale = target_scaler.fit_transform(y_train.reshape(-1,1))\n```\n:::\n\n\nThere are many different options for selecting the kernels however I found that combining the radial basis function (RBF) kernel with a constant to account for mean offset and a white kernel to account for noisy data seemed to be a successful approach for this type of dataset.  And by success, I mean a fit that converges to a GP model without further warnings and with a respectable R-squared.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nkernel = ConstantKernel() * RBF() + WhiteKernel()\n\ngp_model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer = 5)\n\ngp_model.fit(X_train_scale, y_train_scale)\n```\n\n::: {.cell-output-display}\n```{=html}\n<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianProcessRegressor(kernel=1**2 * RBF(length_scale=1) + WhiteKernel(noise_level=1),\n                         n_restarts_optimizer=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianProcessRegressor</label><div class=\"sk-toggleable__content\"><pre>GaussianProcessRegressor(kernel=1**2 * RBF(length_scale=1) + WhiteKernel(noise_level=1),\n                         n_restarts_optimizer=5)</pre></div></div></div></div></div>\n```\n:::\n\n```{.python .cell-code}\ngp_model.kernel_\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.88**2 * RBF(length_scale=2.77) + WhiteKernel(noise_level=0.0679)\n```\n:::\n:::\n\n\nYou need to reverse the scaling in order to compare with the original values and more easily assess the metrics.\n\n::: {.cell}\n\n```{.python .cell-code}\n#Model Evaluation and error calculations\ny_pred_tr_scale, y_pred_tr_std_scale = gp_model.predict(X_train_scale, return_std=True)\ny_pred_tr = target_scaler.inverse_transform(y_pred_tr_scale.reshape(-1,1))\n\ntrain_metrics = [[\"RSq\", metrics.r2_score(y_train, y_pred_tr)], [\"Adjusted RSq\", 1 - (1-metrics.r2_score(y_train, y_pred_tr))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)], [\"MAE\", metrics.mean_absolute_error(y_train, y_pred_tr)], [\"MSE\", metrics.mean_squared_error(y_train, y_pred_tr)], [\"RMSE\", np.sqrt(metrics.mean_squared_error(y_train, y_pred_tr))]]\n\ntrain_metrics_df = pd.DataFrame(train_metrics, columns = [\"metric\", \"value\"])\nprint(train_metrics_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         metric      value\n0           RSq   0.951056\n1  Adjusted RSq   0.950414\n2           MAE   2.761809\n3           MSE  13.810692\n4          RMSE   3.716274\n```\n:::\n:::\n\nWe can visualize the predicted vs. actual (measured) compressive strengths in the figure below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train <- tibble(y_train = py$y_train, y_pred_tr = as.vector(py$y_pred_tr))\n\nggplot(data = pred_train, aes(x = y_train, y = y_pred_tr)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Gaussian Process Model: Predicted vs. Measured for Training Data\",\n       x = \"Actual Compressive Strength (MPa)\",\n       y = \"Predicted Compressive Strength (MPa)\") +\n  theme_light()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](2022-10-13_ConcreteGP_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThe distribution of model residuals should ideally be centered around 0 and normally distributed.  The residuals for the Gaussian Process Model are shown in the figure below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train %>%\n  mutate(resid_tr = y_train - y_pred_tr) %>%\n  ggplot(aes(x = resid_tr)) +\n    geom_histogram(aes(y = ..density..), fill=\"lightblue\") +\n    geom_density(color=\"darkblue\") +\n    geom_vline(aes(xintercept = mean(resid_tr)), linetype = \"dashed\") +\n    labs(title = \"Residual Error\",\n         x = \"Compressive Strength\") +\n  theme_light()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](2022-10-13_ConcreteGP_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThe residuals are centered around zero and about evenly distributed between negative and positive deviations.  A few outliers are evident especially in the prediction beyond -20 MPa.  The model performance on the training data was deemed acceptable to proceed to evaluation of the test data.\n\n## Evaluating the GP model predictions for the test data\n\nYou need to scale the test data before prediction using the same scaling used on the training dataset.\n\n::: {.cell}\n\n```{.python .cell-code}\nX_test_scale = scaler.transform(X_test)\ny_pred_te_scale, y_pred_te_std_scale = gp_model.predict(X_test_scale, return_std=True)\ny_pred_te = target_scaler.inverse_transform(y_pred_te_scale.reshape(-1,1))\ny_pred_te_std = y_pred_te_std_scale * target_scaler.scale_\n\ntpred_gp = metrics.r2_score(y_test, y_pred_te)\n\ntest_metrics = [[\"Rsq\", metrics.r2_score(y_test, y_pred_te)], [\"Adjusted RSq\", 1 - (1-metrics.r2_score(y_test, y_pred_te))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)], [\"MAE\", metrics.mean_absolute_error(y_test, y_pred_te)], [\"MSE\", metrics.mean_squared_error(y_test, y_pred_te)], [\"RMSE\", np.sqrt(metrics.mean_squared_error(y_test, y_pred_te))]]\n\ntest_metrics_df = pd.DataFrame(test_metrics, columns = [\"metric\", \"value\"])\nprint(test_metrics_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         metric      value\n0           Rsq   0.891769\n1  Adjusted RSq   0.889621\n2           MAE   4.019690\n3           MSE  29.627775\n4          RMSE   5.443140\n```\n:::\n:::\n\n\nThe model performance was a bit worse for the testing data as compared to the training data.  I believe one reason is that cross-validation was not used and the model is overfitting the training data.  One of the advantages of the Gaussian Process model is the estimation of uncertainty in the prediction.  In the figure below, the predicted vs. measured compressive strengths for the test dataset are displayed along with error bars for +/- 1 standard deviation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_test <- tibble(y_test = py$y_test, y_pred_te = as.vector(py$y_pred_te), y_pred_te_std = py$y_pred_te_std)\n\nggplot(data = pred_test, aes(x = y_test, y = y_pred_te)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = y_pred_te - y_pred_te_std, ymax = y_pred_te + y_pred_te_std)) +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Gaussian Process Model: Predicted vs. Measured for Testing Data\",\n       x = \"Actual Compressive Strength (MPa)\",\n       y = \"Predicted Compressive Strength (MPa)\") +\n  theme_light()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](2022-10-13_ConcreteGP_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n## Summary\nA Gaussian process model has been built for the concrete dataset.  The predictive performance of this model was lower than for random forest and xgboost models (GP R^2^ = 0.89 vs. RF R^2^ = 0.94).  The main advantage of the Gaussian Process model is the calculation of prediction error which can be very helpful in assessing confidence in future predictions.\n\n:::{.callout-tip collapse=\"true\"}\n## Expand for Session Info\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22 ucrt)\n os       Windows 10 x64 (build 19043)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2022-09-02\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.0.36 @ C:\\\\PROGRA~1\\\\RStudio\\\\bin\\\\quarto\\\\bin\\\\quarto.cmd\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P dplyr       * 1.0.10  2022-09-01 [?] CRAN (R 4.2.0)\n P forcats     * 0.5.2   2022-08-19 [?] CRAN (R 4.2.1)\n P ggplot2     * 3.3.6   2022-05-03 [?] CRAN (R 4.2.1)\n P purrr       * 0.3.4   2020-04-17 [?] CRAN (R 4.2.1)\n P readr       * 2.1.2   2022-01-30 [?] CRAN (R 4.2.1)\n P readxl      * 1.4.1   2022-08-17 [?] CRAN (R 4.2.1)\n P reticulate  * 1.26    2022-08-31 [?] CRAN (R 4.2.1)\n P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.2.1)\n P stringr     * 1.4.1   2022-08-20 [?] CRAN (R 4.2.1)\n P tibble      * 3.1.8   2022-07-22 [?] CRAN (R 4.2.1)\n P tidyr       * 1.2.0   2022-02-01 [?] CRAN (R 4.2.1)\n P tidyverse   * 1.3.2   2022-07-18 [?] CRAN (R 4.2.1)\n\n [1] C:/Users/David Zoller/AppData/Local/Temp/RtmpQ5SNYG/renv-library-35903dd013ae\n [2] C:/Users/David Zoller/Documents/datadavidz.github.io/renv/library/R-4.2/x86_64-w64-mingw32\n [3] C:/Program Files/R/R-4.2.0/library\n\n P ── Loaded and on-disk path mismatch.\n\n─ Python configuration ───────────────────────────────────────────────────────\n python:         C:/miniconda/envs/py3.8/python.exe\n libpython:      C:/miniconda/envs/py3.8/python38.dll\n pythonhome:     C:/miniconda/envs/py3.8\n version:        3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 05:59:45) [MSC v.1929 64 bit (AMD64)]\n Architecture:   64bit\n numpy:          C:/miniconda/envs/py3.8/Lib/site-packages/numpy\n numpy_version:  1.22.4\n \n NOTE: Python version was forced by use_python function\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n\n:::",
    "supporting": [
      "2022-10-13_ConcreteGP_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}