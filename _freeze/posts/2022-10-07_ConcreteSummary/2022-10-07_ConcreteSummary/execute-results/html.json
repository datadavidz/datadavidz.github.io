{
  "hash": "d732feb750e21adc9cd070ebba0d6f86",
  "result": {
    "markdown": "---\ntitle: \"Summary of Concrete Models\"\ndate: \"2022-10-07\"\n---\n\n\nA comparison of the predictive performance and speed for the different modeling approaches.\n\nSeveral models have been created to predict the compressive strength of high performance concrete based on the I-Cheng Yeh [dataset](http://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength).  A conventional material model using a pre-determined transfer function which was fit to the data using a non-linear least squares approach. Four different models were created using machine learning algorithms, elastic net (glmnet), single-layer neural net (nnet), random forest (ranger) and boosted tree (xgboost), applied to the dataset.\n\n## Load libraries and data\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(knitr)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(tidymodels)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfilename <- \"Concrete_Data.xls\"\n\nfolder <- \"../data/\"\nnumberCols <- 9 #total number of columns in spreadsheet\n\ncolTypes <- rep(\"numeric\", numberCols)\nconcrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)\n\nconcrete_tbl <- concrete_tbl %>%\n  rename(cement = starts_with(\"Cement\")) %>%\n  rename(blast_furnace_slag = starts_with(\"Blast\")) %>%\n  rename(fly_ash = starts_with(\"Fly Ash\")) %>%\n  rename(water = starts_with(\"Water\")) %>%\n  rename(superplasticizer = starts_with(\"Super\")) %>%\n  rename(coarse_aggregate = starts_with(\"Coarse\")) %>%\n  rename(fine_aggregate = starts_with(\"Fine\")) %>%\n  rename(age = starts_with(\"Age\")) %>%\n  rename(compressive_strength = starts_with(\"Concrete\"))\n```\n:::\n\n\n## Predictive Accuracy\n\nEach model performance was assessed by several metrics: R-squared (R^2^), Root Mean Square Error (RMSE) and Mean Absolute Error (MAE).\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2022-10-07_ConcreteSummary_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n|metric |  nls|   glm|  mlp|   rf| xgboost|\n|:------|----:|-----:|----:|----:|-------:|\n|rmse   | 7.76| 11.37| 6.53| 5.08|    4.33|\n|rsq    | 0.78|  0.62| 0.88| 0.93|    0.95|\n|mae    | 5.95|  9.09| 4.94| 3.32|    2.69|\n:::\n:::\n\nAs shown in the figure and table above, the random forest (rf) and boosted tree (xgboost) models showed a significant improvement in predictive capability as compared with the conventional modeling approach (nls).  The xgboost model had an R^2^ of 0.95 compared to 0.78 for the nls model with similar improvements in root mean square error and mean absolute error.  The glmnet model gave worse performance than the non-linear models.\n\n## Benchmark performance\n\nHere we load the final models for the different approaches for comparison of prediction time.  In this case, the time to make predictions for 10,300 (10 times the original dataset) was determined as a benchmark.\n\n::: {.cell}\n\n```{.r .cell-code}\n#load all the models\nconcrete_nls <- readRDS(\"../results/concrete_nls_model.rds\")\nconcrete_glm <- readRDS(\"../results/concrete_glm_model.rds\")\nconcrete_mlp <- readRDS(\"../results/concrete_mlp_model.rds\")\nconcrete_rf <- readRDS(\"../results/concrete_rf_model.rds\")\nconcrete_xgb <- readRDS(\"../results/concrete_xgb_model.rds\")\n```\n:::\n\n\nCreate the prediction dataset using 10 times the original dataset for purpose of comparing very fast prediction times.\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp <- concrete_tbl %>% slice(rep(row_number(), 10))\n```\n:::\n\n\nBenchmarking was performed in the following manner using Sys.time to capture the time before and after each set of model predictions.\n\n::: {.cell}\n\n```{.r .cell-code}\nbegin <- Sys.time()\na_temp <- predict(concrete_nls, new_data = temp)\nend1 <- Sys.time()\n\nb_temp <- predict(concrete_glm, new_data = temp)\nend2 <- Sys.time()\n\nc_temp <- predict(concrete_mlp, new_data = temp)\nend3 <- Sys.time()\n\nd_temp <- predict(concrete_rf, new_data = temp)\nend4 <- Sys.time()\n\ne_temp <- predict(concrete_xgb, new_data = temp)\nend5 <- Sys.time()\n\n# print(end1 - begin)[[1]]\n# print(end2 - end1)[[1]]\n# print(end3 - end2)[[1]]\n# print(end4 - end3)[[1]]\n\n#rm(temp)\n```\n:::\n\n\nAs shown in the figure and table below, the xgboost model was the slowest taking about 1 second to perform 10,300 predictions.  For the example of making a prediction of compressive strength of concrete for a particular formulation, however, this amount of time is trivial and the increased accuracy would be preferred over a faster and less accurate model.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2022-10-07_ConcreteSummary_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n|Model   | Time (ms)|\n|:-------|---------:|\n|nls     |         3|\n|glm     |       168|\n|mlp     |        75|\n|rf      |       741|\n|xgboost |       771|\n:::\n:::\n\n## Summary\nPrediction with the conventional model (nls) is about two orders of magnitude faster than the boosted tree model (xgboost).  It should be noted that the random forest model is about 40% faster than the xgboost model, in this case, with similar predictive accuracy.\n",
    "supporting": [
      "2022-10-07_ConcreteSummary_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}