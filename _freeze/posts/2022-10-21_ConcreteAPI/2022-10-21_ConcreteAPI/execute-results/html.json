{
  "hash": "0137b0d14c00dcf82eff8061a9ecf792",
  "result": {
    "markdown": "---\ntitle: \"Create a Dockerized API Running on an AWS EC2 instance\"\ndate: \"2022-10-21\"\ncategories: [tidymodels, MLOps]\n---\n\n::: {.cell}\n\n:::\n\n\nAn Application Programming Interface (API) to predict concrete compressive strength is implemented in the cloud using an AWS EC2 instance.\n\nIn this post, we create an API for the deployable model object pinned to an S3 bucket as described in the previous [post](https://datadavidz.github.io/posts/2022-10-14_ConcreteS3/2022-10-14_ConcreteS3.html).  We start by creating an Elastic Compute (EC2) instance on AWS to run the API.  The ```vetiver``` package is used to write a Dockerfile for running the API inside a Docker container on the EC2 instance.  The R script to run a ```plumber``` API for the model object is also created using the ```vetiver``` package.  The EC2 instance is set up with Docker and the container is created from the Dockerfile.  The API can then be run to provide compressive strength predictions for different concrete formulations.\n\n## Create an EC2 instance to run a Docker container with a Plumber API\n\nI chose to use the Elastic Compute (EC2) service from AWS to run the API in a docker container.  The basics of setting up an EC2 instance are captured well in the [The Shiny AWS book](https://business-science.github.io/shiny-production-with-aws-book/aws-ec2-server-setup.html).  I mostly accepted the defaults offered by AWS as described below:\n\n1. Enter a name for the instance such as *docker-api-test*.\n2. Select *Amazon Machine Image (AMI)* for which I chose the free tier eligible, Amazon Linux (Amazon Linux 2 Kernel 5.10 AMI 2.0.20220912.1 x86_64 HVM gp2).\n3. Select the *Instance Type*.  Here is chose free tier eligible, *t2.micro* (1 CPU, 1 GB memory).\n4. Create a key-pair for security access (unless you already have one you would like to re-use). This generates a .pem file to save.\n5. Leave the *Network Settings* as default for now. These settings will need to be changed later.\n6. For *Storage*, I increased the amount to the free tier limit of 30 GB.\n7. No changes to *Detailed Settings*.\n8. Launch the instance.\n\n## Create a Security Group to Allow Access to the API\n\nThe inbound and outbound rules need to be adjusted for the API to work properly.  I found an [article](https://mblukac.github.io/posts/2021/05/plumber_AWSEC2/) by Martin Lukac on \"Deploying a plumber API to AWS EC2 instance\" which contained the rules which also worked for me.\n\nInbound rules:  \n1. **Type:** SSH, **Protocol:** TCP, **Port:** 22, **Source:** 0.0.0.0/0 (by default this was added)  \n2. **Type:** HTTP, **Protocol:** TCP, **Port:** 80, **Source:** 0.0.0.0/0  \n3. **Type:** Custom TCP, **Protocol:** TCP, **Port:** 8000, **Source:** 0.0.0.0/0 (for accessing the API)  \n4. **Type:** Custom ICMP Rule IPv4, **Protocol:** Echo Request, **Port:** N/A, **Source:** 0.0.0.0/0 (for testing)  \n  \nOutbound rule:  \n1. Type: All traffic, Protocol: All, Port Range: All, Destination: 0.0.0.0/0 (by default this was added)  \n  \nThe *Security Group* is then added to the proper EC2 instance.  \n\n## Connecting to your EC2 instance with PuTTY\n\nYou need to connect to your EC2 instance via an SSH client.  A convenient option for Windows users is [PuTTY](https://www.putty.org/) an SSH and telnet client originally developed by Simon Tatham. The website for the Alaska Satellite Facility has a nice walkthrough on setting up PuTTY to access your EC2 instance.\n\n### Generate a PuTTY private key file (.ppk)\nIn brief, the steps to create the .ppk file from the AWS .pem file are as follows:\n\n1. Start the *puttygen.exe* program.\n2. Click on *Load* and find your .pem file you generated in Step 4 of *Create an EC2 Instance* above.\n3. Make sure *Type of key to generate* is set to **RSA**.\n4. Click on *Save private key*, name the file and save the ppk file.\n\n### Configure PuTTY to connect to your EC2 instance\n\n1. Start the *putty.exe* program\n2. Enter the Host name as ec2-user@*your_public_DNS* where your_public_DNS is listed in the description for your EC2 on the AWS console.  It should be something like *ec2-12-345-678-910.compute-1.amazonaws.com*.  Note: If you use an Ubuntu machine image the host name will begin with *ubuntu@* instead of *ec2-user@*.\n3. Make sure the *Port* is set to **22**.\n4. The *Connection type* needs to be set to **SSH**.\n5. In the *Category* pane on the left side of PuTTY configuration window, find the *Connection* category and expand the *SSH* options by clicking on the \"+\" and then click on *Auth*.\n6. Under Private key for authentification, click on the Browse button and load the ppk file you generated above.\n7. In the *Category* pane, click on *Session* and in the box under *Saved Sessions*, enter a name for this connection and then click on *Save*.\n\n### Connect to your EC2\n1. Start the *\"putty.exe\"* program.\n2. Click on the *Saved Sessions* you named in Step 7 above.\n3. Click on *Open* to bring up a terminal session connected to your EC2.\n\n## Load and Start Docker on EC2\n\nThe AWS EC2 instance does not come pre-loaded with Docker so it needs to be installed.  A nice walkthrough can be found on the [Workfall Blog] (https://www.workfall.com/learning/blog/how-to-install-and-run-docker-containers-on-amazon-ec2-instance/).  You first need to connect to your EC2 instance with PuTTY as described above.  The next steps are as follows:\n\n1. Update the installed packages and package cache by running the command:```sudo yum update -y```\n2. Install the most recent Docker package by running the command: ```sudo amazon-linux-extras install docker```\n3. Start the Docker service: ```sudo service docker start```\n4. Add the ec2-user to the Docker group so you don't need to execute Docker commands with sudo: ```sudo usermod -a -G Docker ec2-user```\n5. Log out and log back into the EC2 instance and run a command to verify: ```docker info```\n\n## Write the Dockerfile and Plumber API script\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vetiver)\nlibrary(pins)\n\nboard <- board_s3(\"pins-test-zoller\", region = \"us-east-2\")\nv <- vetiver_pin_read(board, name = \"concrete-xgb\", version = \"20221019T191525Z-b1278\")\n\nvetiver_write_plumber(board, name = \"concrete-xgb\", version = \"20221019T191525Z-b1278\", rsconnect = FALSE)\nvetiver_write_docker(v)\n```\n:::\n\nThe contents of the Dockerfile\n```\n# Generated by the vetiver package; edit with care\n\nFROM rocker/r-ver:4.2.0\nENV RENV_CONFIG_REPOS_OVERRIDE https://packagemanager.rstudio.com/cran/latest\n\nRUN apt-get update -qq && apt-get install -y --no-install-recommends \\\n  libcurl4-openssl-dev \\\n  libicu-dev \\\n  libsodium-dev \\\n  libssl-dev \\\n  make \\\n  zlib1g-dev \\\n  && apt-get clean\n\nCOPY vetiver_renv.lock renv.lock\nRUN Rscript -e \"install.packages('renv')\"\nRUN Rscript -e \"renv::restore()\"\nCOPY plumber.R /opt/ml/plumber.R\nEXPOSE 8000\nENTRYPOINT [\"R\", \"-e\", \"pr <- plumber::plumb('/opt/ml/plumber.R'); pr$run(host = '0.0.0.0', port = 8000)\"]\n```\n\nThe contents of the plumber.R file\n```\n# Generated by the vetiver package; edit with care\n\nlibrary(pins)\nlibrary(plumber)\nlibrary(rapidoc)\nlibrary(vetiver)\nb <- board_s3(bucket = \"pins-test-zoller\", region = structure(\"us-east-2\", tags = list(type = \"scalar\")))\nv <- vetiver_pin_read(b, \"concrete-xgb\", version = \"20221019T191525Z-b1278\")\n\n#* @plumber\nfunction(pr) {\n    pr %>% vetiver_api(v)\n}\n```\n\n## Editing the Dockerfile for Amazon Linux EC2\n\nI found that a couple of edits were necessary to successfully create the Docker container.  The libxml2 library needs to be installed in the container which can be added into the ```RUN apt-get update``` command line.  The paws.storage R library needs to be installed by adding the command ```RUN Rscript -e \"install.packages('paws.storage')\"```.  This issue has been reported on the vetiver Github as it is not recognized as a dependency when creating the renv lockfile.\n\nThe edited version should be:\n```\n# Generated by the vetiver package; edit with care\n\nFROM rocker/r-ver:4.2.0\nENV RENV_CONFIG_REPOS_OVERRIDE https://packagemanager.rstudio.com/cran/latest\n\nRUN apt-get update -qq && apt-get install -y --no-install-recommends \\\n  libcurl4-openssl-dev \\\n  libicu-dev \\\n  libsodium-dev \\\n  libssl-dev \\\n  libxml2 \\\n  make \\\n  zlib1g-dev \\\n  && apt-get clean\n\nCOPY vetiver_renv.lock renv.lock\nRUN Rscript -e \"install.packages('renv')\"\nRUN Rscript -e \"renv::restore()\"\nRUN Rscript -e \"install.packages('paws.storage')\"\nCOPY plumber.R /opt/ml/plumber.R\nEXPOSE 8000\nENTRYPOINT [\"R\", \"-e\", \"pr <- plumber::plumb('/opt/ml/plumber.R'); pr$run(host = '0.0.0.0', port = 8000)\"]\n```\n\n## Copying files to your EC2 instance\n\nThe article at this [link](https://asf.alaska.edu/how-to/data-recipes/moving-files-into-and-out-of-an-aws-ec2-instance-windows/) describes two different methods for copying files from Windows to an EC2 instance using WinSCP and using PuTTY secure copy.  I used PuTTY secure copy (PSCP) since PuTTY is already installed on my computer and there are not many files to copy.  PSCP is a command line utility \n\n```pscp -i your-key.ppk yourfilename ec2-user@yourPublicDNS:/home/ec2-user/```\n\nwhere your-key.ppk is your PuTTY private key file previously generated to connect to your EC2 instance.  The ppk file needs to be in the same directory with the files you are copying to the EC2 instance.\n\nYou need to copy the Dockerfile, vetiver_renv.lock and plumber.R files to your EC2 instance in the ec2-user directory.\n\n\n## Build and run the Docker API\n\n1. Log in to (SSH) the EC2 instance and start the Docker service: ```sudo service docker start``` \n2. Build the docker container from the Dockerfile: ```docker build -t concrete-xgb-api .```\n3. Start the API: ```docker run --rm -p 8000:8000 concrete-xgb-api```\n4. The API can then be tested by connecting to the rapiddoc Docs on your instance via a web browser:\n\n```http://yourEC2PublicDNS:8000/__docs__/```\n\nIf you successfully connect to the API, you will see a page such as shown below.\n\n![Your rapiddoc API home page](ConcreteAPI_DOCS.PNG)\n\nYou can click on GET and TRY to ping the status of your API.  You can test the predictive model by clicking on POST and then under REQUEST click on the Example tab.  You can then enter different values for the predictors in the format shown in the Example and then click on TRY and the outcome will show below in the Response window.  An example for the Concrete API is shown in the figure below.\n\n![Example of Concrete Strength Prediction from the rapiddoc page](ConcreteAPI_POST.PNG)\n\n## Summary\n\nIn this post, I have shown how to launch an API inside a Docker Container running on an EC2 instance.  The Dockerfile and ```plumber``` API script was created using the ```vetiver``` package.  The API accesses an xgboost model to predict concrete strength from ingredients and age that was versioned and posted to an AWS S3 container as described in the previous post.  I am curious about building a Shiny app to utilize this API but this will need to wait for a future post.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}