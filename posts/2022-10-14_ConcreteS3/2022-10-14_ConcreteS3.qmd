---
title: "Pin a Vetiver Model to an AWS S3 Container"
date: "2022-10-14"
categories: [tidymodels, MLOps]
---
```{r}
#| echo: false
#| results: "hide"
renv::use(lockfile = "renv.lock")
```

An XGBoost model for predicting concrete strength is transformed into a deployable model object and uploaded to an AWS S3 container.

In this post, I will take the XGBoost model for predicting concrete compressive strength described in a previous [post](https://datadavidz.github.io/posts/2022-09-24_ConcreteXGB/2022-09-24_ConcreteXGB.html), convert the model into a deployable model object using ```vetiver```and "pin" it to an S3 bucket.  The purpose of this effort is to make the model accessible in the cloud to an API running in a different location.  The development of the API will be discussed in the next post.  S3 stands for the AWS Simple Storage Service which exists in the cloud.  I chose AWS over other ```vetiver```-compatible options simply because I already had an existing account.   

## Build the model (again)
This section just performs the steps to build the XGBoost model described in detail in the previous post.

*Expand to see the code*
```{r}
#| warning: false
#| message: false
#| code-fold: true
library(readxl)
library(tidyverse)

#Tidymodels
library(tidymodels)
library(xgboost)

#MLOps
library(vetiver)
library(pins)

#Load the dataset
filename <- "Concrete_Data.xls"

folder <- "../data/"
numberCols <- 9 #total number of columns in spreadsheet

colTypes <- rep("numeric", numberCols)
concrete_tbl <- read_excel(path = paste0(folder, filename), col_types = colTypes)

concrete_tbl <- concrete_tbl %>%
  rename(cement = starts_with("Cement")) %>%
  rename(blast_furnace_slag = starts_with("Blast")) %>%
  rename(fly_ash = starts_with("Fly Ash")) %>%
  rename(water = starts_with("Water")) %>%
  rename(superplasticizer = starts_with("Super")) %>%
  rename(coarse_aggregate = starts_with("Coarse")) %>%
  rename(fine_aggregate = starts_with("Fine")) %>%
  rename(age = starts_with("Age")) %>%
  rename(compressive_strength = starts_with("Concrete"))

#Split the data into training and testing datasets
set.seed(123)
concrete_split <- initial_split(concrete_tbl, prop = 0.80)
concrete_train <- training(concrete_split)
concrete_test <- testing(concrete_split)

#Create the model recipe
concrete_rec <- recipe(compressive_strength ~ ., data = concrete_train) %>%
  step_normalize(all_predictors())

#Create the model specification. Parameters were specified from tuning in a previous post.
xgboost_spec = boost_tree(
  trees = 1000,
  min_n = 18,
  tree_depth = 10,
  learn_rate = 0.02647525
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

#Create the modeling workflow
concrete_wf <- workflow() %>%
  add_recipe(concrete_rec) %>%
  add_model(xgboost_spec)

#Fit model on train and evaluate on test.
final_res <- last_fit(concrete_wf, concrete_split, metrics = metric_set(rmse, rsq, mae))

#Assess final model performance metrics
collect_metrics(final_res)
```

## Create the Deployable Model Object

The deployable model object is created using the ```vetiver``` package.  It is really as simple as extracting the workflow and passing it to the ```vetiver_model``` function.

```{r}
v <- final_res %>%
  extract_workflow() %>%
  vetiver_model(model_name = "concrete-xgb")

v
```
## Pins and AWS s3

The ```pins``` package allows you to save data, models or R objects to the cloud such as an AWS S3 container.  A new S3 container can be set up within AWS.  In my case, I just used the default settings for the S3 bucket with the name ```pins-test-zoller```.  A security id and access key need to be set up to enable saving of data from your local computer to the S3 container.  In your AWS account options under Security Credentials, you can configure your security id and access key and save the file to your local computer.  There are multiple options to tell R where to find this information but I preferred to create a shared AWS credentials file in a text editor as follows:

```
[default]
aws_access_key_id=your AWS access key
aws_secret_access_key=your AWS secret key
```

On a Windows computer, the file needs to be saved with the name ```credentials``` without any extension.  The file location needs to be ```C:\Users\[your username]\.aws\```.  You may need to create the ```.aws``` directory.

You can then connect to the board where you want to place the pin using ```board_s3``` command.  Here, we pin the vetiver model for the concrete data.
```{r}
board <- board_s3("pins-test-zoller", region = "us-east-2")
board %>% vetiver_pin_write(v)
```
In the AWS S3 bucket with the name "pins-test-zoller", a new folder is created with the same name as the model, concrete-xgb.  Within this folder, there is a subfolder with the named according to the model version number and, within the subfolder, is the model object in rds form (concrete-xgb.rds) and a data.txt file with summary information about the model object.

## Summary

An XGBoost model for the concrete dataset has been converted to a deployable model object using the ```vetiver``` package and then uploaded (i.e. pinned) to an AWS S3 bucket.  The model object can now be accessed in the cloud for different purposes including creating an API to provide model predictions.  The API use case will be discussed further in the next post.

:::{.callout-tip collapse="true"}
## Expand for Session Info
```{r}
#| warning: false
#| echo: false
library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```
:::